{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FedImpute FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios. Installation Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, } Step 2. Simulate Federated Missing Data Scenario from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) Step 3. Execute Federated Imputation Algorithms from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation() Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Home"},{"location":"#welcome-to-fedimpute","text":"FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios.","title":"Welcome to FedImpute"},{"location":"#installation","text":"Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"#basic-usage","text":"","title":"Basic Usage"},{"location":"#step-1-prepare-data","text":"import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, }","title":"Step 1. Prepare Data"},{"location":"#step-2-simulate-federated-missing-data-scenario","text":"from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 )","title":"Step 2. Simulate Federated Missing Data Scenario"},{"location":"#step-3-execute-federated-imputation-algorithms","text":"from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation()","title":"Step 3. Execute Federated Imputation Algorithms"},{"location":"#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Step 4. Evaluate imputation outcomes"},{"location":"about/","text":"Arbor mecum nostro Rhesi Illic spumam Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus! Est niveo serpentem sit Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail(office.monochrome(46), koffice_ascii, mailItunes); node += cable_bezel_menu; if (powerFatAtm.clipboard_text_vertical.bccKvmLeaf(dmaOverwriteAscii, plain) + passiveCameraWaveform(462879)) { gateway = multicasting(drive, hubMashupAvatar.floppy_zero_qwerty.rom_linux_log(digital, dllDebug, 2)); } Sit est ternisque inter Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Arbor mecum nostro Rhesi"},{"location":"about/#arbor-mecum-nostro-rhesi","text":"","title":"Arbor mecum nostro Rhesi"},{"location":"about/#illic-spumam","text":"Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus!","title":"Illic spumam"},{"location":"about/#est-niveo-serpentem-sit","text":"Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail(office.monochrome(46), koffice_ascii, mailItunes); node += cable_bezel_menu; if (powerFatAtm.clipboard_text_vertical.bccKvmLeaf(dmaOverwriteAscii, plain) + passiveCameraWaveform(462879)) { gateway = multicasting(drive, hubMashupAvatar.floppy_zero_qwerty.rom_linux_log(digital, dllDebug, 2)); }","title":"Est niveo serpentem sit"},{"location":"about/#sit-est-ternisque-inter","text":"Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Sit est ternisque inter"},{"location":"get_started/","text":"Installation Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data import numpy as np data = np.random.rand(10000, 10) data_config = { 'target': 9, 'task_type': 'regression', 'clf_type': None, 'num_cols': 10, } Step 2. Simulate Federated Missing Data Scenario from fedimpute.simulator import Simulator simulator = Simulator() # classifical simulation function simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) # lite simulation function simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='niid-dir@0.1', ms_scenario = 'mar-heter', verbose=1 ) Step 3. Execute Federated Imputation Algorithms from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation() Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Install & Basic Usage"},{"location":"get_started/#installation","text":"Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"get_started/#basic-usage","text":"","title":"Basic Usage"},{"location":"get_started/#step-1-prepare-data","text":"import numpy as np data = np.random.rand(10000, 10) data_config = { 'target': 9, 'task_type': 'regression', 'clf_type': None, 'num_cols': 10, }","title":"Step 1. Prepare Data"},{"location":"get_started/#step-2-simulate-federated-missing-data-scenario","text":"from fedimpute.simulator import Simulator simulator = Simulator() # classifical simulation function simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) # lite simulation function simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='niid-dir@0.1', ms_scenario = 'mar-heter', verbose=1 )","title":"Step 2. Simulate Federated Missing Data Scenario"},{"location":"get_started/#step-3-execute-federated-imputation-algorithms","text":"from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation()","title":"Step 3. Execute Federated Imputation Algorithms"},{"location":"get_started/#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Step 4. Evaluate imputation outcomes"},{"location":"overview/","text":"Overview of FedImpute FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment. Understanding Missing Data in Horizontal Federated Learning Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions. Types of Missing Data Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction . Missing Data Under Federated Learning Scenario Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees. Missing Data Imputation of Federated Datasets A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need. Features of FedImpute Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs. Architecture and Components FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms. License FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"Overview of FedImpute"},{"location":"overview/#overview-of-fedimpute","text":"FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment.","title":"Overview of FedImpute"},{"location":"overview/#understanding-missing-data-in-horizontal-federated-learning","text":"Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions.","title":"Understanding Missing Data in Horizontal Federated Learning"},{"location":"overview/#types-of-missing-data","text":"Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction .","title":"Types of Missing Data"},{"location":"overview/#missing-data-under-federated-learning-scenario","text":"Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees.","title":"Missing Data Under Federated Learning Scenario"},{"location":"overview/#missing-data-imputation-of-federated-datasets","text":"A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need.","title":"Missing Data Imputation of Federated Datasets"},{"location":"overview/#features-of-fedimpute","text":"Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs.","title":"Features of FedImpute"},{"location":"overview/#architecture-and-components","text":"FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms.","title":"Architecture and Components"},{"location":"overview/#license","text":"FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"License"},{"location":"api/fed_impute/","text":"","title":"Federated imputation"},{"location":"api/simulation/","text":"","title":"Scenario simulation"},{"location":"tutorial/basic_usage/","text":"","title":"Basic Usage"},{"location":"user-guide/data_prep/","text":"Dataset and Preprocessing The first step for using FedImpute is to prepare the data. Input Data Format and Preprocessing The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced. Required Preprocessing Steps There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ | Ordering Features To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset. One-hot Encoding Categorical Features Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute. Data Normalization (Optional) It is recommended to normalize the numerical features in the dataset within range of 0 and 1. Helper Functions for Preprocessing FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features, one_hot_encoding # Example for data with numpy array data = ... data = ordering_features(data, numerical_cols=[0, 1, 3, 4, 8], target_col=-1) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) # Example data with pandas dataframe data = ... data = ordering_features( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) Data Configuration Dictionary To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target': 'house_price', 'task_type': 'classification', 'clf_type': 'binary', 'num_cols': 10, } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Preparation"},{"location":"user-guide/data_prep/#dataset-and-preprocessing","text":"The first step for using FedImpute is to prepare the data.","title":"Dataset and Preprocessing"},{"location":"user-guide/data_prep/#input-data-format-and-preprocessing","text":"The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced.","title":"Input Data Format and Preprocessing"},{"location":"user-guide/data_prep/#required-preprocessing-steps","text":"There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ |","title":"Required Preprocessing Steps"},{"location":"user-guide/data_prep/#ordering-features","text":"To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset.","title":"Ordering Features"},{"location":"user-guide/data_prep/#one-hot-encoding-categorical-features","text":"Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute.","title":"One-hot Encoding Categorical Features"},{"location":"user-guide/data_prep/#data-normalization-optional","text":"It is recommended to normalize the numerical features in the dataset within range of 0 and 1.","title":"Data Normalization (Optional)"},{"location":"user-guide/data_prep/#helper-functions-for-preprocessing","text":"FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features, one_hot_encoding # Example for data with numpy array data = ... data = ordering_features(data, numerical_cols=[0, 1, 3, 4, 8], target_col=-1) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) # Example data with pandas dataframe data = ... data = ordering_features( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' )","title":"Helper Functions for Preprocessing"},{"location":"user-guide/data_prep/#data-configuration-dictionary","text":"To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target': 'house_price', 'task_type': 'classification', 'clf_type': 'binary', 'num_cols': 10, } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Configuration Dictionary"},{"location":"user-guide/evaluation/","text":"Evaluation of Imputation Outcomes Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Downstream Prediction : Evaluate the performance of downstream prediction tasks using imputed data (supports both local or federated prediction). Basic Usage The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results() The Evaluator.evaluate() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect. Supported Evaluation Metrics The following evaluation metrics are supported for each evaluation aspect: Imputation Quality Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions. Downstream Prediction After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. We contains three built-in local downstream prediction models (Linear regression linear , Random Forest tree , and two-layer neural network nn ) and various metrics such as accuracy accu , AUROC auroc , f1 score f1 for evaluation of classification tasks, and mean square error mse , r2 score r2 , mean square log error msle , mean abusolute error mae for evaluation of regression tasks. {\\fedimpute} also supports federated downstream task evaluation for a two-layer neural network with \\textbf{FedAvg} as federated learning strategies. By assessing the performance of the downstream tasks, we can gain insights into how different imputation methods impact the overall effectiveness of the learning process in the presence of missing data. Variance Metrics (Measure the variance of imputationa and prediction outcomes) Variance variance : Variance is a statistical measure that quantifies the spread of a set of values around their mean. We calculate the variance of each client's imputation quality metric value. Jain Index jain-index :The Jain index, also known as the Jain fairness index, is a metric used to evaluate the fairness or equality of resource allocation in distributed systems. We adopt this metric to assess the equality of imputation quality across clients. The Jain index ranges from $ 1/N $ to $1$, where $1$ is the number of clients. A value of $1$ indicates perfect equality, meaning that all clients have the same imputation quality, while a value of $1/N$ represents the worst-case scenario, where one client dominates the imputation performance. Entropy entropy : We utilize entropy to capture the distributional variation of imputation quality across clients. We compute the entropy of this distribution using the standard formula: $-\\sum(p_i * log(p_i))$, where $p_i$ is the normalized imputation quality metric value for client $i$. A higher entropy value indicates a more","title":"Evaluation"},{"location":"user-guide/evaluation/#evaluation-of-imputation-outcomes","text":"Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Downstream Prediction : Evaluate the performance of downstream prediction tasks using imputed data (supports both local or federated prediction).","title":"Evaluation of Imputation Outcomes"},{"location":"user-guide/evaluation/#basic-usage","text":"The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results() The Evaluator.evaluate() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect.","title":"Basic Usage"},{"location":"user-guide/evaluation/#supported-evaluation-metrics","text":"The following evaluation metrics are supported for each evaluation aspect:","title":"Supported Evaluation Metrics"},{"location":"user-guide/evaluation/#imputation-quality","text":"Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions.","title":"Imputation Quality"},{"location":"user-guide/evaluation/#downstream-prediction","text":"After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. We contains three built-in local downstream prediction models (Linear regression linear , Random Forest tree , and two-layer neural network nn ) and various metrics such as accuracy accu , AUROC auroc , f1 score f1 for evaluation of classification tasks, and mean square error mse , r2 score r2 , mean square log error msle , mean abusolute error mae for evaluation of regression tasks. {\\fedimpute} also supports federated downstream task evaluation for a two-layer neural network with \\textbf{FedAvg} as federated learning strategies. By assessing the performance of the downstream tasks, we can gain insights into how different imputation methods impact the overall effectiveness of the learning process in the presence of missing data.","title":"Downstream Prediction"},{"location":"user-guide/evaluation/#variance-metrics-measure-the-variance-of-imputationa-and-prediction-outcomes","text":"Variance variance : Variance is a statistical measure that quantifies the spread of a set of values around their mean. We calculate the variance of each client's imputation quality metric value. Jain Index jain-index :The Jain index, also known as the Jain fairness index, is a metric used to evaluate the fairness or equality of resource allocation in distributed systems. We adopt this metric to assess the equality of imputation quality across clients. The Jain index ranges from $ 1/N $ to $1$, where $1$ is the number of clients. A value of $1$ indicates perfect equality, meaning that all clients have the same imputation quality, while a value of $1/N$ represents the worst-case scenario, where one client dominates the imputation performance. Entropy entropy : We utilize entropy to capture the distributional variation of imputation quality across clients. We compute the entropy of this distribution using the standard formula: $-\\sum(p_i * log(p_i))$, where $p_i$ is the normalized imputation quality metric value for client $i$. A higher entropy value indicates a more","title":"Variance Metrics (Measure the variance of imputationa and prediction outcomes)"},{"location":"user-guide/extend_guide/","text":"","title":"Extend guide"},{"location":"user-guide/fed_imp/","text":"Executing Federated Imputation Algorithms The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms. Overview and Basic Usage Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation() Environment Configuration The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation. Supported Federated Imputation Algorithms Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Fed-Mean Non-NN - fed_mean simple - Fed-EM Non-NN - fed_em em EM , FedEM Fed-ICE Non-NN - fed_ice ice FedICE Fed-MissForest Non-NN - fed_missforest ice MissForest , Fed Randomforest MIWAE NN fedavg , fedprox , fedavg_ft miwae jm MIWAE GAIN NN fedavg , fedprox , fedavg_ft gain jm GAIN Federated Strategies: Method Type Fed_strategy(code) Reference FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAvg-FT personalized FL fedavg_ft FedAvg-FT Environment Setup After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env.setup_from_simulator(simulator, verbose=1) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [np.random.rand(100, 10) for _ in range(10)] clients_train_data_ms = [np.random.rand(100, 10) for _ in range(10)] clients_test_data = [np.random.rand(100, 10) for _ in range(10)] global_test = np.random.rand(100, 10) data_config = { 'target': 9, 'task_type': 'regression', 'clf_type': None, 'num_cols': 9, } clients_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] env.setup_from_data( clients_train_data, clients_test_data, clients_train_data_ms, clients_seeds, global_test, data_config, verbose=1 ) Execute Federated Imputation After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. env.run_fed_imputation() Miscellaneous verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Federated Imputation"},{"location":"user-guide/fed_imp/#executing-federated-imputation-algorithms","text":"The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms.","title":"Executing Federated Imputation Algorithms"},{"location":"user-guide/fed_imp/#overview-and-basic-usage","text":"Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation()","title":"Overview and Basic Usage"},{"location":"user-guide/fed_imp/#environment-configuration","text":"The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation.","title":"Environment Configuration"},{"location":"user-guide/fed_imp/#supported-federated-imputation-algorithms","text":"Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Fed-Mean Non-NN - fed_mean simple - Fed-EM Non-NN - fed_em em EM , FedEM Fed-ICE Non-NN - fed_ice ice FedICE Fed-MissForest Non-NN - fed_missforest ice MissForest , Fed Randomforest MIWAE NN fedavg , fedprox , fedavg_ft miwae jm MIWAE GAIN NN fedavg , fedprox , fedavg_ft gain jm GAIN Federated Strategies: Method Type Fed_strategy(code) Reference FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAvg-FT personalized FL fedavg_ft FedAvg-FT","title":"Supported Federated Imputation Algorithms"},{"location":"user-guide/fed_imp/#environment-setup","text":"After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env.setup_from_simulator(simulator, verbose=1) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [np.random.rand(100, 10) for _ in range(10)] clients_train_data_ms = [np.random.rand(100, 10) for _ in range(10)] clients_test_data = [np.random.rand(100, 10) for _ in range(10)] global_test = np.random.rand(100, 10) data_config = { 'target': 9, 'task_type': 'regression', 'clf_type': None, 'num_cols': 9, } clients_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] env.setup_from_data( clients_train_data, clients_test_data, clients_train_data_ms, clients_seeds, global_test, data_config, verbose=1 )","title":"Environment Setup"},{"location":"user-guide/fed_imp/#execute-federated-imputation","text":"After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. env.run_fed_imputation()","title":"Execute Federated Imputation"},{"location":"user-guide/fed_imp/#miscellaneous","text":"verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Miscellaneous"},{"location":"user-guide/scenario_simulation/","text":"Simulating Federated Missing Data Scenarios In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section. Overview and Basic Usage The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) # or use the lite simulation function simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_scenario='mar-homo', dp_split_col_option = 'target', verbose=1 ) Classical Simulation Function - simulate_scenario The simulate_scenario method has the following major parameters for data partitioning and missing data simulation. Data Partitioning Parameters The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton. Missing Data Simulation Parameters The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type. Lite Simulation Function - Simluting with Predefined Strategies and Scenarios We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_scenario='mar-heter', dp_split_col_option = 'target', verbose=1 ) The simulate_scenario_lite method has the following major parameters for data partitioning and missing data simulation. Data Partitioning Options - dp_strategy iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir@<alpha> : Partition the data samples i.i.d across the clients with sample sizes follows dirichlet distribution with parameter alpha ,e.g. iid-dir@0.5 . niid-dir@<alpha> : Partition the data samples non-i.i.d across the clients with dirichlet distribution with parameter alpha ,e.g. niid-dir@0.5 niid-path@<k> : Partition the data samples non-i.i.d across the clients with pathological distribution with parameter k ,e.g. niid-path@2 . Missing Data Simulation Scenarios - ms_scenario mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"Federated Missing Data Scenario Simulation"},{"location":"user-guide/scenario_simulation/#simulating-federated-missing-data-scenarios","text":"In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section.","title":"Simulating Federated Missing Data Scenarios"},{"location":"user-guide/scenario_simulation/#overview-and-basic-usage","text":"The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) # or use the lite simulation function simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_scenario='mar-homo', dp_split_col_option = 'target', verbose=1 )","title":"Overview and Basic Usage"},{"location":"user-guide/scenario_simulation/#classical-simulation-function-simulate_scenario","text":"The simulate_scenario method has the following major parameters for data partitioning and missing data simulation.","title":"Classical Simulation Function - simulate_scenario"},{"location":"user-guide/scenario_simulation/#data-partitioning-parameters","text":"The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton.","title":"Data Partitioning Parameters"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-parameters","text":"The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type.","title":"Missing Data Simulation Parameters"},{"location":"user-guide/scenario_simulation/#lite-simulation-function-simluting-with-predefined-strategies-and-scenarios","text":"We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario_lite( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_scenario='mar-heter', dp_split_col_option = 'target', verbose=1 ) The simulate_scenario_lite method has the following major parameters for data partitioning and missing data simulation.","title":"Lite Simulation Function - Simluting with Predefined Strategies and Scenarios"},{"location":"user-guide/scenario_simulation/#data-partitioning-options-dp_strategy","text":"iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir@<alpha> : Partition the data samples i.i.d across the clients with sample sizes follows dirichlet distribution with parameter alpha ,e.g. iid-dir@0.5 . niid-dir@<alpha> : Partition the data samples non-i.i.d across the clients with dirichlet distribution with parameter alpha ,e.g. niid-dir@0.5 niid-path@<k> : Partition the data samples non-i.i.d across the clients with pathological distribution with parameter k ,e.g. niid-path@2 .","title":"Data Partitioning Options - dp_strategy"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-scenarios-ms_scenario","text":"mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"Missing Data Simulation Scenarios - ms_scenario"}]}