{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FedImpute FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios. Installation Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, } Step 2. Simulate Federated Missing Data Scenario from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) Step 3. Execute Federated Imputation Algorithms from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation() Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Home"},{"location":"#welcome-to-fedimpute","text":"FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios.","title":"Welcome to FedImpute"},{"location":"#installation","text":"Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"#basic-usage","text":"","title":"Basic Usage"},{"location":"#step-1-prepare-data","text":"import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, }","title":"Step 1. Prepare Data"},{"location":"#step-2-simulate-federated-missing-data-scenario","text":"from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 )","title":"Step 2. Simulate Federated Missing Data Scenario"},{"location":"#step-3-execute-federated-imputation-algorithms","text":"from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation()","title":"Step 3. Execute Federated Imputation Algorithms"},{"location":"#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Step 4. Evaluate imputation outcomes"},{"location":"about/","text":"Arbor mecum nostro Rhesi Illic spumam Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus! Est niveo serpentem sit Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail(office.monochrome(46), koffice_ascii, mailItunes); node += cable_bezel_menu; if (powerFatAtm.clipboard_text_vertical.bccKvmLeaf(dmaOverwriteAscii, plain) + passiveCameraWaveform(462879)) { gateway = multicasting(drive, hubMashupAvatar.floppy_zero_qwerty.rom_linux_log(digital, dllDebug, 2)); } Sit est ternisque inter Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Arbor mecum nostro Rhesi"},{"location":"about/#arbor-mecum-nostro-rhesi","text":"","title":"Arbor mecum nostro Rhesi"},{"location":"about/#illic-spumam","text":"Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus!","title":"Illic spumam"},{"location":"about/#est-niveo-serpentem-sit","text":"Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail(office.monochrome(46), koffice_ascii, mailItunes); node += cable_bezel_menu; if (powerFatAtm.clipboard_text_vertical.bccKvmLeaf(dmaOverwriteAscii, plain) + passiveCameraWaveform(462879)) { gateway = multicasting(drive, hubMashupAvatar.floppy_zero_qwerty.rom_linux_log(digital, dllDebug, 2)); }","title":"Est niveo serpentem sit"},{"location":"about/#sit-est-ternisque-inter","text":"Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Sit est ternisque inter"},{"location":"get_started/","text":"Installation Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, } Step 2. Simulate Federated Missing Data Scenario from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) Step 3. Execute Federated Imputation Algorithms from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation() Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Install & Basic Usage"},{"location":"get_started/#installation","text":"Install python >= 3.8.0 python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"get_started/#basic-usage","text":"","title":"Basic Usage"},{"location":"get_started/#step-1-prepare-data","text":"import numpy as np data = np.random.rand(10000, 10) data_config = { 'task_type': 'regression', 'num_cols': 9, }","title":"Step 1. Prepare Data"},{"location":"get_started/#step-2-simulate-federated-missing-data-scenario","text":"from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 )","title":"Step 2. Simulate Federated Missing Data Scenario"},{"location":"get_started/#step-3-execute-federated-imputation-algorithms","text":"from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv() env.configuration(imputer = 'gain', fed_strategy='fedavg', fit_mode = 'fed') env.setup_from_simulator(simulator = simulator, verbose=1) env.run_fed_imputation()","title":"Step 3. Execute Federated Imputation Algorithms"},{"location":"get_started/#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator() evaluator.evaluate(env, ['imp_quality', 'pred_downstream_local', 'pred_downstream_fed']) evaluator.show_results()","title":"Step 4. Evaluate imputation outcomes"},{"location":"overview/","text":"Overview of FedImpute FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment. Understanding Missing Data in Horizontal Federated Learning Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions. Types of Missing Data Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction . Missing Data Under Federated Learning Scenario Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees. Missing Data Imputation of Federated Datasets A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need. Features of FedImpute Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs. Architecture and Components FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms. License FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"Overview of FedImpute"},{"location":"overview/#overview-of-fedimpute","text":"FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment.","title":"Overview of FedImpute"},{"location":"overview/#understanding-missing-data-in-horizontal-federated-learning","text":"Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions.","title":"Understanding Missing Data in Horizontal Federated Learning"},{"location":"overview/#types-of-missing-data","text":"Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction .","title":"Types of Missing Data"},{"location":"overview/#missing-data-under-federated-learning-scenario","text":"Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees.","title":"Missing Data Under Federated Learning Scenario"},{"location":"overview/#missing-data-imputation-of-federated-datasets","text":"A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need.","title":"Missing Data Imputation of Federated Datasets"},{"location":"overview/#features-of-fedimpute","text":"Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs.","title":"Features of FedImpute"},{"location":"overview/#architecture-and-components","text":"FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms.","title":"Architecture and Components"},{"location":"overview/#license","text":"FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"License"},{"location":"api/fed_impute/","text":"","title":"Federated imputation"},{"location":"api/simulation/","text":"","title":"Scenario simulation"},{"location":"tutorial/basic_usage/","text":"","title":"Basic Usage"},{"location":"user-guide/data_prep/","text":"Dataset and Preprocessing The first step for using FedImpute is to prepare the data. Input Data Format and Preprocessing The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced. Required Preprocessing Steps There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ | Ordering Features To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset. One-hot Encoding Categorical Features Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute. Data Normalization (Optional) It is recommended to normalize the numerical features in the dataset within range of 0 and 1. Helper Functions for Preprocessing FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features, one_hot_encoding # Example for data with numpy array data = ... data = ordering_features(data, numerical_cols=[0, 1, 3, 4, 8], target_col=-1) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) # Example data with pandas dataframe data = ... data = ordering_features( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) Data Configuration Dictionary To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target': 'house_price', 'task_type': 'classification', 'clf_type': 'binary', 'num_cols': 10, } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Preparation"},{"location":"user-guide/data_prep/#dataset-and-preprocessing","text":"The first step for using FedImpute is to prepare the data.","title":"Dataset and Preprocessing"},{"location":"user-guide/data_prep/#input-data-format-and-preprocessing","text":"The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced.","title":"Input Data Format and Preprocessing"},{"location":"user-guide/data_prep/#required-preprocessing-steps","text":"There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ |","title":"Required Preprocessing Steps"},{"location":"user-guide/data_prep/#ordering-features","text":"To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset.","title":"Ordering Features"},{"location":"user-guide/data_prep/#one-hot-encoding-categorical-features","text":"Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute.","title":"One-hot Encoding Categorical Features"},{"location":"user-guide/data_prep/#data-normalization-optional","text":"It is recommended to normalize the numerical features in the dataset within range of 0 and 1.","title":"Data Normalization (Optional)"},{"location":"user-guide/data_prep/#helper-functions-for-preprocessing","text":"FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features, one_hot_encoding # Example for data with numpy array data = ... data = ordering_features(data, numerical_cols=[0, 1, 3, 4, 8], target_col=-1) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) # Example data with pandas dataframe data = ... data = ordering_features( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' ) data = one_hot_encoding(data, numerical_cols_num=5, max_cateogories=10) ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data( data, numerical_cols=['age', 'income', 'height', 'weight', 'temperature'], target_col='house_price' )","title":"Helper Functions for Preprocessing"},{"location":"user-guide/data_prep/#data-configuration-dictionary","text":"To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target': 'house_price', 'task_type': 'classification', 'clf_type': 'binary', 'num_cols': 10, } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Configuration Dictionary"},{"location":"user-guide/evaluation/","text":"","title":"Evaluation of Imputation Outcome"},{"location":"user-guide/extend_guide/","text":"","title":"Extend guide"},{"location":"user-guide/fed_imp/","text":"","title":"Federated Imputation"},{"location":"user-guide/scenario_simulation/","text":"Simulating Federated Missing Data Scenarios In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section. Overview and Basic Usage The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 ) Classical Simulation Function - simulate_scenario The simulate_scenario method has the following major parameters for data partitioning and missing data simulation. Data Partitioning Parameters The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton. Missing Data Simulation Parameters The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type. Lite Simulation Function - Simluting with Predefined Strategies and Scenarios We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use.","title":"Federated Missing Data Scenario Simulation"},{"location":"user-guide/scenario_simulation/#simulating-federated-missing-data-scenarios","text":"In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section.","title":"Simulating Federated Missing Data Scenarios"},{"location":"user-guide/scenario_simulation/#overview-and-basic-usage","text":"The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator() simulation_results = simulator.simulate_scenario( data, data_config, num_clients = 10, dp_strategy='iid-even', ms_mech_type='mcar', verbose=1 )","title":"Overview and Basic Usage"},{"location":"user-guide/scenario_simulation/#classical-simulation-function-simulate_scenario","text":"The simulate_scenario method has the following major parameters for data partitioning and missing data simulation.","title":"Classical Simulation Function - simulate_scenario"},{"location":"user-guide/scenario_simulation/#data-partitioning-parameters","text":"The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton.","title":"Data Partitioning Parameters"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-parameters","text":"The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type.","title":"Missing Data Simulation Parameters"},{"location":"user-guide/scenario_simulation/#lite-simulation-function-simluting-with-predefined-strategies-and-scenarios","text":"We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use.","title":"Lite Simulation Function - Simluting with Predefined Strategies and Scenarios"}]}