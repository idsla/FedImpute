{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FedImpute FedImpute is an open-source framework implemented in Python for distributed imputation research for horizontally partitioned data. Features of FedImpute Flexible Distributed Missing Data Scenario Builder : Provide flexible API to construct distributedmissing data scenarios under various missing data distribution and data partition strategies or naturally distributed data with existing missingness. Built-in Distributed Imputation Methods : Supports multiple distributed imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with distributed imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs. Quick Start Overview Installation and Basic Usage User Guide Data Preparation Distributed Missing Data Scenario Builder Distributed Imputation Evaluation Tutorials Support and Contact FedImpute is developed by Rutgers Institute for Data Science, Learning, and Applications (i-DLSA) , lead by Professor Jaideep Vaidya . For any questions, please contact Sitao Min for support.","title":"Home"},{"location":"#welcome-to-fedimpute","text":"FedImpute is an open-source framework implemented in Python for distributed imputation research for horizontally partitioned data.","title":"Welcome to FedImpute"},{"location":"#features-of-fedimpute","text":"Flexible Distributed Missing Data Scenario Builder : Provide flexible API to construct distributedmissing data scenarios under various missing data distribution and data partition strategies or naturally distributed data with existing missingness. Built-in Distributed Imputation Methods : Supports multiple distributed imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with distributed imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs.","title":"Features of FedImpute"},{"location":"#quick-start","text":"Overview Installation and Basic Usage","title":"Quick Start"},{"location":"#user-guide","text":"Data Preparation Distributed Missing Data Scenario Builder Distributed Imputation Evaluation","title":"User Guide"},{"location":"#tutorials","text":"","title":"Tutorials"},{"location":"#support-and-contact","text":"FedImpute is developed by Rutgers Institute for Data Science, Learning, and Applications (i-DLSA) , lead by Professor Jaideep Vaidya . For any questions, please contact Sitao Min for support.","title":"Support and Contact"},{"location":"get_started/","text":"Installation Firstly, install python >= 3.10.0, we have two ways to install Install from pip: pip install fedimpute Install from package repo: git clone https://github.com/idsla/FedImpute cd FedImpute # create virtual env python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate # Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) # Structure of data_config: # { # 'target': 'y', # 'task_type': 'classification', # 'natural_partition': False # } data . head () Step 2. Build Distributed Missing Data Scenario from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) scenario_builder . summarize_scenario () Step 3. Execute Distributed Imputation Algorithms Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation () Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all ()","title":"Install & Basic Usage"},{"location":"get_started/#installation","text":"Firstly, install python >= 3.10.0, we have two ways to install Install from pip: pip install fedimpute Install from package repo: git clone https://github.com/idsla/FedImpute cd FedImpute # create virtual env python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate # Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"get_started/#basic-usage","text":"","title":"Basic Usage"},{"location":"get_started/#step-1-prepare-data","text":"from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) # Structure of data_config: # { # 'target': 'y', # 'task_type': 'classification', # 'natural_partition': False # } data . head ()","title":"Step 1. Prepare Data"},{"location":"get_started/#step-2-build-distributed-missing-data-scenario","text":"from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) scenario_builder . summarize_scenario ()","title":"Step 2. Build Distributed Missing Data Scenario"},{"location":"get_started/#step-3-execute-distributed-imputation-algorithms","text":"Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ()","title":"Step 3. Execute Distributed Imputation Algorithms"},{"location":"get_started/#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all ()","title":"Step 4. Evaluate imputation outcomes"},{"location":"overview/","text":"Overview of FedImpute FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in distributed environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under distributed data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under distributed learning evironment. Understanding Missing Data in Horizontal Federated Learning Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions. Types of Missing Data Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction . Missing Data Under Distributed Learning Scenario Horizontal distributed data involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees. Missing Data Imputation of Federated Datasets A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under horizontally partitioned data, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common distributed learning solutions to allow for collaborative imputation without exposing individual data points, we call it Distributed Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of distributed imputation algorithms under different missing data scenarios. FedImpute is designed to address this need. Architecture and Components FedImpute is built on top of the following components: Distributed Missing Data Scenario Builder , Distributed Imputation Execution Environment , and Evaluation . Distributed Missing Data Scenario Builder : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Distributed Imputation Execution Environment : This component executes distributed imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms. License FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"Overview of FedImpute"},{"location":"overview/#overview-of-fedimpute","text":"FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in distributed environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under distributed data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under distributed learning evironment.","title":"Overview of FedImpute"},{"location":"overview/#understanding-missing-data-in-horizontal-federated-learning","text":"Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions.","title":"Understanding Missing Data in Horizontal Federated Learning"},{"location":"overview/#types-of-missing-data","text":"Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction .","title":"Types of Missing Data"},{"location":"overview/#missing-data-under-distributed-learning-scenario","text":"Horizontal distributed data involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees.","title":"Missing Data Under Distributed Learning Scenario"},{"location":"overview/#missing-data-imputation-of-federated-datasets","text":"A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under horizontally partitioned data, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common distributed learning solutions to allow for collaborative imputation without exposing individual data points, we call it Distributed Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of distributed imputation algorithms under different missing data scenarios. FedImpute is designed to address this need.","title":"Missing Data Imputation of Federated Datasets"},{"location":"overview/#architecture-and-components","text":"FedImpute is built on top of the following components: Distributed Missing Data Scenario Builder , Distributed Imputation Execution Environment , and Evaluation . Distributed Missing Data Scenario Builder : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Distributed Imputation Execution Environment : This component executes distributed imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms.","title":"Architecture and Components"},{"location":"overview/#license","text":"FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"License"},{"location":"api/fed_impute/","text":"Federated Imputation Execution Environment Environment fedimpute . execution_environment . env_fedimp .FedImputeEnv class FedImputeEnv ( debug_mode : bool = False ) Methods configuration setup_from_data setup_from_scenario_builder clear_env run_fed_imputation show_env_info save_env load_env reset_env get_data fedimpute . execution_environment . loaders . register . Register .register_imputer method Register . register_imputer ( imputer_name : str , imputer_class : Union [ BaseMLImputer , BaseNNImputer ] , workflow_name : str , supported_fed_strategies : List [str] ) Register a new imputer to the environment Parameters imputer_name : str \u2014 str, name of the imputer imputer_class : Union [ BaseMLImputer , BaseNNImputer ] \u2014 Union[BaseMLImputer, BaseNNImputer], class of the imputer workflow_name : str \u2014 str, name of the workflow supported_fed_strategies : List [str] \u2014 List[str], list of supported federated strategies Note This function will register the imputer to the environment. Raises ValueError fedimpute . execution_environment . loaders . register . Register .register_workflow method Register . register_workflow ( workflow_name : str , workflow_class : BaseWorkflow ) Register a new workflow to the environment Parameters workflow_name : str \u2014 str, name of the workflow workflow_class : BaseWorkflow \u2014 BaseWorkflow, class of the workflow Note This function will register the workflow to the environment. Raises ValueError fedimpute . execution_environment . loaders . register . Register .register_strategy method Register . register_strategy ( strategy_name : str , strategy_client : StrategyBaseClient , strategy_server : Union [ RawBaseStrategyServer , NNStrategyBaseServer ] ) Register a new strategy to the environment Parameters strategy_name : str \u2014 str, name of the strategy strategy_client : StrategyBaseClient \u2014 StrategyBaseClient, class of the strategy client strategy_server : Union [ RawBaseStrategyServer , NNStrategyBaseServer ] \u2014 Union[RawBaseStrategyServer, NNStrategyBaseServer], class of the strategy server Note This function will register the strategy to the environment. Raises ValueError Client fedimpute . execution_environment . client . client .Client class Client ( client_id : int , train_data : np .ndarray , test_data : np .ndarray , X_train_ms : np .ndarray , data_config : dict , imp_model_name , imp_model_params , fed_strategy : str , fed_strategy_params : dict , client_config : dict , columns : List [str] , register : Register , seed = 0 ) Client class presenting a client in the federated imputation execution environment, it contains the training and testing data, missing data, imputed data, imputation model class, and federated strategy class. Attributes client_id : int \u2014 client id X_train : np .ndarray \u2014 training data y_train : np .ndarray \u2014 training labels X_test : np .ndarray \u2014 testing data y_test : np .ndarray \u2014 testing labels X_train_ms : np .ndarray \u2014 missing data X_train_mask : np .ndarray \u2014 missing data mask X_train_imp : np .ndarray \u2014 imputed data data_utils : dict \u2014 data statistics imputer : BaseImputer \u2014 imputation model fed_strategy : BaseFedStrategy \u2014 federated strategy seed : int \u2014 seed client_config : dict \u2014 client configuration Methods initial_impute \u2014 Initial imputation fit_local_imp_model \u2014 Fit a local imputation model update_local_imp_model \u2014 Fit a local imputation model local_imputation \u2014 Perform local imputation save_imp_model \u2014 Save imputation model load_imp_model \u2014 Save imputation model calculate_data_utils \u2014 Calculate data statistic profile fedimpute . execution_environment . client . client . Client .initial_impute method Client . initial_impute ( imp_values : np .ndarray , col_type : str = 'num' ) \u2192 None Initial imputation Parameters imp_values : np .ndarray \u2014 imputation values col_type : str \u2014 column type, 'num' or 'cat' fedimpute . execution_environment . client . client . Client .fit_local_imp_model method Client . fit_local_imp_model ( params : dict ) \u2192 Tuple [ Union [dict, torch . nn . Module ], dict] Fit a local imputation model Parameters params : dict \u2014 instructions for fitting the imputation model Returns Tuple [ Union [dict, torch . nn . Module ], dict] \u2014 model parameters and fitting results dictionary fedimpute . execution_environment . client . client . Client .local_imputation method Client . local_imputation ( params : dict ) \u2192 Union [None, np .ndarray] Perform local imputation Parameters params : dict \u2014 instructions for imputation - e.g temp_imp for temporary imputation Returns Union [None, np .ndarray] \u2014 imputed data or None Server fedimpute . execution_environment . server . server .Server class Server ( fed_strategy_name : str , fed_strategy_params : dict , imputer_name : str , imputer_params : dict , global_test : np .ndarray , data_config : dict , server_config : Dict [str, Union [str, int, float]] , seed : int = 21 , columns : List [str] = None , register : Register = None ) Server class to be used in the federated imputation environment Attributes fed_strategy \u2014 str - name of the federated strategy fed_strategy_params \u2014 dict - parameters of the federated strategy server_config \u2014 dict - configuration of the server X_test_global \u2014 np.ndarray - global test data Methods initial_impute \u2014 Initial imputation local_imputation \u2014 Perform local imputation calculate_data_utils profile Workflow fedimpute . execution_environment . workflows . workflow .BaseWorkflow class BaseWorkflow ( name : str ) Bases : ABC Abstract class for the workflow to be used in the federated imputation environment Methods fed_imp_sequential \u2014 Sequential federated imputation workflow fed_imp_parallel \u2014 Parallel federated imputation workflow run_fed_imp \u2014 Run the federated imputation workflow based on the eval_and_track eval_and_track_parallel fedimpute . execution_environment . workflows . workflow . BaseWorkflow .run_fed_imp method BaseWorkflow . run_fed_imp ( clients : List [ Client ] , server : Server , evaluator , tracker : Tracker , run_type : str , verbose : int ) \u2192 Tracker Run the federated imputation workflow based on the Parameters clients : List [ Client ] \u2014 List Client - list of clients server : Server \u2014 Server - server evaluator \u2014 Evaluator - evaluator tracker : Tracker \u2014 Tracker - tracker to tracking results run_type : str \u2014 str - type of the workflow run (sequential or parallel) Returns Tracker \u2014 Tracker - tracker with tracked results Raises ValueError","title":"Imputation Execution Environment"},{"location":"api/fed_impute/#federated-imputation-execution-environment","text":"","title":"Federated Imputation Execution Environment"},{"location":"api/fed_impute/#environment","text":"fedimpute . execution_environment . env_fedimp .FedImputeEnv class FedImputeEnv ( debug_mode : bool = False ) Methods configuration setup_from_data setup_from_scenario_builder clear_env run_fed_imputation show_env_info save_env load_env reset_env get_data fedimpute . execution_environment . loaders . register . Register .register_imputer method Register . register_imputer ( imputer_name : str , imputer_class : Union [ BaseMLImputer , BaseNNImputer ] , workflow_name : str , supported_fed_strategies : List [str] ) Register a new imputer to the environment Parameters imputer_name : str \u2014 str, name of the imputer imputer_class : Union [ BaseMLImputer , BaseNNImputer ] \u2014 Union[BaseMLImputer, BaseNNImputer], class of the imputer workflow_name : str \u2014 str, name of the workflow supported_fed_strategies : List [str] \u2014 List[str], list of supported federated strategies Note This function will register the imputer to the environment. Raises ValueError fedimpute . execution_environment . loaders . register . Register .register_workflow method Register . register_workflow ( workflow_name : str , workflow_class : BaseWorkflow ) Register a new workflow to the environment Parameters workflow_name : str \u2014 str, name of the workflow workflow_class : BaseWorkflow \u2014 BaseWorkflow, class of the workflow Note This function will register the workflow to the environment. Raises ValueError fedimpute . execution_environment . loaders . register . Register .register_strategy method Register . register_strategy ( strategy_name : str , strategy_client : StrategyBaseClient , strategy_server : Union [ RawBaseStrategyServer , NNStrategyBaseServer ] ) Register a new strategy to the environment Parameters strategy_name : str \u2014 str, name of the strategy strategy_client : StrategyBaseClient \u2014 StrategyBaseClient, class of the strategy client strategy_server : Union [ RawBaseStrategyServer , NNStrategyBaseServer ] \u2014 Union[RawBaseStrategyServer, NNStrategyBaseServer], class of the strategy server Note This function will register the strategy to the environment. Raises ValueError","title":"Environment"},{"location":"api/fed_impute/#client","text":"fedimpute . execution_environment . client . client .Client class Client ( client_id : int , train_data : np .ndarray , test_data : np .ndarray , X_train_ms : np .ndarray , data_config : dict , imp_model_name , imp_model_params , fed_strategy : str , fed_strategy_params : dict , client_config : dict , columns : List [str] , register : Register , seed = 0 ) Client class presenting a client in the federated imputation execution environment, it contains the training and testing data, missing data, imputed data, imputation model class, and federated strategy class. Attributes client_id : int \u2014 client id X_train : np .ndarray \u2014 training data y_train : np .ndarray \u2014 training labels X_test : np .ndarray \u2014 testing data y_test : np .ndarray \u2014 testing labels X_train_ms : np .ndarray \u2014 missing data X_train_mask : np .ndarray \u2014 missing data mask X_train_imp : np .ndarray \u2014 imputed data data_utils : dict \u2014 data statistics imputer : BaseImputer \u2014 imputation model fed_strategy : BaseFedStrategy \u2014 federated strategy seed : int \u2014 seed client_config : dict \u2014 client configuration Methods initial_impute \u2014 Initial imputation fit_local_imp_model \u2014 Fit a local imputation model update_local_imp_model \u2014 Fit a local imputation model local_imputation \u2014 Perform local imputation save_imp_model \u2014 Save imputation model load_imp_model \u2014 Save imputation model calculate_data_utils \u2014 Calculate data statistic profile fedimpute . execution_environment . client . client . Client .initial_impute method Client . initial_impute ( imp_values : np .ndarray , col_type : str = 'num' ) \u2192 None Initial imputation Parameters imp_values : np .ndarray \u2014 imputation values col_type : str \u2014 column type, 'num' or 'cat' fedimpute . execution_environment . client . client . Client .fit_local_imp_model method Client . fit_local_imp_model ( params : dict ) \u2192 Tuple [ Union [dict, torch . nn . Module ], dict] Fit a local imputation model Parameters params : dict \u2014 instructions for fitting the imputation model Returns Tuple [ Union [dict, torch . nn . Module ], dict] \u2014 model parameters and fitting results dictionary fedimpute . execution_environment . client . client . Client .local_imputation method Client . local_imputation ( params : dict ) \u2192 Union [None, np .ndarray] Perform local imputation Parameters params : dict \u2014 instructions for imputation - e.g temp_imp for temporary imputation Returns Union [None, np .ndarray] \u2014 imputed data or None","title":"Client"},{"location":"api/fed_impute/#server","text":"fedimpute . execution_environment . server . server .Server class Server ( fed_strategy_name : str , fed_strategy_params : dict , imputer_name : str , imputer_params : dict , global_test : np .ndarray , data_config : dict , server_config : Dict [str, Union [str, int, float]] , seed : int = 21 , columns : List [str] = None , register : Register = None ) Server class to be used in the federated imputation environment Attributes fed_strategy \u2014 str - name of the federated strategy fed_strategy_params \u2014 dict - parameters of the federated strategy server_config \u2014 dict - configuration of the server X_test_global \u2014 np.ndarray - global test data Methods initial_impute \u2014 Initial imputation local_imputation \u2014 Perform local imputation calculate_data_utils profile","title":"Server"},{"location":"api/fed_impute/#workflow","text":"fedimpute . execution_environment . workflows . workflow .BaseWorkflow class BaseWorkflow ( name : str ) Bases : ABC Abstract class for the workflow to be used in the federated imputation environment Methods fed_imp_sequential \u2014 Sequential federated imputation workflow fed_imp_parallel \u2014 Parallel federated imputation workflow run_fed_imp \u2014 Run the federated imputation workflow based on the eval_and_track eval_and_track_parallel fedimpute . execution_environment . workflows . workflow . BaseWorkflow .run_fed_imp method BaseWorkflow . run_fed_imp ( clients : List [ Client ] , server : Server , evaluator , tracker : Tracker , run_type : str , verbose : int ) \u2192 Tracker Run the federated imputation workflow based on the Parameters clients : List [ Client ] \u2014 List Client - list of clients server : Server \u2014 Server - server evaluator \u2014 Evaluator - evaluator tracker : Tracker \u2014 Tracker - tracker to tracking results run_type : str \u2014 str - type of the workflow run (sequential or parallel) Returns Tracker \u2014 Tracker - tracker with tracked results Raises ValueError","title":"Workflow"},{"location":"api/imputation_models/","text":"Imputation Models Non-NN Based Imputer fedimpute . execution_environment . imputation . base . base_imputer .BaseMLImputer class BaseMLImputer ( name : str , model_persistable : bool ) Abstract class for the non-NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .initialize method BaseMLImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 int - seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .fit method BaseMLImputer . fit ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 dict Fit imputer to train local imputation models Parameters X : np .array \u2014 np.array - float numpy array features y : np .array \u2014 np.array - target missing_mask : np .array \u2014 np.array - missing mask params : dict \u2014 parameters for local training fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .impute method BaseMLImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X Mean fedimpute . execution_environment . imputation . imputers . simple_imputer .SimpleImputer class SimpleImputer ( strategy : str = 'mean' ) Bases : BaseMLImputer Simple imputer class for imputing missing values in data using simple strategies like mean, median etc. Attributes strategy : str \u2014 strategy for imputation - mean, median etc. mean_params : np .array \u2014 mean parameters for imputation model_type : str \u2014 type of the model - numpy or sklearn model_persistable : bool \u2014 whether model is persistable or not name : str \u2014 name of the imputer Raises ValueError Methods get_imp_model_params set_imp_model_params initialize fit impute get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model EM fedimpute . execution_environment . imputation . imputers . em_imputer .EMImputer class EMImputer ( clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin EM imputer class for imputing missing values in data using Expectation Maximization algorithm. Attributes clip \u2014 bool - whether to clip the imputed values use_y \u2014 bool - whether to use target variable in imputation min_values \u2014 np.array - minimum values for clipping max_values \u2014 np.array - maximum values for clipping data_utils_info \u2014 dict - information about data seed \u2014 int - seed for randomization name \u2014 str = 'em' - name of the imputer model_type \u2014 str = 'simple' - type of the imputer - simple or nn - neural network based or not mu \u2014 np.array - mean of the data sigma \u2014 np.array - covariance matrix of the data miss \u2014 np.array - missing values indices obs \u2014 np.array - observed values indices model_persistable \u2014 bool - whether model is persistable or not Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit the imputer on the data. impute \u2014 Impute the missing values in the data. get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds get_visit_indices _em \u2014 Perform the EM step for imputing missing values. _converged \u2014 Checks if the EM loop has converged. ICE fedimpute . execution_environment . imputation . imputers . linear_ice_imputer .LinearICEImputer class LinearICEImputer ( estimator_num : str = 'ridge_cv' , estimator_cat : str = 'logistic' , mm_model : str = 'logistic' , mm_model_params = None , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin Linear ICE imputer class for imputing missing values in data using linear models. Attributes estimator_num : str \u2014 estimator for numerical columns estimator_cat : str \u2014 estimator for categorical columns mm_model \u2014 missing mechanism model mm_model_params : dict \u2014 missing mechanism model parameters clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target variable in imputation imp_models : list \u2014 list of imputation models data_utils_info : dict \u2014 information about data seed : int \u2014 seed for randomization model_type : str \u2014 type of the imputer - simple or nn - neural network based or not, defaults to 'sklearn' model_persistable : bool \u2014 whether model is persistable or not, defaults to False name : str \u2014 name of the imputer, defaults to 'linear_ice' Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model get_fit_res save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices MissForest fedimpute . execution_environment . imputation . imputers . missforest_imputer .MissForestImputer class MissForestImputer ( n_estimators : int = 200 , bootstrap : bool = True , n_jobs : int = 2 , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin MissForest imputer class for the federated imputation environment Attributes n_estimators : int \u2014 number of trees in the forest bootstrap : bool \u2014 whether bootstrap samples are used when building trees n_jobs : int \u2014 number of jobs to run in parallel clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target values for imputation imp_models : list \u2014 list of imputation models mm_model : object \u2014 model for missing mask imputation data_utils_info : dict \u2014 data utils information seed : int \u2014 seed for randomization model_type : str \u2014 type of the model, defaults to 'sklearn' model_persistable : bool \u2014 whether the model is persistable, defaults to False name : str \u2014 name of the imputer, defaults to 'missforest' Methods get_imp_model_params set_imp_model_params initialize fit impute get_fit_res save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices NN Based Imputer fedimpute . execution_environment . imputation . base . base_imputer .BaseNNImputer class BaseNNImputer ( ) Abstract class for the NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. configure_model \u2014 Fetch model for training configure_optimizer \u2014 Configure optimizer for training impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .initialize method BaseNNImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .configure_model method BaseNNImputer . configure_model ( params : dict , X : np .ndarray , y : np .ndarray , missing_mask : np .ndarray ) \u2192 Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] Fetch model for training Parameters params : dict \u2014 parameters for training X : np .ndarray \u2014 imputed data y : np .ndarray \u2014 target missing_mask : np .ndarray \u2014 missing mask Returns Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] \u2014 model, train_dataloader fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .impute method BaseNNImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X GAIN fedimpute . execution_environment . imputation . imputers . gain_imputer .GAINImputer class GAINImputer ( h_dim : int = 20 , n_layers : int = 2 , activation : str = 'relu' , initializer : str = 'kaiming' , loss_alpha : float = 10 , hint_rate : float = 0.9 , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin GAIN imputer class for imputing missing values in data using Generative Adversarial Imputation Networks. Attributes h_dim : int \u2014 dimension of hidden layers n_layers : int \u2014 number of layers activation : str \u2014 activation function initializer : str \u2014 initializer for weights loss_alpha : float \u2014 alpha parameter for loss hint_rate : float \u2014 hint rate for loss clip : bool \u2014 whether to clip the imputed values batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training scheduler_params : dict \u2014 scheduler parameters Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds MIWAE fedimpute . execution_environment . imputation . imputers . miwae_imputer .MIWAEImputer class MIWAEImputer ( name : str = 'miwae' , latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit NOTMIWAE fedimpute . execution_environment . imputation . imputers . notmiwae_imputer .NotMIWAEImputer class NotMIWAEImputer ( latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , mask_net_type : str = 'linear' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit GNR fedimpute . execution_environment . imputation . imputers . gnr_imputer .GNRImputer class GNRImputer ( latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , loss_coef = 10 , mr_loss_coef : bool = True , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"Imputation Models"},{"location":"api/imputation_models/#imputation-models","text":"","title":"Imputation Models"},{"location":"api/imputation_models/#non-nn-based-imputer","text":"fedimpute . execution_environment . imputation . base . base_imputer .BaseMLImputer class BaseMLImputer ( name : str , model_persistable : bool ) Abstract class for the non-NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .initialize method BaseMLImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 int - seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .fit method BaseMLImputer . fit ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 dict Fit imputer to train local imputation models Parameters X : np .array \u2014 np.array - float numpy array features y : np .array \u2014 np.array - target missing_mask : np .array \u2014 np.array - missing mask params : dict \u2014 parameters for local training fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .impute method BaseMLImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X","title":"Non-NN Based Imputer"},{"location":"api/imputation_models/#mean","text":"fedimpute . execution_environment . imputation . imputers . simple_imputer .SimpleImputer class SimpleImputer ( strategy : str = 'mean' ) Bases : BaseMLImputer Simple imputer class for imputing missing values in data using simple strategies like mean, median etc. Attributes strategy : str \u2014 strategy for imputation - mean, median etc. mean_params : np .array \u2014 mean parameters for imputation model_type : str \u2014 type of the model - numpy or sklearn model_persistable : bool \u2014 whether model is persistable or not name : str \u2014 name of the imputer Raises ValueError Methods get_imp_model_params set_imp_model_params initialize fit impute get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model","title":"Mean"},{"location":"api/imputation_models/#em","text":"fedimpute . execution_environment . imputation . imputers . em_imputer .EMImputer class EMImputer ( clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin EM imputer class for imputing missing values in data using Expectation Maximization algorithm. Attributes clip \u2014 bool - whether to clip the imputed values use_y \u2014 bool - whether to use target variable in imputation min_values \u2014 np.array - minimum values for clipping max_values \u2014 np.array - maximum values for clipping data_utils_info \u2014 dict - information about data seed \u2014 int - seed for randomization name \u2014 str = 'em' - name of the imputer model_type \u2014 str = 'simple' - type of the imputer - simple or nn - neural network based or not mu \u2014 np.array - mean of the data sigma \u2014 np.array - covariance matrix of the data miss \u2014 np.array - missing values indices obs \u2014 np.array - observed values indices model_persistable \u2014 bool - whether model is persistable or not Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit the imputer on the data. impute \u2014 Impute the missing values in the data. get_fit_res save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds get_visit_indices _em \u2014 Perform the EM step for imputing missing values. _converged \u2014 Checks if the EM loop has converged.","title":"EM"},{"location":"api/imputation_models/#ice","text":"fedimpute . execution_environment . imputation . imputers . linear_ice_imputer .LinearICEImputer class LinearICEImputer ( estimator_num : str = 'ridge_cv' , estimator_cat : str = 'logistic' , mm_model : str = 'logistic' , mm_model_params = None , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin Linear ICE imputer class for imputing missing values in data using linear models. Attributes estimator_num : str \u2014 estimator for numerical columns estimator_cat : str \u2014 estimator for categorical columns mm_model \u2014 missing mechanism model mm_model_params : dict \u2014 missing mechanism model parameters clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target variable in imputation imp_models : list \u2014 list of imputation models data_utils_info : dict \u2014 information about data seed : int \u2014 seed for randomization model_type : str \u2014 type of the imputer - simple or nn - neural network based or not, defaults to 'sklearn' model_persistable : bool \u2014 whether model is persistable or not, defaults to False name : str \u2014 name of the imputer, defaults to 'linear_ice' Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model get_fit_res save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices","title":"ICE"},{"location":"api/imputation_models/#missforest","text":"fedimpute . execution_environment . imputation . imputers . missforest_imputer .MissForestImputer class MissForestImputer ( n_estimators : int = 200 , bootstrap : bool = True , n_jobs : int = 2 , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin MissForest imputer class for the federated imputation environment Attributes n_estimators : int \u2014 number of trees in the forest bootstrap : bool \u2014 whether bootstrap samples are used when building trees n_jobs : int \u2014 number of jobs to run in parallel clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target values for imputation imp_models : list \u2014 list of imputation models mm_model : object \u2014 model for missing mask imputation data_utils_info : dict \u2014 data utils information seed : int \u2014 seed for randomization model_type : str \u2014 type of the model, defaults to 'sklearn' model_persistable : bool \u2014 whether the model is persistable, defaults to False name : str \u2014 name of the imputer, defaults to 'missforest' Methods get_imp_model_params set_imp_model_params initialize fit impute get_fit_res save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices","title":"MissForest"},{"location":"api/imputation_models/#nn-based-imputer","text":"fedimpute . execution_environment . imputation . base . base_imputer .BaseNNImputer class BaseNNImputer ( ) Abstract class for the NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. configure_model \u2014 Fetch model for training configure_optimizer \u2014 Configure optimizer for training impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .initialize method BaseNNImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .configure_model method BaseNNImputer . configure_model ( params : dict , X : np .ndarray , y : np .ndarray , missing_mask : np .ndarray ) \u2192 Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] Fetch model for training Parameters params : dict \u2014 parameters for training X : np .ndarray \u2014 imputed data y : np .ndarray \u2014 target missing_mask : np .ndarray \u2014 missing mask Returns Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] \u2014 model, train_dataloader fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .impute method BaseNNImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X","title":"NN Based Imputer"},{"location":"api/imputation_models/#gain","text":"fedimpute . execution_environment . imputation . imputers . gain_imputer .GAINImputer class GAINImputer ( h_dim : int = 20 , n_layers : int = 2 , activation : str = 'relu' , initializer : str = 'kaiming' , loss_alpha : float = 10 , hint_rate : float = 0.9 , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin GAIN imputer class for imputing missing values in data using Generative Adversarial Imputation Networks. Attributes h_dim : int \u2014 dimension of hidden layers n_layers : int \u2014 number of layers activation : str \u2014 activation function initializer : str \u2014 initializer for weights loss_alpha : float \u2014 alpha parameter for loss hint_rate : float \u2014 hint rate for loss clip : bool \u2014 whether to clip the imputed values batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training scheduler_params : dict \u2014 scheduler parameters Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds","title":"GAIN"},{"location":"api/imputation_models/#miwae","text":"fedimpute . execution_environment . imputation . imputers . miwae_imputer .MIWAEImputer class MIWAEImputer ( name : str = 'miwae' , latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"MIWAE"},{"location":"api/imputation_models/#notmiwae","text":"fedimpute . execution_environment . imputation . imputers . notmiwae_imputer .NotMIWAEImputer class NotMIWAEImputer ( latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , mask_net_type : str = 'linear' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"NOTMIWAE"},{"location":"api/imputation_models/#gnr","text":"fedimpute . execution_environment . imputation . imputers . gnr_imputer .GNRImputer class GNRImputer ( latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , loss_coef = 10 , mr_loss_coef : bool = True , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"GNR"},{"location":"api/simulation/","text":"Simulator fedimpute . scenario . scenario_builder .ScenarioBuilder class ScenarioBuilder ( debug_mode : bool = False ) ScenarioBuilder class for simulating or constructing missing data scenarios in federated learning environment Attributes data : np .ndarray \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary clients_train_data : List [ np .ndarray] \u2014 list of clients training data clients_test_data : List [ np .ndarray] \u2014 list of clients test data clients_train_data_ms : List [ np .ndarray] \u2014 list of clients training data with missing values global_test : np .ndarray \u2014 global test data client_seeds : List [int] \u2014 list of seeds for clients stats : dict \u2014 simulation statistics debug_mode : bool \u2014 whether to enable debug mode Methods create_simulated_scenario \u2014 Simulate missing data scenario create_simulated_scenario_lite \u2014 Simulate missing data scenario create_real_scenario \u2014 Create a real scenario from a list of pandas DataFrames save load export_data summarize_scenario show_missing_data_details visualize_missing_pattern visualize_missing_distribution visualize_data_heterogeneity Build Simulated Scenario Function fedimpute . scenario . scenario_builder . ScenarioBuilder .create_simulated_scenario method ScenarioBuilder . create_simulated_scenario ( data : Union [ np .array, pd . DataFrame ] , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , dp_split_cols : Union [str, int] = 'target' , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 2000 , dp_sample_iid_direct : bool = False , dp_local_test_size : float = 0.1 , dp_global_test_size : float = 0.1 , dp_local_backup_size : float = 0.05 , dp_reg_bins : int = 50 , ms_scenario : str = None , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , ms_mech_type : str = 'mcar' , ms_global_mechanism : bool = False , ms_mr_dist_clients : str = 'randu' , ms_mf_dist_clients : str = 'identity' , ms_mm_dist_clients : str = 'random' , ms_missing_features : str = 'all' , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , ms_mm_funcs_bank : str = 'lr' , ms_mm_strictness : bool = True , ms_mm_obs : bool = False , ms_mm_feature_option : str = 'allk=0.2' , ms_mm_beta_option : str = None , seed : int = 100330201 , verbose : int = 0 ) \u2192 Dict [str, List [ np .ndarray]] Simulate missing data scenario Parameters data : Union [ np .array, pd . DataFrame ] \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path dp_split_cols : Union [str, int, List [int]] \u2014 split columns option - target , feature , default: target dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 2000 dp_sample_iid_direct : bool \u2014 sample iid data directly, default: False dp_local_test_size : float \u2014 local test size ratio, default: 0.1 dp_global_test_size : float \u2014 global test size ratio, default: 0.1 dp_local_backup_size : float \u2014 local backup size ratio, default: 0.05 dp_reg_bins : int \u2014 regression bins, default: 50 ms_mech_type : str \u2014 missing mechanism type, default: 'mcar' - mcar , mar_sigmoid , mnar_sigmoid , mar_quantile , mnar_quantile ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - all , all-num , random obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - random , rest ms_global_mechanism : bool \u2014 global missing mechanism, default: False ms_mr_dist_clients : str \u2014 missing ratio distribution, default: 'randu-int' - 'fixed', 'uniform', 'uniform_int', 'gaussian', 'gaussian_int' ms_mf_dist_clients : str \u2014 missing features distribution, default: 'identity' - 'identity', 'random', 'random2' ms_mm_dist_clients : str \u2014 missing mechanism functions distribution, default: 'random' - 'identity', 'random', 'random2' ms_missing_features : str \u2014 missing features strategy, default: 'all' - 'all', 'all-num' ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 ms_mm_funcs_bank : str \u2014 missing mechanism functions banks, default: 'lr' - None, 'lr', 'mt', 'all' ms_mm_strictness : bool \u2014 missing adding probabilistic or deterministic, default: True ms_mm_obs : bool \u2014 missing adding based on observed data, default: False ms_mm_feature_option : str \u2014 missing mechanism associated with which features, default: 'allk=0.2' - 'self', 'all', 'allk=0.1' ms_mm_beta_option : str \u2014 mechanism beta coefficient option, default: None - (mnar) self, sphere, randu, (mar) fixed, randu, randn seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns Dict [str, List [ np .ndarray]] \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError NotImplementedError Build from Real Federated Scenario Function fedimpute . scenario . scenario_builder . ScenarioBuilder .create_real_scenario method ScenarioBuilder . create_real_scenario ( datas : List [ pd . DataFrame ] , data_config : Dict , seed : int = 100330201 , verbose : int = 0 ) Create a real scenario from a list of pandas DataFrames Parameters datas : List [ pd . DataFrame ] \u2014 list of pandas DataFrames data_config : Dict \u2014 data configuration dictionary seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Raises ValueError","title":"Scenario Builder"},{"location":"api/simulation/#simulator","text":"fedimpute . scenario . scenario_builder .ScenarioBuilder class ScenarioBuilder ( debug_mode : bool = False ) ScenarioBuilder class for simulating or constructing missing data scenarios in federated learning environment Attributes data : np .ndarray \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary clients_train_data : List [ np .ndarray] \u2014 list of clients training data clients_test_data : List [ np .ndarray] \u2014 list of clients test data clients_train_data_ms : List [ np .ndarray] \u2014 list of clients training data with missing values global_test : np .ndarray \u2014 global test data client_seeds : List [int] \u2014 list of seeds for clients stats : dict \u2014 simulation statistics debug_mode : bool \u2014 whether to enable debug mode Methods create_simulated_scenario \u2014 Simulate missing data scenario create_simulated_scenario_lite \u2014 Simulate missing data scenario create_real_scenario \u2014 Create a real scenario from a list of pandas DataFrames save load export_data summarize_scenario show_missing_data_details visualize_missing_pattern visualize_missing_distribution visualize_data_heterogeneity","title":"Simulator"},{"location":"api/simulation/#build-simulated-scenario-function","text":"fedimpute . scenario . scenario_builder . ScenarioBuilder .create_simulated_scenario method ScenarioBuilder . create_simulated_scenario ( data : Union [ np .array, pd . DataFrame ] , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , dp_split_cols : Union [str, int] = 'target' , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 2000 , dp_sample_iid_direct : bool = False , dp_local_test_size : float = 0.1 , dp_global_test_size : float = 0.1 , dp_local_backup_size : float = 0.05 , dp_reg_bins : int = 50 , ms_scenario : str = None , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , ms_mech_type : str = 'mcar' , ms_global_mechanism : bool = False , ms_mr_dist_clients : str = 'randu' , ms_mf_dist_clients : str = 'identity' , ms_mm_dist_clients : str = 'random' , ms_missing_features : str = 'all' , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , ms_mm_funcs_bank : str = 'lr' , ms_mm_strictness : bool = True , ms_mm_obs : bool = False , ms_mm_feature_option : str = 'allk=0.2' , ms_mm_beta_option : str = None , seed : int = 100330201 , verbose : int = 0 ) \u2192 Dict [str, List [ np .ndarray]] Simulate missing data scenario Parameters data : Union [ np .array, pd . DataFrame ] \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path dp_split_cols : Union [str, int, List [int]] \u2014 split columns option - target , feature , default: target dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 2000 dp_sample_iid_direct : bool \u2014 sample iid data directly, default: False dp_local_test_size : float \u2014 local test size ratio, default: 0.1 dp_global_test_size : float \u2014 global test size ratio, default: 0.1 dp_local_backup_size : float \u2014 local backup size ratio, default: 0.05 dp_reg_bins : int \u2014 regression bins, default: 50 ms_mech_type : str \u2014 missing mechanism type, default: 'mcar' - mcar , mar_sigmoid , mnar_sigmoid , mar_quantile , mnar_quantile ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - all , all-num , random obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - random , rest ms_global_mechanism : bool \u2014 global missing mechanism, default: False ms_mr_dist_clients : str \u2014 missing ratio distribution, default: 'randu-int' - 'fixed', 'uniform', 'uniform_int', 'gaussian', 'gaussian_int' ms_mf_dist_clients : str \u2014 missing features distribution, default: 'identity' - 'identity', 'random', 'random2' ms_mm_dist_clients : str \u2014 missing mechanism functions distribution, default: 'random' - 'identity', 'random', 'random2' ms_missing_features : str \u2014 missing features strategy, default: 'all' - 'all', 'all-num' ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 ms_mm_funcs_bank : str \u2014 missing mechanism functions banks, default: 'lr' - None, 'lr', 'mt', 'all' ms_mm_strictness : bool \u2014 missing adding probabilistic or deterministic, default: True ms_mm_obs : bool \u2014 missing adding based on observed data, default: False ms_mm_feature_option : str \u2014 missing mechanism associated with which features, default: 'allk=0.2' - 'self', 'all', 'allk=0.1' ms_mm_beta_option : str \u2014 mechanism beta coefficient option, default: None - (mnar) self, sphere, randu, (mar) fixed, randu, randn seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns Dict [str, List [ np .ndarray]] \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError NotImplementedError","title":"Build Simulated Scenario Function"},{"location":"api/simulation/#build-from-real-federated-scenario-function","text":"fedimpute . scenario . scenario_builder . ScenarioBuilder .create_real_scenario method ScenarioBuilder . create_real_scenario ( datas : List [ pd . DataFrame ] , data_config : Dict , seed : int = 100330201 , verbose : int = 0 ) Create a real scenario from a list of pandas DataFrames Parameters datas : List [ pd . DataFrame ] \u2014 list of pandas DataFrames data_config : Dict \u2014 data configuration dictionary seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Raises ValueError","title":"Build from Real Federated Scenario Function"},{"location":"tutorials/basic_usage/","text":"import numpy as np import pandas as pd import tabulate import matplotlib.pyplot as plt plt . rc ( 'font' , family = 'arial' ) plt . rc ( 'pdf' , fonttype = 42 ) plt . rc ( 'ps' , fonttype = 42 ) Load Data % load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False} Scenario Simulation Basic Usage % load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... Results Structure ( Dict Keys ) : [ ' clients_train_data ' , ' clients_test_data ' , ' clients_train_data_ms ' , ' clients_seeds ' , ' global_test_data ' , ' data_config ' , ' stats ' ] ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ================================================================== Exploring Scenario Data Heterogeneity scenario_builder . visualize_data_heterogeneity ( client_ids = [ 0 , 1 , 2 , 3 ], distance_method = 'swd' , pca_col_threshold = 20 , fontsize = 18 , title = False , save_path = './plots/data_heterogeneity.svg' ) Missing Data Inspection scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], save_path = './plots/ms_pattern.pdf' ) scenario_builder . visualize_missing_distribution ( client_ids = [ 0 , 1 ], feature_ids = [ 0 , 1 , 2 , 3 , 4 ], stat = 'proportion' , bins = 20 , kde = True , save_path = './plots/ms_distribution.pdf' ) Running Federated Imputation Basic Usage % load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 2 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mice , fed - strategy : fedmice - Client 1 : imputer : mice , fed - strategy : fedmice - Client 2 : imputer : mice , fed - strategy : fedmice - Client 3 : imputer : mice , fed - strategy : fedmice Server : fed - strategy : fedmice ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m \u001b [ 1 mInitial : imp_rmse : 0.1664 imp_ws : 0.0831 \u001b [ 0 m \u001b [ 1 mEpoch 8 : loss : 0.0026 \u001b [ 0 m \u001b [ 1 mAll clients converged , iteration 8 \u001b [ 0 m \u001b [ 1 mFinal : imp_rmse : 0.1530 imp_ws : 0.0687 \u001b [ 0 m \u001b [ 32 m \u001b [ 1 mFinished . Running time : 10.4787 seconds \u001b [ 0 m Monitoring Imputation Progress env . tracker . visualize_imputation_process () Evaluation Imputation Quality % load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator X_trains = env . get_data ( client_ids = 'all' , data_type = 'train' ) X_train_imps = env . get_data ( client_ids = 'all' , data_type = 'train_imp' ) X_train_masks = env . get_data ( client_ids = 'all' , data_type = 'train_mask' ) evaluator = Evaluator () ret = evaluator . evaluate_imp_quality ( X_train_imps = X_train_imps , X_train_origins = X_trains , X_train_masks = X_train_masks , metrics = [ 'rmse' , 'nrmse' , 'sliced-ws' ] ) evaluator . show_imp_results () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ================================================ Imputation Quality ================================================ rmse nrmse sliced - ws ---------- ---------- ---------- ----------- Client 1 0.181 0.505 0.080 Client 2 0.187 0.543 0.094 Client 3 0.117 0.325 0.045 Client 4 0.126 0.354 0.056 ---------- ---------- ---------- ---------- Average 0.153 0.432 0.069 Std 0.032 0.094 0.019 ================================================ X_trains = env . get_data ( client_ids = 'all' , data_type = 'train' ) X_train_imps = env . get_data ( client_ids = 'all' , data_type = 'train_imp' ) evaluator . tsne_visualization ( X_imps = X_train_imps , X_origins = X_trains , seed = 0 ) Evaluating TSNE for Client 1 ... Evaluating TSNE for Client 2 ... Evaluating TSNE for Client 3 ... Evaluating TSNE for Client 4 ... Local Prediction X_train_imps , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train_imp' , include_y = True ) X_tests , y_tests = env . get_data ( client_ids = 'all' , data_type = 'test' , include_y = True ) X_global_test , y_global_test = env . get_data ( data_type = 'global_test' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) ret = evaluator . run_local_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , data_config = data_config , model = 'rf' , seed = 0 ) evaluator . show_local_prediction_results () Clients : 0 %| | 0 /4 [00:00<?, ?it/s ] ========================================================== Downstream Prediction ( Local ) ========================================================== accuracy f1 auc prc ---------- ---------- ---------- ---------- ---------- Client 1 0.894 0.842 0.975 0.946 Client 2 0.965 0.946 0.996 0.992 Client 3 0.956 0.932 0.982 0.975 Client 4 0.920 0.873 0.961 0.924 ---------- ---------- ---------- ---------- ---------- Average 0.934 0.898 0.979 0.959 Std 0.028 0.042 0.013 0.026 ========================================================== ret = evaluator . run_local_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , data_config = data_config , model = 'lr' , seed = 0 ) evaluator . show_local_prediction_results () Clients : 0 %| | 0 /4 [00:00<?, ?it/s ] ========================================================== Downstream Prediction ( Local ) ========================================================== accuracy f1 auc prc ---------- ---------- ---------- ---------- ---------- Client 1 0.903 0.871 0.990 0.977 Client 2 0.903 0.845 0.962 0.935 Client 3 0.850 0.809 0.950 0.911 Client 4 0.823 0.778 0.973 0.968 ---------- ---------- ---------- ---------- ---------- Average 0.869 0.826 0.968 0.948 Std 0.034 0.035 0.015 0.027 ========================================================== Federated Prediction ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , X_test_global = X_global_test , y_test_global = y_global_test , data_config = data_config , model_name = 'rf' , seed = 0 ) evaluator . show_fed_prediction_results () =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.956 0.932 0.988 0.962 Client 2 0.956 0.930 0.995 0.989 Client 3 0.956 0.932 0.995 0.989 Client 4 0.876 0.800 0.969 0.943 ---------- ---------- ---------- ---------- ---------- Global 0.914 0.862 0.952 0.906 =============================================================== ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , X_test_global = X_global_test , y_test_global = y_global_test , data_config = data_config , model_name = 'lr' , seed = 0 ) evaluator . show_fed_prediction_results () (900, 8) (900,) 3.52it/s] =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.912 0.878 0.982 0.955 Client 2 0.920 0.892 0.996 0.992 Client 3 0.903 0.867 0.968 0.947 Client 4 0.876 0.833 0.972 0.960 ---------- ---------- ---------- ---------- ---------- Global 0.912 0.878 0.979 0.953 =============================================================== All In One % load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all () \u001b [ 1 mEvaluating imputation quality ... \u001b [ 0 m \u001b [ 1 mImputation quality evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluating downstream prediction ... \u001b [ 0 m Clients : 0 %| | 0 / 4 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEarly stopping at epoch 593 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 287 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 583 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 360 \u001b [ 0 m \u001b [ 1 mDownstream prediction evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluating federated downstream prediction ... \u001b [ 0 m Global Epoch : 0 %| | 0 / 100 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 0 - average loss : 0.6716053117724026 \u001b [ 0 m \u001b [ 1 mEpoch 10 - average loss : 0.5709273157750859 \u001b [ 0 m \u001b [ 1 mEpoch 20 - average loss : 0.48411076998009406 \u001b [ 0 m \u001b [ 1 mEpoch 30 - average loss : 0.4436344894416192 \u001b [ 0 m \u001b [ 1 mEpoch 40 - average loss : 0.43356676005265293 \u001b [ 0 m \u001b [ 1 mEpoch 50 - average loss : 0.4266376635607551 \u001b [ 0 m \u001b [ 1 mEpoch 60 - average loss : 0.4221082873204175 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 65 \u001b [ 0 m \u001b [ 1 mEpoch 70 - average loss : 0.40617619396424764 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 71 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 74 \u001b [ 0 m \u001b [ 1 mEpoch 80 - average loss : 0.3788260501973769 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 81 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 100 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 121 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 125 \u001b [ 0 m \u001b [ 1 mFederated downstream prediction evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluation completed . \u001b [ 0 m .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } imp_quality pred_downstream_local pred_downstream_fed rmse nrmse sliced-ws accuracy f1 auc prc personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.172028 0.479522 0.075509 0.787611 0.636364 0.860953 0.741470 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.184269 0.533933 0.090226 0.893805 0.823529 0.952703 0.934722 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.119754 0.331969 0.047390 0.929204 0.894737 0.975818 0.969489 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.128211 0.359113 0.057849 0.752212 0.481481 0.854196 0.760059 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098 Export Evaluation Results in Different Format evaluator . export_results ( format = 'dataframe' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } imp_quality pred_downstream_local pred_downstream_fed rmse nrmse sliced-ws accuracy f1 auc prc personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.172028 0.479522 0.075509 0.787611 0.636364 0.860953 0.741470 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.184269 0.533933 0.090226 0.893805 0.823529 0.952703 0.934722 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.119754 0.331969 0.047390 0.929204 0.894737 0.975818 0.969489 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.128211 0.359113 0.057849 0.752212 0.481481 0.854196 0.760059 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098 ret = evaluator . export_results ( format = 'dict-dataframe' ) ret [ 'imp_quality' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rmse nrmse sliced-ws 0 0.172028 0.479522 0.075509 1 0.184269 0.533933 0.090226 2 0.119754 0.331969 0.047390 3 0.128211 0.359113 0.057849 ret = evaluator . export_results ( format = 'dict-dataframe' ) ret [ 'pred_downstream_fed' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098 regression analysis X_trains , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) data_config [ 'task_type' ] = 'classification' ret = evaluator . run_local_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_local_regression_results ( client_idx = 0 ) Local Logit Regression Results (client 1) ============================================================================== Dep. Variable: y No. Observations: 1125 Model: Logit Df Residuals: 1116 Method: MLE Df Model: 8 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.2545 Time: 16:28:07 Log-Likelihood: -528.54 converged: True LL-Null: -708.95 Covariance Type: nonrobust LLR p-value: 4.432e-73 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 47.6797 4.807 9.919 0.000 38.258 57.101 X1 -82.3736 7.642 -10.779 0.000 -97.351 -67.396 X2 -19.5991 2.591 -7.563 0.000 -24.678 -14.520 X3 10.0812 3.236 3.116 0.002 3.739 16.423 X4 8.6467 4.951 1.747 0.081 -1.056 18.350 X5 4.8074 6.234 0.771 0.441 -7.411 17.026 X6 15.6919 4.340 3.616 0.000 7.187 24.197 X7 11.8661 4.610 2.574 0.010 2.831 20.901 X8 5.4795 6.139 0.893 0.372 -6.553 17.512 ============================================================================== X_trains , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) data_config [ 'task_type' ] = 'classification' evaluator . run_fed_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_fed_regression_results () Federated Logit Regression Result ============================================================================== Dep. Variable: y No. Observations: 4500 Model: Logit Df Residuals: 4491 Method: MLE Df Model: 8 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.2478 Time: 16:28:24 Log-Likelihood: -2133.1 converged: True LL-Null: -2835.8 Covariance Type: nonrobust LLR p-value: 4.035e-298 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 30.4160 0.129 235.463 0.000 30.163 30.669 X1 -63.3583 0.231 -274.762 0.000 -63.810 -62.906 X2 -16.1744 0.064 -254.610 0.000 -16.299 -16.050 X3 15.2472 0.078 194.635 0.000 15.094 15.401 X4 11.3237 0.084 134.553 0.000 11.159 11.489 X5 10.7434 0.103 103.972 0.000 10.541 10.946 X6 10.6433 0.083 128.952 0.000 10.482 10.805 X7 8.6910 0.090 96.851 0.000 8.515 8.867 X8 1.0197 0.114 8.956 0.000 0.797 1.243 ==============================================================================","title":"Basic Usage"},{"location":"tutorials/basic_usage/#load-data","text":"% load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False}","title":"Load Data"},{"location":"tutorials/basic_usage/#scenario-simulation","text":"","title":"Scenario Simulation"},{"location":"tutorials/basic_usage/#basic-usage","text":"% load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... Results Structure ( Dict Keys ) : [ ' clients_train_data ' , ' clients_test_data ' , ' clients_train_data_ms ' , ' clients_seeds ' , ' global_test_data ' , ' data_config ' , ' stats ' ] ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ==================================================================","title":"Basic Usage"},{"location":"tutorials/basic_usage/#exploring-scenario","text":"","title":"Exploring Scenario"},{"location":"tutorials/basic_usage/#data-heterogeneity","text":"scenario_builder . visualize_data_heterogeneity ( client_ids = [ 0 , 1 , 2 , 3 ], distance_method = 'swd' , pca_col_threshold = 20 , fontsize = 18 , title = False , save_path = './plots/data_heterogeneity.svg' )","title":"Data Heterogeneity"},{"location":"tutorials/basic_usage/#missing-data-inspection","text":"scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], save_path = './plots/ms_pattern.pdf' ) scenario_builder . visualize_missing_distribution ( client_ids = [ 0 , 1 ], feature_ids = [ 0 , 1 , 2 , 3 , 4 ], stat = 'proportion' , bins = 20 , kde = True , save_path = './plots/ms_distribution.pdf' )","title":"Missing Data Inspection"},{"location":"tutorials/basic_usage/#running-federated-imputation","text":"","title":"Running Federated Imputation"},{"location":"tutorials/basic_usage/#basic-usage_1","text":"% load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 2 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mice , fed - strategy : fedmice - Client 1 : imputer : mice , fed - strategy : fedmice - Client 2 : imputer : mice , fed - strategy : fedmice - Client 3 : imputer : mice , fed - strategy : fedmice Server : fed - strategy : fedmice ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m \u001b [ 1 mInitial : imp_rmse : 0.1664 imp_ws : 0.0831 \u001b [ 0 m \u001b [ 1 mEpoch 8 : loss : 0.0026 \u001b [ 0 m \u001b [ 1 mAll clients converged , iteration 8 \u001b [ 0 m \u001b [ 1 mFinal : imp_rmse : 0.1530 imp_ws : 0.0687 \u001b [ 0 m \u001b [ 32 m \u001b [ 1 mFinished . Running time : 10.4787 seconds \u001b [ 0 m","title":"Basic Usage"},{"location":"tutorials/basic_usage/#monitoring-imputation-progress","text":"env . tracker . visualize_imputation_process ()","title":"Monitoring Imputation Progress"},{"location":"tutorials/basic_usage/#evaluation","text":"","title":"Evaluation"},{"location":"tutorials/basic_usage/#imputation-quality","text":"% load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator X_trains = env . get_data ( client_ids = 'all' , data_type = 'train' ) X_train_imps = env . get_data ( client_ids = 'all' , data_type = 'train_imp' ) X_train_masks = env . get_data ( client_ids = 'all' , data_type = 'train_mask' ) evaluator = Evaluator () ret = evaluator . evaluate_imp_quality ( X_train_imps = X_train_imps , X_train_origins = X_trains , X_train_masks = X_train_masks , metrics = [ 'rmse' , 'nrmse' , 'sliced-ws' ] ) evaluator . show_imp_results () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ================================================ Imputation Quality ================================================ rmse nrmse sliced - ws ---------- ---------- ---------- ----------- Client 1 0.181 0.505 0.080 Client 2 0.187 0.543 0.094 Client 3 0.117 0.325 0.045 Client 4 0.126 0.354 0.056 ---------- ---------- ---------- ---------- Average 0.153 0.432 0.069 Std 0.032 0.094 0.019 ================================================ X_trains = env . get_data ( client_ids = 'all' , data_type = 'train' ) X_train_imps = env . get_data ( client_ids = 'all' , data_type = 'train_imp' ) evaluator . tsne_visualization ( X_imps = X_train_imps , X_origins = X_trains , seed = 0 ) Evaluating TSNE for Client 1 ... Evaluating TSNE for Client 2 ... Evaluating TSNE for Client 3 ... Evaluating TSNE for Client 4 ...","title":"Imputation Quality"},{"location":"tutorials/basic_usage/#local-prediction","text":"X_train_imps , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train_imp' , include_y = True ) X_tests , y_tests = env . get_data ( client_ids = 'all' , data_type = 'test' , include_y = True ) X_global_test , y_global_test = env . get_data ( data_type = 'global_test' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) ret = evaluator . run_local_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , data_config = data_config , model = 'rf' , seed = 0 ) evaluator . show_local_prediction_results () Clients : 0 %| | 0 /4 [00:00<?, ?it/s ] ========================================================== Downstream Prediction ( Local ) ========================================================== accuracy f1 auc prc ---------- ---------- ---------- ---------- ---------- Client 1 0.894 0.842 0.975 0.946 Client 2 0.965 0.946 0.996 0.992 Client 3 0.956 0.932 0.982 0.975 Client 4 0.920 0.873 0.961 0.924 ---------- ---------- ---------- ---------- ---------- Average 0.934 0.898 0.979 0.959 Std 0.028 0.042 0.013 0.026 ========================================================== ret = evaluator . run_local_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , data_config = data_config , model = 'lr' , seed = 0 ) evaluator . show_local_prediction_results () Clients : 0 %| | 0 /4 [00:00<?, ?it/s ] ========================================================== Downstream Prediction ( Local ) ========================================================== accuracy f1 auc prc ---------- ---------- ---------- ---------- ---------- Client 1 0.903 0.871 0.990 0.977 Client 2 0.903 0.845 0.962 0.935 Client 3 0.850 0.809 0.950 0.911 Client 4 0.823 0.778 0.973 0.968 ---------- ---------- ---------- ---------- ---------- Average 0.869 0.826 0.968 0.948 Std 0.034 0.035 0.015 0.027 ==========================================================","title":"Local Prediction"},{"location":"tutorials/basic_usage/#federated-prediction","text":"ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , X_test_global = X_global_test , y_test_global = y_global_test , data_config = data_config , model_name = 'rf' , seed = 0 ) evaluator . show_fed_prediction_results () =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.956 0.932 0.988 0.962 Client 2 0.956 0.930 0.995 0.989 Client 3 0.956 0.932 0.995 0.989 Client 4 0.876 0.800 0.969 0.943 ---------- ---------- ---------- ---------- ---------- Global 0.914 0.862 0.952 0.906 =============================================================== ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_tests , y_tests = y_tests , X_test_global = X_global_test , y_test_global = y_global_test , data_config = data_config , model_name = 'lr' , seed = 0 ) evaluator . show_fed_prediction_results () (900, 8) (900,) 3.52it/s] =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.912 0.878 0.982 0.955 Client 2 0.920 0.892 0.996 0.992 Client 3 0.903 0.867 0.968 0.947 Client 4 0.876 0.833 0.972 0.960 ---------- ---------- ---------- ---------- ---------- Global 0.912 0.878 0.979 0.953 ===============================================================","title":"Federated Prediction"},{"location":"tutorials/basic_usage/#all-in-one","text":"% load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all () \u001b [ 1 mEvaluating imputation quality ... \u001b [ 0 m \u001b [ 1 mImputation quality evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluating downstream prediction ... \u001b [ 0 m Clients : 0 %| | 0 / 4 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEarly stopping at epoch 593 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 287 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 583 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 360 \u001b [ 0 m \u001b [ 1 mDownstream prediction evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluating federated downstream prediction ... \u001b [ 0 m Global Epoch : 0 %| | 0 / 100 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 0 - average loss : 0.6716053117724026 \u001b [ 0 m \u001b [ 1 mEpoch 10 - average loss : 0.5709273157750859 \u001b [ 0 m \u001b [ 1 mEpoch 20 - average loss : 0.48411076998009406 \u001b [ 0 m \u001b [ 1 mEpoch 30 - average loss : 0.4436344894416192 \u001b [ 0 m \u001b [ 1 mEpoch 40 - average loss : 0.43356676005265293 \u001b [ 0 m \u001b [ 1 mEpoch 50 - average loss : 0.4266376635607551 \u001b [ 0 m \u001b [ 1 mEpoch 60 - average loss : 0.4221082873204175 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 65 \u001b [ 0 m \u001b [ 1 mEpoch 70 - average loss : 0.40617619396424764 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 71 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 74 \u001b [ 0 m \u001b [ 1 mEpoch 80 - average loss : 0.3788260501973769 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 81 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 100 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 121 \u001b [ 0 m \u001b [ 1 mEarly stopping at epoch 125 \u001b [ 0 m \u001b [ 1 mFederated downstream prediction evaluation completed . \u001b [ 0 m \u001b [ 1 mEvaluation completed . \u001b [ 0 m .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } imp_quality pred_downstream_local pred_downstream_fed rmse nrmse sliced-ws accuracy f1 auc prc personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.172028 0.479522 0.075509 0.787611 0.636364 0.860953 0.741470 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.184269 0.533933 0.090226 0.893805 0.823529 0.952703 0.934722 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.119754 0.331969 0.047390 0.929204 0.894737 0.975818 0.969489 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.128211 0.359113 0.057849 0.752212 0.481481 0.854196 0.760059 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098","title":"All In One"},{"location":"tutorials/basic_usage/#export-evaluation-results-in-different-format","text":"evaluator . export_results ( format = 'dataframe' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } imp_quality pred_downstream_local pred_downstream_fed rmse nrmse sliced-ws accuracy f1 auc prc personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.172028 0.479522 0.075509 0.787611 0.636364 0.860953 0.741470 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.184269 0.533933 0.090226 0.893805 0.823529 0.952703 0.934722 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.119754 0.331969 0.047390 0.929204 0.894737 0.975818 0.969489 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.128211 0.359113 0.057849 0.752212 0.481481 0.854196 0.760059 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098 ret = evaluator . export_results ( format = 'dict-dataframe' ) ret [ 'imp_quality' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rmse nrmse sliced-ws 0 0.172028 0.479522 0.075509 1 0.184269 0.533933 0.090226 2 0.119754 0.331969 0.047390 3 0.128211 0.359113 0.057849 ret = evaluator . export_results ( format = 'dict-dataframe' ) ret [ 'pred_downstream_fed' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } personalized_accuracy personalized_f1 personalized_auc personalized_prc global_accuracy global_f1 global_auc global_prc 0 0.884956 0.831169 0.958037 0.900117 0.906 0.86217 0.968807 0.94098 1 0.920354 0.883117 0.981508 0.956229 0.906 0.86217 0.968807 0.94098 2 0.884956 0.835443 0.975462 0.951035 0.906 0.86217 0.968807 0.94098 3 0.867257 0.819277 0.972617 0.961279 0.906 0.86217 0.968807 0.94098","title":"Export Evaluation Results in Different Format"},{"location":"tutorials/basic_usage/#regression-analysis","text":"X_trains , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) data_config [ 'task_type' ] = 'classification' ret = evaluator . run_local_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_local_regression_results ( client_idx = 0 ) Local Logit Regression Results (client 1) ============================================================================== Dep. Variable: y No. Observations: 1125 Model: Logit Df Residuals: 1116 Method: MLE Df Model: 8 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.2545 Time: 16:28:07 Log-Likelihood: -528.54 converged: True LL-Null: -708.95 Covariance Type: nonrobust LLR p-value: 4.432e-73 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 47.6797 4.807 9.919 0.000 38.258 57.101 X1 -82.3736 7.642 -10.779 0.000 -97.351 -67.396 X2 -19.5991 2.591 -7.563 0.000 -24.678 -14.520 X3 10.0812 3.236 3.116 0.002 3.739 16.423 X4 8.6467 4.951 1.747 0.081 -1.056 18.350 X5 4.8074 6.234 0.771 0.441 -7.411 17.026 X6 15.6919 4.340 3.616 0.000 7.187 24.197 X7 11.8661 4.610 2.574 0.010 2.831 20.901 X8 5.4795 6.139 0.893 0.372 -6.553 17.512 ============================================================================== X_trains , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train' , include_y = True ) data_config = env . get_data ( data_type = 'config' ) data_config [ 'task_type' ] = 'classification' evaluator . run_fed_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_fed_regression_results () Federated Logit Regression Result ============================================================================== Dep. Variable: y No. Observations: 4500 Model: Logit Df Residuals: 4491 Method: MLE Df Model: 8 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.2478 Time: 16:28:24 Log-Likelihood: -2133.1 converged: True LL-Null: -2835.8 Covariance Type: nonrobust LLR p-value: 4.035e-298 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 30.4160 0.129 235.463 0.000 30.163 30.669 X1 -63.3583 0.231 -274.762 0.000 -63.810 -62.906 X2 -16.1744 0.064 -254.610 0.000 -16.299 -16.050 X3 15.2472 0.078 194.635 0.000 15.094 15.401 X4 11.3237 0.084 134.553 0.000 11.159 11.489 X5 10.7434 0.103 103.972 0.000 10.541 10.946 X6 10.6433 0.083 128.952 0.000 10.482 10.805 X7 8.6910 0.090 96.851 0.000 8.515 8.867 X8 1.0197 0.114 8.956 0.000 0.797 1.243 ==============================================================================","title":"regression analysis"},{"location":"tutorials/benchmark/","text":"import numpy as np import pandas as pd import tabulate Load Data and Scenario % load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False} % load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ================================================================== Benchmarking Pipeline % load_ext autoreload % autoreload 2 from fedimpute.pipeline import FedImputePipeline pipeline = FedImputePipeline () pipeline . setup ( id = 'benchmark_demo' , fed_imp_configs = [ ( 'em' , [ 'local' , 'fedem' ], {}, [{}, {}]), ( 'mice' , [ 'local' , 'fedmice' ], {}, [{}, {}]), ( 'gain' , [ 'local' , 'fedavg' ], {}, [{}, {}]), ], evaluation_params = { 'metrics' : [ 'imp_quality' , 'local_pred' , 'fed_pred' ], 'model' : 'lr' , }, persist_data = False , description = 'benchmark demonstration' ) pipeline . pipeline_setup_summary () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ============================================================== Experiment ID : benchmark_demo ============================================================== Description : benchmark demonstration Persist Data : False Evaluation : - metrics : [ ' imp_quality ' , ' local_pred ' , ' fed_pred ' ] - model : lr Seed : 100330201 -------------------------------------------------------------- Imputer Fed Strategy Imp Params Strategy Params -- --------- -------------- ------------ ----------------- 0 em local {} {} 1 em fedem {} {} 2 mice local {} {} 3 mice fedmice {} {} 4 gain local {} {} 5 gain fedavg {} {} ============================================================== pipeline . run_pipeline ( scenario_builder , repeats = 5 , verbose = 0 ) Result Analysis import matplotlib.pyplot as plt plt . rc ( 'font' , family = 'arial' ) plt . rc ( 'pdf' , fonttype = 42 ) plt . rc ( 'ps' , fonttype = 42 ) pipeline . plot_pipeline_results ( metric_aspect = 'fed_pred_personalized' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_fedpred.png\" , legend = False , dpi = 300 ) pipeline . plot_pipeline_results ( metric_aspect = 'local_pred' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_localpred.png\" , legend = False ) pipeline . plot_pipeline_results ( metric_aspect = 'imp_quality' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_impquality.png\" ) pipeline . plot_pipeline_results ( metric_aspect = 'fed_pred_global' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_fedpredglobal.png\" , legend = False ) data = pipeline . show_pipeline_results ( format = 'dataframe' , metric_aspect = 'imp_quality' , metric_name = 'rmse' , show_round_variation = False ) data . to_excel ( \"./plots/benchmark_impquality.xlsx\" )","title":"Benchmarking Pipeline Demo"},{"location":"tutorials/benchmark/#load-data-and-scenario","text":"% load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False} % load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ==================================================================","title":"Load Data and Scenario"},{"location":"tutorials/benchmark/#benchmarking-pipeline","text":"% load_ext autoreload % autoreload 2 from fedimpute.pipeline import FedImputePipeline pipeline = FedImputePipeline () pipeline . setup ( id = 'benchmark_demo' , fed_imp_configs = [ ( 'em' , [ 'local' , 'fedem' ], {}, [{}, {}]), ( 'mice' , [ 'local' , 'fedmice' ], {}, [{}, {}]), ( 'gain' , [ 'local' , 'fedavg' ], {}, [{}, {}]), ], evaluation_params = { 'metrics' : [ 'imp_quality' , 'local_pred' , 'fed_pred' ], 'model' : 'lr' , }, persist_data = False , description = 'benchmark demonstration' ) pipeline . pipeline_setup_summary () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ============================================================== Experiment ID : benchmark_demo ============================================================== Description : benchmark demonstration Persist Data : False Evaluation : - metrics : [ ' imp_quality ' , ' local_pred ' , ' fed_pred ' ] - model : lr Seed : 100330201 -------------------------------------------------------------- Imputer Fed Strategy Imp Params Strategy Params -- --------- -------------- ------------ ----------------- 0 em local {} {} 1 em fedem {} {} 2 mice local {} {} 3 mice fedmice {} {} 4 gain local {} {} 5 gain fedavg {} {} ============================================================== pipeline . run_pipeline ( scenario_builder , repeats = 5 , verbose = 0 )","title":"Benchmarking Pipeline"},{"location":"tutorials/benchmark/#result-analysis","text":"import matplotlib.pyplot as plt plt . rc ( 'font' , family = 'arial' ) plt . rc ( 'pdf' , fonttype = 42 ) plt . rc ( 'ps' , fonttype = 42 ) pipeline . plot_pipeline_results ( metric_aspect = 'fed_pred_personalized' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_fedpred.png\" , legend = False , dpi = 300 ) pipeline . plot_pipeline_results ( metric_aspect = 'local_pred' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_localpred.png\" , legend = False ) pipeline . plot_pipeline_results ( metric_aspect = 'imp_quality' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_impquality.png\" ) pipeline . plot_pipeline_results ( metric_aspect = 'fed_pred_global' , plot_type = 'bar' , plot_params = { 'font_size' : 20 , 'bar_width' : 0.2 }, save_path = \"./plots/benchmark_fedpredglobal.png\" , legend = False ) data = pipeline . show_pipeline_results ( format = 'dataframe' , metric_aspect = 'imp_quality' , metric_name = 'rmse' , show_round_variation = False ) data . to_excel ( \"./plots/benchmark_impquality.xlsx\" )","title":"Result Analysis"},{"location":"tutorials/develop_new_method/","text":"cd .. d : \\ min \\ research_projects \\ FedImpute d : \\ min \\ research_projects \\ FedImpute \\ . venv \\ Lib \\ site - packages \\ IPython \\ core \\ magics \\ osm . py : 417 : UserWarning : This is now an optional IPython functionality , set ting dhist requires you to install the `pickleshare` library . self . shell . db [ 'dhist' ] = compress_dhist ( dhist ) [ - 100 : ] Load Dataset We first load codrna dataset from fedimpute. % load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) The autoreload extension is already loaded . To reload it , use: %reload_ext autoreload +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False} Construct a distributed data scenario We then construct a distributed data scenario with 4 clients and heterogenous MNAR missingness. % load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... Results Structure ( Dict Keys ) : [ ' clients_train_data ' , ' clients_test_data ' , ' clients_train_data_ms ' , ' clients_seeds ' , ' global_test_data ' , ' data_config ' , ' stats ' ] ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ================================================================== Build New Imputer In the following example, we develop a new imputer MICE imputation with a 2 layer neural network as underlying machine learning model for imputation, it should inherit the abstract class BaseMLImputer and implement all its abstract methods. It also inherit ICEImputerMixin class which contains some helper function for ICE imputation. We add comment in class to give more instructions on how we implement it. from fedimpute.execution_environment.imputation.base import BaseMLImputer , ICEImputerMixin from sklearn.neural_network import MLPRegressor import numpy as np from collections import OrderedDict class MLPICEImputer ( BaseMLImputer , ICEImputerMixin ): def __init__ ( self ): super () . __init__ ( name = 'mlp_mice' , model_persistable = False ) # it needs two parameters: name of imputer and whether the model is persistable (can be saved to disk), we set it to False because ICE imptutation is not persistable self . imp_models = [] # list of imputation models (each for a feature) self . min_values = None # min values of features used for clipping self . max_values = None # max values of features used for clipping self . seed = None # seed for randomization self . fit_res_history = {} # fit results history def initialize ( self , X : np . array , missing_mask : np . array , data_utils : dict , params : dict , seed : int ) -> None : \"\"\" Initialize imputer - statistics imputation models etc. Args: X: data with intial imputed values missing_mask: missing mask of data data_utils: data utils dictionary - contains information about data params: params for initialization seed: int - seed for randomization \"\"\" # initialized imputation models (from sklearn's MLPRegressor (fully connected neural network)) self . imp_models = [] for i in range ( data_utils [ 'n_features' ]): estimator = MLPRegressor ( hidden_layer_sizes = ( 100 , 100 ), max_iter = 1000 ) X_train = X [:, np . arange ( X . shape [ 1 ]) != i ][ 0 : 10 ] y_train = X [:, i ][ 0 : 10 ] estimator . fit ( X_train , y_train ) self . imp_models . append ( estimator ) # initialize min max values for a clipping threshold (this method is defined in `ICEImputerMixin`) self . min_values , self . max_values = self . get_clip_thresholds ( data_utils ) self . seed = seed def get_imp_model_params ( self , params : dict ) -> OrderedDict : \"\"\" Return model parameters Args: params: dict contains parameters for get_imp_model_params Returns: OrderedDict - model parameters dictionary \"\"\" # This method is used to get the parameters of the imputation model for a given feature # get feature index of imputation models feature_idx = params [ 'feature_idx' ] imp_model = self . imp_models [ feature_idx ] # get parameters from sklearn model coefs = imp_model . coefs_ intercept = imp_model . intercepts_ # convert parameters to a dictionary (we need to convert parameters to ordered dictionary as required by `BaseMLImputer`) parameters = {} for i in range ( len ( coefs )): parameters [ f 'coef_ { i } ' ] = coefs [ i ] parameters [ f 'intercept_ { i } ' ] = intercept [ i ] return OrderedDict ( parameters ) def set_imp_model_params ( self , updated_model_dict : OrderedDict , params : dict ) -> None : \"\"\" Set model parameters Args: updated_model_dict: global model parameters dictionary params: parameters for set parameters function \"\"\" # This method is used to set the parameters of the imputation model for a given feature (update models) # get feature index of imputation models feature_idx = params [ 'feature_idx' ] imp_model = self . imp_models [ feature_idx ] # set parameters to sklearn model coefs = [] intercepts = [] for i in range ( len ( imp_model . coefs_ )): coefs . append ( updated_model_dict [ f 'coef_ { i } ' ]) intercepts . append ( updated_model_dict [ f 'intercept_ { i } ' ]) imp_model . coefs_ = coefs imp_model . intercepts_ = intercepts def fit ( self , X : np . array , y : np . array , missing_mask : np . array , params : dict ) -> dict : \"\"\" Fit imputer to train local imputation models Args: X: np.array - float numpy array features y: np.array - target missing_mask: np.array - missing mask params: parameters for local training \"\"\" # This method is used to fit the imputation model for a given feature # get complete data of the feature feature_idx = params [ 'feature_idx' ] row_mask = missing_mask [:, feature_idx ] X_train = X [ ~ row_mask ][:, np . arange ( X . shape [ 1 ]) != feature_idx ] y_train = X [ ~ row_mask ][:, feature_idx ] # fit MLP imputation models estimator = self . imp_models [ feature_idx ] estimator . fit ( X_train , y_train ) y_pred = estimator . predict ( X_train ) loss = np . mean (( y_pred - y_train ) ** 2 ) self . fit_res_history [ feature_idx ] . append ({ 'loss' : loss , 'sample_size' : X_train . shape [ 0 ] }) return { 'loss' : loss , 'sample_size' : X_train . shape [ 0 ] } def impute ( self , X : np . array , y : np . array , missing_mask : np . array , params : dict ) -> np . ndarray : \"\"\" Impute missing values using an imputation model Args: X (np.array): numpy array of features y (np.array): numpy array of target missing_mask (np.array): missing mask params (dict): parameters for imputation Returns: np.ndarray: imputed data - numpy array - same dimension as X \"\"\" # This method is used to impute the missing values using the imputation model for a given feature # get incomplete data of the feature feature_idx = params [ 'feature_idx' ] row_mask = missing_mask [:, feature_idx ] if np . sum ( row_mask ) == 0 : return X # impute missing values X_test = X [ row_mask ][:, np . arange ( X . shape [ 1 ]) != feature_idx ] estimator = self . imp_models [ feature_idx ] imputed_values = estimator . predict ( X_test ) imputed_values = np . clip ( imputed_values , self . min_values [ feature_idx ], self . max_values [ feature_idx ]) X [ row_mask , feature_idx ] = np . squeeze ( imputed_values ) return X def get_fit_res ( self , params : dict ) -> dict : # This method is used to get fit results for a given feature from the fit history try : feature_idx = params [ 'feature_idx' ] except KeyError : raise ValueError ( \"Feature index not found in params\" ) return self . fit_res_history [ feature_idx ][ - 1 ] Register New Imputer to Environment Once finishing develop new imputer, it needs to be registered into fedimpute, so it can be used by constructed environment. We need to call register_imputer method from env.register object. It takes name of imputer, class of imputer, workflow associated with imputer and a list of supported federated strategy of imputer. % load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . register . register_imputer ( name = 'mlp_mice' , # name of we give to the new imputer imputer = MLPICEImputer , # the class of the new imputer we just developed workflow = 'ice' , # because it is ICE imputation, we use 'ice' workflow fed_strategy = [ 'local' , 'fedmice' ] # we support local and fedmice strategy for this imputer ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload # then we can use the new imputer in the environment and run the federated imputation env . configuration ( imputer = 'mlp_mice' , fed_strategy = 'local' , workflow_params = { 'log_metric' : None }) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 2 ) \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mlp_mice , fed - strategy : local - Client 1 : imputer : mlp_mice , fed - strategy : local - Client 2 : imputer : mlp_mice , fed - strategy : local - Client 3 : imputer : mlp_mice , fed - strategy : local Server : fed - strategy : local ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m \u001b [ 1 mInitial : imp_rmse : 0.1658 imp_ws : 0.0827 \u001b [ 0 m ICE Iterations : 0 %| | 0 / 20 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 0 : imp_rmse : 0.2145 imp_ws : 0.1100 loss : 0.0074 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 1 : imp_rmse : 0.2279 imp_ws : 0.1138 loss : 0.0065 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 2 : imp_rmse : 0.2276 imp_ws : 0.1131 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 3 : imp_rmse : 0.2351 imp_ws : 0.1153 loss : 0.0061 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 4 : imp_rmse : 0.2383 imp_ws : 0.1164 loss : 0.0062 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 5 : imp_rmse : 0.2378 imp_ws : 0.1158 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 6 : imp_rmse : 0.2397 imp_ws : 0.1158 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 7 : imp_rmse : 0.2432 imp_ws : 0.1186 loss : 0.0063 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 8 : imp_rmse : 0.2389 imp_ws : 0.1167 loss : 0.0063 \u001b [ 0 m \u001b [ 1 mAll clients converged , iteration 8 \u001b [ 0 m \u001b [ 1 mFinal : imp_rmse : 0.2389 imp_ws : 0.1167 \u001b [ 0 m \u001b [ 32 m \u001b [ 1 mFinished . Running time : 65.1471 seconds \u001b [ 0 m","title":"Developing Customized Methods"},{"location":"tutorials/develop_new_method/#load-dataset","text":"We first load codrna dataset from fedimpute. % load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data data , data_config = load_data ( \"codrna\" ) display_data ( data ) print ( \"Data Dimensions: \" , data . shape ) print ( \"Data Config: \\n \" , data_config ) The autoreload extension is already loaded . To reload it , use: %reload_ext autoreload +--------+--------+--------+--------+--------+--------+--------+--------+--------+ | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | y | | --------+--------+--------+--------+--------+--------+--------+--------+-------- | | 0 . 7554 | 0 . 1364 | 0 . 0352 | 0 . 4132 | 0 . 6937 | 0 . 1591 | 0 . 3329 | 0 . 7154 | 1 . 0000 | | 0 . 7334 | 0 . 7879 | 0 . 3819 | 0 . 3693 | 0 . 5619 | 0 . 4830 | 0 . 4351 | 0 . 5160 | 0 . 0000 | | 0 . 7752 | 0 . 1364 | 0 . 1761 | 0 . 3290 | 0 . 7410 | 0 . 4259 | 0 . 4644 | 0 . 5268 | 1 . 0000 | | 0 . 5905 | 0 . 7424 | 0 . 2720 | 0 . 2898 | 0 . 6920 | 0 . 3205 | 0 . 4019 | 0 . 6290 | 1 . 0000 | | 0 . 7366 | 0 . 1212 | 0 . 2465 | 0 . 3290 | 0 . 7410 | 0 . 3249 | 0 . 5086 | 0 . 5631 | 1 . 0000 | +--------+--------+--------+--------+--------+--------+--------+--------+--------+ Data Dimensions: (5000 , 9) Data Config: {'target': 'y' , 'task_type': 'classification' , 'natural_partition': False}","title":"Load Dataset"},{"location":"tutorials/develop_new_method/#construct-a-distributed-data-scenario","text":"We then construct a distributed data scenario with 4 clients and heterogenous MNAR missingness. % load_ext autoreload % autoreload 2 from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Missing data simulation ... Results Structure ( Dict Keys ) : [ ' clients_train_data ' , ' clients_test_data ' , ' clients_train_data_ms ' , ' clients_seeds ' , ' global_test_data ' , ' data_config ' , ' stats ' ] ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 500 , 9 ) Missing Mechanism Category : MNAR ( Self Masking Logit ) Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 6077 C2 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.51 8 / 8 577 C3 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.46 8 / 8 7231 C4 ( 1125 , 9 ) ( 113 , 9 ) ( 1125 , 8 ) 0.47 8 / 8 5504 ==================================================================","title":"Construct a distributed data scenario"},{"location":"tutorials/develop_new_method/#build-new-imputer","text":"In the following example, we develop a new imputer MICE imputation with a 2 layer neural network as underlying machine learning model for imputation, it should inherit the abstract class BaseMLImputer and implement all its abstract methods. It also inherit ICEImputerMixin class which contains some helper function for ICE imputation. We add comment in class to give more instructions on how we implement it. from fedimpute.execution_environment.imputation.base import BaseMLImputer , ICEImputerMixin from sklearn.neural_network import MLPRegressor import numpy as np from collections import OrderedDict class MLPICEImputer ( BaseMLImputer , ICEImputerMixin ): def __init__ ( self ): super () . __init__ ( name = 'mlp_mice' , model_persistable = False ) # it needs two parameters: name of imputer and whether the model is persistable (can be saved to disk), we set it to False because ICE imptutation is not persistable self . imp_models = [] # list of imputation models (each for a feature) self . min_values = None # min values of features used for clipping self . max_values = None # max values of features used for clipping self . seed = None # seed for randomization self . fit_res_history = {} # fit results history def initialize ( self , X : np . array , missing_mask : np . array , data_utils : dict , params : dict , seed : int ) -> None : \"\"\" Initialize imputer - statistics imputation models etc. Args: X: data with intial imputed values missing_mask: missing mask of data data_utils: data utils dictionary - contains information about data params: params for initialization seed: int - seed for randomization \"\"\" # initialized imputation models (from sklearn's MLPRegressor (fully connected neural network)) self . imp_models = [] for i in range ( data_utils [ 'n_features' ]): estimator = MLPRegressor ( hidden_layer_sizes = ( 100 , 100 ), max_iter = 1000 ) X_train = X [:, np . arange ( X . shape [ 1 ]) != i ][ 0 : 10 ] y_train = X [:, i ][ 0 : 10 ] estimator . fit ( X_train , y_train ) self . imp_models . append ( estimator ) # initialize min max values for a clipping threshold (this method is defined in `ICEImputerMixin`) self . min_values , self . max_values = self . get_clip_thresholds ( data_utils ) self . seed = seed def get_imp_model_params ( self , params : dict ) -> OrderedDict : \"\"\" Return model parameters Args: params: dict contains parameters for get_imp_model_params Returns: OrderedDict - model parameters dictionary \"\"\" # This method is used to get the parameters of the imputation model for a given feature # get feature index of imputation models feature_idx = params [ 'feature_idx' ] imp_model = self . imp_models [ feature_idx ] # get parameters from sklearn model coefs = imp_model . coefs_ intercept = imp_model . intercepts_ # convert parameters to a dictionary (we need to convert parameters to ordered dictionary as required by `BaseMLImputer`) parameters = {} for i in range ( len ( coefs )): parameters [ f 'coef_ { i } ' ] = coefs [ i ] parameters [ f 'intercept_ { i } ' ] = intercept [ i ] return OrderedDict ( parameters ) def set_imp_model_params ( self , updated_model_dict : OrderedDict , params : dict ) -> None : \"\"\" Set model parameters Args: updated_model_dict: global model parameters dictionary params: parameters for set parameters function \"\"\" # This method is used to set the parameters of the imputation model for a given feature (update models) # get feature index of imputation models feature_idx = params [ 'feature_idx' ] imp_model = self . imp_models [ feature_idx ] # set parameters to sklearn model coefs = [] intercepts = [] for i in range ( len ( imp_model . coefs_ )): coefs . append ( updated_model_dict [ f 'coef_ { i } ' ]) intercepts . append ( updated_model_dict [ f 'intercept_ { i } ' ]) imp_model . coefs_ = coefs imp_model . intercepts_ = intercepts def fit ( self , X : np . array , y : np . array , missing_mask : np . array , params : dict ) -> dict : \"\"\" Fit imputer to train local imputation models Args: X: np.array - float numpy array features y: np.array - target missing_mask: np.array - missing mask params: parameters for local training \"\"\" # This method is used to fit the imputation model for a given feature # get complete data of the feature feature_idx = params [ 'feature_idx' ] row_mask = missing_mask [:, feature_idx ] X_train = X [ ~ row_mask ][:, np . arange ( X . shape [ 1 ]) != feature_idx ] y_train = X [ ~ row_mask ][:, feature_idx ] # fit MLP imputation models estimator = self . imp_models [ feature_idx ] estimator . fit ( X_train , y_train ) y_pred = estimator . predict ( X_train ) loss = np . mean (( y_pred - y_train ) ** 2 ) self . fit_res_history [ feature_idx ] . append ({ 'loss' : loss , 'sample_size' : X_train . shape [ 0 ] }) return { 'loss' : loss , 'sample_size' : X_train . shape [ 0 ] } def impute ( self , X : np . array , y : np . array , missing_mask : np . array , params : dict ) -> np . ndarray : \"\"\" Impute missing values using an imputation model Args: X (np.array): numpy array of features y (np.array): numpy array of target missing_mask (np.array): missing mask params (dict): parameters for imputation Returns: np.ndarray: imputed data - numpy array - same dimension as X \"\"\" # This method is used to impute the missing values using the imputation model for a given feature # get incomplete data of the feature feature_idx = params [ 'feature_idx' ] row_mask = missing_mask [:, feature_idx ] if np . sum ( row_mask ) == 0 : return X # impute missing values X_test = X [ row_mask ][:, np . arange ( X . shape [ 1 ]) != feature_idx ] estimator = self . imp_models [ feature_idx ] imputed_values = estimator . predict ( X_test ) imputed_values = np . clip ( imputed_values , self . min_values [ feature_idx ], self . max_values [ feature_idx ]) X [ row_mask , feature_idx ] = np . squeeze ( imputed_values ) return X def get_fit_res ( self , params : dict ) -> dict : # This method is used to get fit results for a given feature from the fit history try : feature_idx = params [ 'feature_idx' ] except KeyError : raise ValueError ( \"Feature index not found in params\" ) return self . fit_res_history [ feature_idx ][ - 1 ]","title":"Build New Imputer"},{"location":"tutorials/develop_new_method/#register-new-imputer-to-environment","text":"Once finishing develop new imputer, it needs to be registered into fedimpute, so it can be used by constructed environment. We need to call register_imputer method from env.register object. It takes name of imputer, class of imputer, workflow associated with imputer and a list of supported federated strategy of imputer. % load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . register . register_imputer ( name = 'mlp_mice' , # name of we give to the new imputer imputer = MLPICEImputer , # the class of the new imputer we just developed workflow = 'ice' , # because it is ICE imputation, we use 'ice' workflow fed_strategy = [ 'local' , 'fedmice' ] # we support local and fedmice strategy for this imputer ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload # then we can use the new imputer in the environment and run the federated imputation env . configuration ( imputer = 'mlp_mice' , fed_strategy = 'local' , workflow_params = { 'log_metric' : None }) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 2 ) \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mlp_mice , fed - strategy : local - Client 1 : imputer : mlp_mice , fed - strategy : local - Client 2 : imputer : mlp_mice , fed - strategy : local - Client 3 : imputer : mlp_mice , fed - strategy : local Server : fed - strategy : local ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m \u001b [ 1 mInitial : imp_rmse : 0.1658 imp_ws : 0.0827 \u001b [ 0 m ICE Iterations : 0 %| | 0 / 20 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 0 : imp_rmse : 0.2145 imp_ws : 0.1100 loss : 0.0074 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 1 : imp_rmse : 0.2279 imp_ws : 0.1138 loss : 0.0065 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 2 : imp_rmse : 0.2276 imp_ws : 0.1131 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 3 : imp_rmse : 0.2351 imp_ws : 0.1153 loss : 0.0061 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 4 : imp_rmse : 0.2383 imp_ws : 0.1164 loss : 0.0062 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 5 : imp_rmse : 0.2378 imp_ws : 0.1158 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 6 : imp_rmse : 0.2397 imp_ws : 0.1158 loss : 0.0059 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 7 : imp_rmse : 0.2432 imp_ws : 0.1186 loss : 0.0063 \u001b [ 0 m Feature_idx : 0 %| | 0 / 8 [ 00 : 00 <? , ? it / s ] \u001b [ 1 mEpoch 8 : imp_rmse : 0.2389 imp_ws : 0.1167 loss : 0.0063 \u001b [ 0 m \u001b [ 1 mAll clients converged , iteration 8 \u001b [ 0 m \u001b [ 1 mFinal : imp_rmse : 0.2389 imp_ws : 0.1167 \u001b [ 0 m \u001b [ 32 m \u001b [ 1 mFinished . Running time : 65.1471 seconds \u001b [ 0 m","title":"Register New Imputer to Environment"},{"location":"tutorials/real_scenario/","text":"import numpy as np import pandas as pd import tabulate Load Data % load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data , column_check from fedimpute.scenario import ScenarioBuilder data , data_config = load_data ( \"fed_heart_disease\" ) scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_real_scenario ( data , data_config , ) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 94 , 15 ) Missing Mechanism Category : MCAR Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 244 , 15 ) ( 28 , 15 ) ( 244 , 14 ) 0.00 0 / 14 6077 C2 ( 237 , 15 ) ( 27 , 15 ) ( 237 , 14 ) 0.05 4 / 14 577 C3 ( 99 , 15 ) ( 11 , 15 ) ( 99 , 14 ) 0.01 3 / 14 7231 C4 ( 162 , 15 ) ( 18 , 15 ) ( 162 , 14 ) 0.10 5 / 14 5504 ================================================================== scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], data_type = 'train' , fontsize = 20 , save_path = './plots/real_pattern_train.png' ) scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], data_type = 'test' , fontsize = 20 , save_path = './plots/real_pattern_test.png' ) Running Federated Imputation % load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' , workflow_params = { 'early_stopping_metric' : 'loss' }) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 1 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mice , fed - strategy : fedmice - Client 1 : imputer : mice , fed - strategy : fedmice - Client 2 : imputer : mice , fed - strategy : fedmice - Client 3 : imputer : mice , fed - strategy : fedmice Server : fed - strategy : fedmice ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m ICE Iterations : 0 %| | 0 / 20 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] \u001b [ 32 m \u001b [ 1 mFinished . Running time : 0.6903 seconds \u001b [ 0 m Evaluation % load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator evaluator = Evaluator () X_train_imps , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train_imp' , include_y = True ) X_tests , y_tests = env . get_data ( client_ids = 'all' , data_type = 'test' , include_y = True ) X_test_imps = env . get_data ( client_ids = 'all' , data_type = 'test_imp' ) X_global_test , y_global_test = env . get_data ( data_type = 'global_test' , include_y = True ) X_global_test_imp = env . get_data ( data_type = 'global_test_imp' ) data_config = env . get_data ( data_type = 'config' ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Federated Prediction evaluator . run_fed_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_fed_regression_results () Federated Logit Regression Result ============================================================================== Dep. Variable: num No. Observations: 742 Model: Logit Df Residuals: 727 Method: MLE Df Model: 14 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.4122 Time: 16:45:26 Log-Likelihood: -299.78 converged: True LL-Null: -509.99 Covariance Type: nonrobust LLR p-value: 6.246e-81 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -2.0137 0.211 -9.561 0.000 -2.426 -1.601 age 1.2148 0.095 12.775 0.000 1.028 1.401 trestbps 0.7584 0.205 3.694 0.000 0.356 1.161 chol -0.8222 0.082 -10.029 0.000 -0.983 -0.662 thalach -1.2272 0.122 -10.030 0.000 -1.467 -0.987 oldpeak 4.1905 0.180 23.263 0.000 3.837 4.544 slope 0.9589 0.081 11.897 0.000 0.801 1.117 sex_1.0 1.2922 0.077 16.878 0.000 1.142 1.442 cp_2.0 -0.8841 0.072 -12.272 0.000 -1.025 -0.743 cp_3.0 -0.3322 0.060 -5.506 0.000 -0.450 -0.214 cp_4.0 1.1759 0.060 19.538 0.000 1.058 1.294 fbs_0.0 -1.6318 0.101 -16.154 0.000 -1.830 -1.434 fbs_1.0 -0.6695 0.103 -6.470 0.000 -0.872 -0.467 exang_0.0 -0.3233 0.037 -8.806 0.000 -0.395 -0.251 exang_1.0 0.7180 0.038 19.114 0.000 0.644 0.792 ============================================================================== ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_test_imps , y_tests = y_tests , X_test_global = X_global_test_imp , y_test_global = y_global_test , data_config = data_config , model_name = 'lr' , seed = 0 ) evaluator . show_fed_prediction_results () (149, 14) (149,) 1.09it/s] =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.714 0.692 0.713 0.771 Client 2 0.926 0.889 0.988 0.981 Client 3 0.364 0.533 0.000 0.798 Client 4 0.500 0.609 0.462 0.697 ---------- ---------- ---------- ---------- ---------- Global 0.809 0.804 0.891 0.903 ===============================================================","title":"Real World Distributed Imputation"},{"location":"tutorials/real_scenario/#load-data","text":"% load_ext autoreload % autoreload 2 from fedimpute.data_prep import load_data , display_data , column_check from fedimpute.scenario import ScenarioBuilder data , data_config = load_data ( \"fed_heart_disease\" ) scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_real_scenario ( data , data_config , ) scenario_builder . summarize_scenario () The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload ================================================================== Scenario Summary ================================================================== Total clients : 4 Global Test Data : ( 94 , 15 ) Missing Mechanism Category : MCAR Clients Data Summary : Train Test Miss MS Ratio MS Feature Seed -- -------- ------- -------- ---------- ------------ ------ C1 ( 244 , 15 ) ( 28 , 15 ) ( 244 , 14 ) 0.00 0 / 14 6077 C2 ( 237 , 15 ) ( 27 , 15 ) ( 237 , 14 ) 0.05 4 / 14 577 C3 ( 99 , 15 ) ( 11 , 15 ) ( 99 , 14 ) 0.01 3 / 14 7231 C4 ( 162 , 15 ) ( 18 , 15 ) ( 162 , 14 ) 0.10 5 / 14 5504 ================================================================== scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], data_type = 'train' , fontsize = 20 , save_path = './plots/real_pattern_train.png' ) scenario_builder . visualize_missing_pattern ( client_ids = [ 0 , 1 , 2 , 3 ], data_type = 'test' , fontsize = 20 , save_path = './plots/real_pattern_test.png' )","title":"Load Data"},{"location":"tutorials/real_scenario/#running-federated-imputation","text":"% load_ext autoreload % autoreload 2 from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' , workflow_params = { 'early_stopping_metric' : 'loss' }) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation ( verbose = 1 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload \u001b [ 1 mSetting up clients ... \u001b [ 0 m \u001b [ 1 mSetting up server ... \u001b [ 0 m \u001b [ 1 mSetting up workflow ... \u001b [ 0 m \u001b [ 1 mEnvironment setup complete . \u001b [ 0 m ============================================================ Environment Information : ============================================================ Workflow : ICE ( Imputation via Chain Equation ) Clients : - Client 0 : imputer : mice , fed - strategy : fedmice - Client 1 : imputer : mice , fed - strategy : fedmice - Client 2 : imputer : mice , fed - strategy : fedmice - Client 3 : imputer : mice , fed - strategy : fedmice Server : fed - strategy : fedmice ============================================================ \u001b [ 32 m \u001b [ 1 mImputation Start ... \u001b [ 0 m ICE Iterations : 0 %| | 0 / 20 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] Feature_idx : 0 %| | 0 / 14 [ 00 : 00 <? , ? it / s ] \u001b [ 32 m \u001b [ 1 mFinished . Running time : 0.6903 seconds \u001b [ 0 m","title":"Running Federated Imputation"},{"location":"tutorials/real_scenario/#evaluation","text":"% load_ext autoreload % autoreload 2 from fedimpute.evaluation import Evaluator evaluator = Evaluator () X_train_imps , y_trains = env . get_data ( client_ids = 'all' , data_type = 'train_imp' , include_y = True ) X_tests , y_tests = env . get_data ( client_ids = 'all' , data_type = 'test' , include_y = True ) X_test_imps = env . get_data ( client_ids = 'all' , data_type = 'test_imp' ) X_global_test , y_global_test = env . get_data ( data_type = 'global_test' , include_y = True ) X_global_test_imp = env . get_data ( data_type = 'global_test_imp' ) data_config = env . get_data ( data_type = 'config' ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload","title":"Evaluation"},{"location":"tutorials/real_scenario/#federated-prediction","text":"evaluator . run_fed_regression_analysis ( X_train_imps = X_train_imps , y_trains = y_trains , data_config = data_config ) evaluator . show_fed_regression_results () Federated Logit Regression Result ============================================================================== Dep. Variable: num No. Observations: 742 Model: Logit Df Residuals: 727 Method: MLE Df Model: 14 Date: Mon, 21 Apr 2025 Pseudo R-squ.: 0.4122 Time: 16:45:26 Log-Likelihood: -299.78 converged: True LL-Null: -509.99 Covariance Type: nonrobust LLR p-value: 6.246e-81 ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -2.0137 0.211 -9.561 0.000 -2.426 -1.601 age 1.2148 0.095 12.775 0.000 1.028 1.401 trestbps 0.7584 0.205 3.694 0.000 0.356 1.161 chol -0.8222 0.082 -10.029 0.000 -0.983 -0.662 thalach -1.2272 0.122 -10.030 0.000 -1.467 -0.987 oldpeak 4.1905 0.180 23.263 0.000 3.837 4.544 slope 0.9589 0.081 11.897 0.000 0.801 1.117 sex_1.0 1.2922 0.077 16.878 0.000 1.142 1.442 cp_2.0 -0.8841 0.072 -12.272 0.000 -1.025 -0.743 cp_3.0 -0.3322 0.060 -5.506 0.000 -0.450 -0.214 cp_4.0 1.1759 0.060 19.538 0.000 1.058 1.294 fbs_0.0 -1.6318 0.101 -16.154 0.000 -1.830 -1.434 fbs_1.0 -0.6695 0.103 -6.470 0.000 -0.872 -0.467 exang_0.0 -0.3233 0.037 -8.806 0.000 -0.395 -0.251 exang_1.0 0.7180 0.038 19.114 0.000 0.644 0.792 ============================================================================== ret = evaluator . run_fed_prediction ( X_train_imps = X_train_imps , y_trains = y_trains , X_tests = X_test_imps , y_tests = y_tests , X_test_global = X_global_test_imp , y_test_global = y_global_test , data_config = data_config , model_name = 'lr' , seed = 0 ) evaluator . show_fed_prediction_results () (149, 14) (149,) 1.09it/s] =============================================================== Downstream Prediction (Fed) =============================================================== Personalized accuracy f1 auc prc -------------- ---------- ---------- ---------- ---------- Client 1 0.714 0.692 0.713 0.771 Client 2 0.926 0.889 0.988 0.981 Client 3 0.364 0.533 0.000 0.798 Client 4 0.500 0.609 0.462 0.697 ---------- ---------- ---------- ---------- ---------- Global 0.809 0.804 0.891 0.903 ===============================================================","title":"Federated Prediction"},{"location":"user-guide/data_prep/","text":"Dataset and Preprocessing The first step for using FedImpute is to prepare the data. Input Data Format and Preprocessing The data should be tabular data in the form of a numpy array ( <np.ndarray> ) or List of numpy arrays for those naturally partitioned federated data , where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced. Currently, FedImpute only supports the numerical typed data, for categorical data, you need to one-hot encode them into binary features. Required Preprocessing Steps There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ | Ordering Features To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset. One-hot Encoding Categorical Features Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute. Data Normalization (Optional) It is recommended to normalize the numerical features in the dataset within range of 0 and 1. Helper Functions for Preprocessing FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features , one_hot_encoding # Example for data with numpy array data = ... data = ordering_features ( data , numerical_cols = [ 0 , 1 , 3 , 4 , 8 ], target_col =- 1 ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) # Example data with pandas dataframe data = ... data = ordering_features ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) - ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. - one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) Data Configuration Dictionary To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target' : 'house_price' , 'task_type' : 'classification' , 'natural_partition' : False } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . natural_partition : Whether the data is naturally partitioned into different parties. If it is, set it to True . Otherwise, set it to False .","title":"Data Preparation"},{"location":"user-guide/data_prep/#dataset-and-preprocessing","text":"The first step for using FedImpute is to prepare the data.","title":"Dataset and Preprocessing"},{"location":"user-guide/data_prep/#input-data-format-and-preprocessing","text":"The data should be tabular data in the form of a numpy array ( <np.ndarray> ) or List of numpy arrays for those naturally partitioned federated data , where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced. Currently, FedImpute only supports the numerical typed data, for categorical data, you need to one-hot encode them into binary features.","title":"Input Data Format and Preprocessing"},{"location":"user-guide/data_prep/#required-preprocessing-steps","text":"There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ |","title":"Required Preprocessing Steps"},{"location":"user-guide/data_prep/#ordering-features","text":"To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset.","title":"Ordering Features"},{"location":"user-guide/data_prep/#one-hot-encoding-categorical-features","text":"Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute.","title":"One-hot Encoding Categorical Features"},{"location":"user-guide/data_prep/#data-normalization-optional","text":"It is recommended to normalize the numerical features in the dataset within range of 0 and 1.","title":"Data Normalization (Optional)"},{"location":"user-guide/data_prep/#helper-functions-for-preprocessing","text":"FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features , one_hot_encoding # Example for data with numpy array data = ... data = ordering_features ( data , numerical_cols = [ 0 , 1 , 3 , 4 , 8 ], target_col =- 1 ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) # Example data with pandas dataframe data = ... data = ordering_features ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) - ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. - one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' )","title":"Helper Functions for Preprocessing"},{"location":"user-guide/data_prep/#data-configuration-dictionary","text":"To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target' : 'house_price' , 'task_type' : 'classification' , 'natural_partition' : False } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . natural_partition : Whether the data is naturally partitioned into different parties. If it is, set it to True . Otherwise, set it to False .","title":"Data Configuration Dictionary"},{"location":"user-guide/evaluation/","text":"Evaluation of Imputation Outcomes Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Local Prediction : Evaluate the performance based on downstream local prediction tasks using imputed data. Federated Prediction : Evaluate the performance based on downstream federated prediction task using imputed data. Basic Usage The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all () The Evaluator.evaluate_all() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results_all() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect. Supported Evaluation The following evaluation metrics are supported for each evaluation aspect: Imputation Quality User can use the specific evaluate_imputation_quality() method in evaluation.Evaluator class provides functionalities to evaluate the quality of imputed data across clients comprehensively. It has several parameters: X_train_imps : lists of client-specific imputed datasets X_train_origins : list of client-specific original complete datasets X_train_masks : list of client-specific missing value masks metrics : to denote the list of metrics for evaluation. Metrics: Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions. User can use show_imp_results() to get the formatted results of evaluation. from fedimpute.evaluation import Evaluator X_trains = env.get_data(client_ids='all', data_type = 'train') X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_train_masks = env.get_data(client_ids='all', data_type = 'train_mask') evaluator = Evaluator() ret = evaluator.evaluate_imp_quality( X_train_imps = X_train_imps, X_train_origins = X_trains, X_train_masks = X_train_masks, metrics = ['rmse', 'nrmse', 'sliced-ws'] ) evaluator.show_imp_results() Imputation Quality via tSNE visualization Evaluator class also provides a method called tsne_visualization() to give the visualized comparison of similarity between the imputed data and the original data (ground-truth data). It visualizes the t-Distributed Stochastic Neighbor Embedding (t-SNE) of imputed data and original data so that the user can visually assess the effectiveness of the imputation outcome. tsne_visualization() takes parameters including client\u2019s imputation data ( X_imp ) and original data (ground-truth data) ( X_origin ) and a random seed ( seed ) used for calculating t-SNE embedding. X_trains = env.get_data(client_ids='all', data_type = 'train') X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') evaluator.tsne_visualization( X_imps = X_train_imps, X_origins = X_trains, seed = 0 ) Local Regression Analysis The run_local_regression_analysis() method in the evaluation.Evaluator class provides functionality for evaluation via local regression analysis tasks. It accepts several parameters: X_train_imps, y_trains : lists of client-specific imputed training datasets and targets data_config : the data configuration dictionary. client_ids : the list of clients ids to used for analysis, default to use all clients The method returns a Dict containing evaluation results. Users can utilize the show_local_regression_results(client_idx) method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_trains, y_trains = env.get_data(client_ids='all', data_type = 'train', include_y=True) X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') data_config = env.get_data(data_type = 'config') ret = evaluator.run_local_regression_analysis( X_train_imps = X_train_imps, y_trains = y_trains, data_config = data_config ) evaluator.show_local_regression_results() Local Prediction After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. The run_local_prediction() method in the evaluation.Evaluator class provides functionality for evaluation via local prediction tasks. It accepts several parameters: X_train_imps, y_train : lists of client-specific imputed training datasets and targets X_tests, y_tests : lists of client-specific local test datasets and targets model : a model specification parameter. The method currently implements three built-in downstream prediction models: linear models ('lr'), random forests ('rf'), and two-layer neural networks ('nn'). client_ids : the list of clients ids to used for analysis, default to use all clients The method trains prediction models for each client using the imputed training data and evaluates performance on the corresponding test data. For classification tasks, the evaluation metrics include accuracy, F1-score, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC). Mean squared error and R2 score are computed for regression tasks. show_local_prediction_results() will give a formatted result summary for the evaluation. X_trains, y_trains = env.get_data(client_ids='all', data_type = 'train', include_y=True) X_tests, y_tests = env.get_data(client_ids='all', data_type = 'test', include_y=True) X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') data_config = env.get_data(data_type = 'config') ret = evaluator.evaluate_local_pred( X_train_imps = X_train_imps, X_train_origins = X_trains, y_trains = y_trains, X_tests = X_tests, y_tests = y_tests, data_config = data_config, model = 'nn', seed= 0 ) evaluator.show_local_prediction_results() Federated Regression Analysis The run_fed_regression_analysis() method in the evaluation.Evaluator class provides functionality for evaluation via federated regression analysis tasks. It accepts several parameters: X_train_imps, y_trains : lists of client-specific imputed training data and targets data_config : the data configuration dictionary. The method returns a Dict containing evaluation results. Users can utilize the show_fed_regression_results() method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_trains, y_trains = env.get_data( client_ids='all', data_type = 'train', include_y=True ) data_config = env.get_data(data_type = 'config') ret = evaluator.run_fed_regression_analysis( X_train_imps = X_train_imps, y_trains = y_trains, data_config = data_config ) evaluator.show_fed_regression_results() Federated Prediction We implement federated prediction functionality by run_fed_prediction() method. The current implementation supports federated prediction using a two-layer neural network with Federated Averaging (FedAvg) as the federated learning strategy. We will include more federated models in the future. Similarly, it uses accuracy, F1-Score, AUROC, AUPRC for classification tasks, and mean square error, R2 score for regression tasks. It accepts multiple parameters: X_train_imps, y_trains : lists of client-specific imputed training data and targets X_tests, y_tests : lists of client-specific local test data and targets X_test_global, y_test_global : global test data model_name : the name of the model to be used for federated prediction. Currently, federated models including lr , svm , rf , xgboost , nn are supported. train_params : the parameters for the federated learning training. model_params : the parameters for the model. seed : the random seed for the evaluation. The method returns a Dict containing evaluation results. Users can utilize the show_fed_pred_result() method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_trains, y_trains = env.get_data( client_ids='all', data_type = 'train', include_y=True ) X_tests, y_tests = env.get_data( client_ids='all', data_type = 'test', include_y=True ) X_global_test, y_global_test = env.get_data( data_type = 'global_test', include_y = True ) data_config = env.get_data(data_type = 'config') ret = evaluator.run_fed_prediction( X_train_imps = X_train_imps, X_train_origins = X_trains, y_trains = y_trains, X_tests = X_tests, y_tests = y_tests, X_test_global = X_global_test, y_test_global = y_global_test, data_config = data_config, train_params = { 'global_epoch': 100, 'local_epoch': 10, 'fine_tune_epoch': 200, }, seed= 0 ) evaluator.show_fed_prediction_results() Save Evaluation Results The evaluation module provides convenient interfaces for presenting and exporting the results. All evaluation functions return results in a dictionary format, which can be formatted into readable tables through dedicated display functions, including show_imp_results() , show_local_prediction_results() , show_fed_prediction_results() for each evaluation aspects. For further analysis and reporting, the export_results() method supports exporting results to different formats, including pandas.DataFrame and structured dictionaries.","title":"Evaluation"},{"location":"user-guide/evaluation/#evaluation-of-imputation-outcomes","text":"Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Local Prediction : Evaluate the performance based on downstream local prediction tasks using imputed data. Federated Prediction : Evaluate the performance based on downstream federated prediction task using imputed data.","title":"Evaluation of Imputation Outcomes"},{"location":"user-guide/evaluation/#basic-usage","text":"The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator () ret = evaluator . evaluate_all ( env , metrics = [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ] ) evaluator . show_results_all () The Evaluator.evaluate_all() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results_all() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect.","title":"Basic Usage"},{"location":"user-guide/evaluation/#supported-evaluation","text":"The following evaluation metrics are supported for each evaluation aspect:","title":"Supported Evaluation"},{"location":"user-guide/evaluation/#imputation-quality","text":"User can use the specific evaluate_imputation_quality() method in evaluation.Evaluator class provides functionalities to evaluate the quality of imputed data across clients comprehensively. It has several parameters: X_train_imps : lists of client-specific imputed datasets X_train_origins : list of client-specific original complete datasets X_train_masks : list of client-specific missing value masks metrics : to denote the list of metrics for evaluation. Metrics: Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions. User can use show_imp_results() to get the formatted results of evaluation. from fedimpute.evaluation import Evaluator X_trains = env.get_data(client_ids='all', data_type = 'train') X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_train_masks = env.get_data(client_ids='all', data_type = 'train_mask') evaluator = Evaluator() ret = evaluator.evaluate_imp_quality( X_train_imps = X_train_imps, X_train_origins = X_trains, X_train_masks = X_train_masks, metrics = ['rmse', 'nrmse', 'sliced-ws'] ) evaluator.show_imp_results()","title":"Imputation Quality"},{"location":"user-guide/evaluation/#imputation-quality-via-tsne-visualization","text":"Evaluator class also provides a method called tsne_visualization() to give the visualized comparison of similarity between the imputed data and the original data (ground-truth data). It visualizes the t-Distributed Stochastic Neighbor Embedding (t-SNE) of imputed data and original data so that the user can visually assess the effectiveness of the imputation outcome. tsne_visualization() takes parameters including client\u2019s imputation data ( X_imp ) and original data (ground-truth data) ( X_origin ) and a random seed ( seed ) used for calculating t-SNE embedding. X_trains = env.get_data(client_ids='all', data_type = 'train') X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') evaluator.tsne_visualization( X_imps = X_train_imps, X_origins = X_trains, seed = 0 )","title":"Imputation Quality via tSNE visualization"},{"location":"user-guide/evaluation/#local-regression-analysis","text":"The run_local_regression_analysis() method in the evaluation.Evaluator class provides functionality for evaluation via local regression analysis tasks. It accepts several parameters: X_train_imps, y_trains : lists of client-specific imputed training datasets and targets data_config : the data configuration dictionary. client_ids : the list of clients ids to used for analysis, default to use all clients The method returns a Dict containing evaluation results. Users can utilize the show_local_regression_results(client_idx) method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_trains, y_trains = env.get_data(client_ids='all', data_type = 'train', include_y=True) X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') data_config = env.get_data(data_type = 'config') ret = evaluator.run_local_regression_analysis( X_train_imps = X_train_imps, y_trains = y_trains, data_config = data_config ) evaluator.show_local_regression_results()","title":"Local Regression Analysis"},{"location":"user-guide/evaluation/#local-prediction","text":"After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. The run_local_prediction() method in the evaluation.Evaluator class provides functionality for evaluation via local prediction tasks. It accepts several parameters: X_train_imps, y_train : lists of client-specific imputed training datasets and targets X_tests, y_tests : lists of client-specific local test datasets and targets model : a model specification parameter. The method currently implements three built-in downstream prediction models: linear models ('lr'), random forests ('rf'), and two-layer neural networks ('nn'). client_ids : the list of clients ids to used for analysis, default to use all clients The method trains prediction models for each client using the imputed training data and evaluates performance on the corresponding test data. For classification tasks, the evaluation metrics include accuracy, F1-score, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC). Mean squared error and R2 score are computed for regression tasks. show_local_prediction_results() will give a formatted result summary for the evaluation. X_trains, y_trains = env.get_data(client_ids='all', data_type = 'train', include_y=True) X_tests, y_tests = env.get_data(client_ids='all', data_type = 'test', include_y=True) X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') data_config = env.get_data(data_type = 'config') ret = evaluator.evaluate_local_pred( X_train_imps = X_train_imps, X_train_origins = X_trains, y_trains = y_trains, X_tests = X_tests, y_tests = y_tests, data_config = data_config, model = 'nn', seed= 0 ) evaluator.show_local_prediction_results()","title":"Local Prediction"},{"location":"user-guide/evaluation/#federated-regression-analysis","text":"The run_fed_regression_analysis() method in the evaluation.Evaluator class provides functionality for evaluation via federated regression analysis tasks. It accepts several parameters: X_train_imps, y_trains : lists of client-specific imputed training data and targets data_config : the data configuration dictionary. The method returns a Dict containing evaluation results. Users can utilize the show_fed_regression_results() method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_trains, y_trains = env.get_data( client_ids='all', data_type = 'train', include_y=True ) data_config = env.get_data(data_type = 'config') ret = evaluator.run_fed_regression_analysis( X_train_imps = X_train_imps, y_trains = y_trains, data_config = data_config ) evaluator.show_fed_regression_results()","title":"Federated Regression Analysis"},{"location":"user-guide/evaluation/#federated-prediction","text":"We implement federated prediction functionality by run_fed_prediction() method. The current implementation supports federated prediction using a two-layer neural network with Federated Averaging (FedAvg) as the federated learning strategy. We will include more federated models in the future. Similarly, it uses accuracy, F1-Score, AUROC, AUPRC for classification tasks, and mean square error, R2 score for regression tasks. It accepts multiple parameters: X_train_imps, y_trains : lists of client-specific imputed training data and targets X_tests, y_tests : lists of client-specific local test data and targets X_test_global, y_test_global : global test data model_name : the name of the model to be used for federated prediction. Currently, federated models including lr , svm , rf , xgboost , nn are supported. train_params : the parameters for the federated learning training. model_params : the parameters for the model. seed : the random seed for the evaluation. The method returns a Dict containing evaluation results. Users can utilize the show_fed_pred_result() method in the evaluation.Evaluator class to print a formatted output of the evaluation results. X_train_imps = env.get_data(client_ids='all', data_type = 'train_imp') X_trains, y_trains = env.get_data( client_ids='all', data_type = 'train', include_y=True ) X_tests, y_tests = env.get_data( client_ids='all', data_type = 'test', include_y=True ) X_global_test, y_global_test = env.get_data( data_type = 'global_test', include_y = True ) data_config = env.get_data(data_type = 'config') ret = evaluator.run_fed_prediction( X_train_imps = X_train_imps, X_train_origins = X_trains, y_trains = y_trains, X_tests = X_tests, y_tests = y_tests, X_test_global = X_global_test, y_test_global = y_global_test, data_config = data_config, train_params = { 'global_epoch': 100, 'local_epoch': 10, 'fine_tune_epoch': 200, }, seed= 0 ) evaluator.show_fed_prediction_results()","title":"Federated Prediction"},{"location":"user-guide/evaluation/#save-evaluation-results","text":"The evaluation module provides convenient interfaces for presenting and exporting the results. All evaluation functions return results in a dictionary format, which can be formatted into readable tables through dedicated display functions, including show_imp_results() , show_local_prediction_results() , show_fed_prediction_results() for each evaluation aspects. For further analysis and reporting, the export_results() method supports exporting results to different formats, including pandas.DataFrame and structured dictionaries.","title":"Save Evaluation Results"},{"location":"user-guide/fed_imp/","text":"Executing Distributed Imputation Algorithms The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms. Overview and Basic Usage Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation () Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" Environment Configuration The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation. Supported Federated Imputation Algorithms Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Mean Non-NN local , fedmean mean MEAN - EM Non-NN local , fedem em EM EM , FedEM MICE Non-NN local , fedmice mice ICE FedICE MissForest Non-NN local , fedtree missforest ICE MissForest , Fed Randomforest MIWAE NN local , fedavg , ... miwae JM MIWAE GAIN NN local , fedavg , ... gain JM GAIN Not-MIWAE NN local , fedavg , ... notmiwae JM Not-MIWAE GNR NN local , fedavg , ... gnr JM GNR Federated Strategies: Method Type Fed_strategy(code) Reference Local non-federated local - FedMean traditional fedmean - FedEM traditional fedem FedEM FedMICE traditional fedmice FedMICE FedTree traditional fedtree FedTree FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAdam global FL fedadam FedAdam FedAdagrad global FL fedadagrad FedAdaGrad FedYogi global FL fedyogi FedYogi FedAvg-FT personalized FL fedavg_ft FedAvg-FT Environment Setup After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env . setup_from_simulator ( simulator , verbose = 1 ) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_train_data_ms = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_test_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] global_test = np . random . rand ( 100 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 9 , } clients_seeds = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] env . setup_from_data ( clients_train_data , clients_test_data , clients_train_data_ms , clients_seeds , global_test , data_config , verbose = 1 ) Execute Federated Imputation After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. Currently, we support two types of simulation execution (1) Run FL in sequential mode ( run_type=\"sequential\" ), in this model, there is no parallel, whole processes of imputation for clients run sequantially by using for loop (2) Run federated imputation in parallel mode ( run_type=\"parallel\" ), it will simulate different processes for clients and server and then using workflow to manage communication between clients and server to approach the real world FL environment. env . run_fed_imputation ( run_type = 'squential' ) Monitoring Imputation Process We provide the Tensorboard utilty so that user can monitoring the imputation progress in real time. By using tensorboard, user need to run the following command in the terminal: tensorboard --logdir .logs We also provide another API env.tracker.visualize_imputation_process() it will show the line chart of imputation process measured by imputation quality or loss, it can only be run after imputation finished unlike tensorboard utility. Develop New Federated Imputation Methods FedImputeEnv class supports functionality to register new imputers, strategies and workflows. User can develope their new imputation methods, federated strategies and workflow. Basically, these three components have to work tightly to formalize a federated imputation algorithm. To develop new workflow, user need to implement a new workflow by inherit the Workflow class from fedimpute.execution_environment.workflow . In implementation of workflow, user need to think how to allow clients and server to interact with each other. To develop new federated strategies, user need to implement new strategy for both client and server. To develop new imputers, user need to implement new imputer by inherit the one of two abstract class BaseMLImputer and BaseNNImputer from fedimpute.execution_environment.imputation.base , one for traditional methods and another for generative model based methods, and implement all its abstract methods (interfaces). Because, each imputer is associated with federated strategy and workflow, user need also to think how to make developed new imputer to be compatible with existed or new federated strategy and workflows. Then user can use env.register.register_imputer , env.register.register_strategy , env.register.register_workflow to register these new developed classes. After registeration, user can use them in FedImputeEnv following the same way as using built-in methods. We provided a detailed example in tutorials. Miscellaneous verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Distributed Imputation"},{"location":"user-guide/fed_imp/#executing-distributed-imputation-algorithms","text":"The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms.","title":"Executing Distributed Imputation Algorithms"},{"location":"user-guide/fed_imp/#overview-and-basic-usage","text":"Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv ( debug_mode = False ) env . configuration ( imputer = 'mice' , fed_strategy = 'fedmice' ) env . setup_from_scenario_builder ( scenario_builder = scenario_builder , verbose = 1 ) env . show_env_info () env . run_fed_imputation () Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\"","title":"Overview and Basic Usage"},{"location":"user-guide/fed_imp/#environment-configuration","text":"The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation.","title":"Environment Configuration"},{"location":"user-guide/fed_imp/#supported-federated-imputation-algorithms","text":"Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Mean Non-NN local , fedmean mean MEAN - EM Non-NN local , fedem em EM EM , FedEM MICE Non-NN local , fedmice mice ICE FedICE MissForest Non-NN local , fedtree missforest ICE MissForest , Fed Randomforest MIWAE NN local , fedavg , ... miwae JM MIWAE GAIN NN local , fedavg , ... gain JM GAIN Not-MIWAE NN local , fedavg , ... notmiwae JM Not-MIWAE GNR NN local , fedavg , ... gnr JM GNR Federated Strategies: Method Type Fed_strategy(code) Reference Local non-federated local - FedMean traditional fedmean - FedEM traditional fedem FedEM FedMICE traditional fedmice FedMICE FedTree traditional fedtree FedTree FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAdam global FL fedadam FedAdam FedAdagrad global FL fedadagrad FedAdaGrad FedYogi global FL fedyogi FedYogi FedAvg-FT personalized FL fedavg_ft FedAvg-FT","title":"Supported Federated Imputation Algorithms"},{"location":"user-guide/fed_imp/#environment-setup","text":"After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env . setup_from_simulator ( simulator , verbose = 1 ) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_train_data_ms = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_test_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] global_test = np . random . rand ( 100 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 9 , } clients_seeds = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] env . setup_from_data ( clients_train_data , clients_test_data , clients_train_data_ms , clients_seeds , global_test , data_config , verbose = 1 )","title":"Environment Setup"},{"location":"user-guide/fed_imp/#execute-federated-imputation","text":"After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. Currently, we support two types of simulation execution (1) Run FL in sequential mode ( run_type=\"sequential\" ), in this model, there is no parallel, whole processes of imputation for clients run sequantially by using for loop (2) Run federated imputation in parallel mode ( run_type=\"parallel\" ), it will simulate different processes for clients and server and then using workflow to manage communication between clients and server to approach the real world FL environment. env . run_fed_imputation ( run_type = 'squential' )","title":"Execute Federated Imputation"},{"location":"user-guide/fed_imp/#monitoring-imputation-process","text":"We provide the Tensorboard utilty so that user can monitoring the imputation progress in real time. By using tensorboard, user need to run the following command in the terminal: tensorboard --logdir .logs We also provide another API env.tracker.visualize_imputation_process() it will show the line chart of imputation process measured by imputation quality or loss, it can only be run after imputation finished unlike tensorboard utility.","title":"Monitoring Imputation Process"},{"location":"user-guide/fed_imp/#develop-new-federated-imputation-methods","text":"FedImputeEnv class supports functionality to register new imputers, strategies and workflows. User can develope their new imputation methods, federated strategies and workflow. Basically, these three components have to work tightly to formalize a federated imputation algorithm. To develop new workflow, user need to implement a new workflow by inherit the Workflow class from fedimpute.execution_environment.workflow . In implementation of workflow, user need to think how to allow clients and server to interact with each other. To develop new federated strategies, user need to implement new strategy for both client and server. To develop new imputers, user need to implement new imputer by inherit the one of two abstract class BaseMLImputer and BaseNNImputer from fedimpute.execution_environment.imputation.base , one for traditional methods and another for generative model based methods, and implement all its abstract methods (interfaces). Because, each imputer is associated with federated strategy and workflow, user need also to think how to make developed new imputer to be compatible with existed or new federated strategy and workflows. Then user can use env.register.register_imputer , env.register.register_strategy , env.register.register_workflow to register these new developed classes. After registeration, user can use them in FedImputeEnv following the same way as using built-in methods. We provided a detailed example in tutorials.","title":"Develop New Federated Imputation Methods"},{"location":"user-guide/fed_imp/#miscellaneous","text":"verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Miscellaneous"},{"location":"user-guide/pipeline/","text":"Pipeline API for Benchmarking To facilitate the systematic evaluation of multiple algorithms, the fedimpute framework provides a streamlined pipeline execution interface through the fedimpute.Pipeline class. This pipeline enables concurrent execution and evaluation of multiple algorithms, supporting comprehensive benchmarking studies and comparative analyses. We briefly introduce core functions below. Pipeline Setup and Execution The pipeline workflow begins with a constructed distributed data scenario. Users first need to instantiate an object from fedimpute.Pipeline class. Then, they need to provide the configuration of the algorithms to execute in the pipeline through the setup() method, which accepts two primary parameters: fed_imp_configs : a list of configuration tuples, where each tuple specifies an imputer, associated federated aggregation strategies, and their parameters. evaluation_aspects : a list of evaluation criteria including imputation quality ( imp_quality ), local prediction ( local_pred ), and federated prediction ( fed_pred ). evaluation_params : a dictionary to specify the evaluation process. It includes metrics a list specifying the aspects of evaluation and model which downstream task used for evaluation. Finally, the pipeline execution is via the run_pipeline() method, which takes a scenario object as input. It executes all configured algorithms on the input scenario, performs evaluations, and stores the results. We also have a pipeline_setup_summary() function to provide a summary of constructed pipeline. from fedimpute.pipeline import FedImputePipeline pipeline = FedImputePipeline() pipeline.setup( id = 'benchmark_demo', fed_imp_configs = [ ('em', ['local', 'fedem'], {}, [{}, {}]), ('mice', ['local', 'fedmice'], {}, [{}, {}]), ('gain', ['local', 'fedavg'], {}, [{}, {}]), ], evaluation_params = { 'metrics': ['imp_quality', 'local_pred', 'fed_pred'], 'model': 'lr', }, persist_data = False, description = 'benchmark demonstration' ) pipeline.pipeline_setup_summary() pipeline.run_pipeline( scenario_builder, repeats = 5, verbose = 0 ) Result Analysis and Visualization The pipeline provides two key functions for helping analyze the pipeline execution results: show_pipeline_results() which generates tabular summaries of specific metrics across different algorithms and strategies. plot_pipeline_results() which creates comparative visualizations of performance metrics across different imputation and federated aggregation strategies. pipeline.plot_pipeline_results(metric_aspect = 'fed_pred_personalized', plot_type = 'bar')","title":"Pipeline API"},{"location":"user-guide/pipeline/#pipeline-api-for-benchmarking","text":"To facilitate the systematic evaluation of multiple algorithms, the fedimpute framework provides a streamlined pipeline execution interface through the fedimpute.Pipeline class. This pipeline enables concurrent execution and evaluation of multiple algorithms, supporting comprehensive benchmarking studies and comparative analyses. We briefly introduce core functions below.","title":"Pipeline API for Benchmarking"},{"location":"user-guide/pipeline/#pipeline-setup-and-execution","text":"The pipeline workflow begins with a constructed distributed data scenario. Users first need to instantiate an object from fedimpute.Pipeline class. Then, they need to provide the configuration of the algorithms to execute in the pipeline through the setup() method, which accepts two primary parameters: fed_imp_configs : a list of configuration tuples, where each tuple specifies an imputer, associated federated aggregation strategies, and their parameters. evaluation_aspects : a list of evaluation criteria including imputation quality ( imp_quality ), local prediction ( local_pred ), and federated prediction ( fed_pred ). evaluation_params : a dictionary to specify the evaluation process. It includes metrics a list specifying the aspects of evaluation and model which downstream task used for evaluation. Finally, the pipeline execution is via the run_pipeline() method, which takes a scenario object as input. It executes all configured algorithms on the input scenario, performs evaluations, and stores the results. We also have a pipeline_setup_summary() function to provide a summary of constructed pipeline. from fedimpute.pipeline import FedImputePipeline pipeline = FedImputePipeline() pipeline.setup( id = 'benchmark_demo', fed_imp_configs = [ ('em', ['local', 'fedem'], {}, [{}, {}]), ('mice', ['local', 'fedmice'], {}, [{}, {}]), ('gain', ['local', 'fedavg'], {}, [{}, {}]), ], evaluation_params = { 'metrics': ['imp_quality', 'local_pred', 'fed_pred'], 'model': 'lr', }, persist_data = False, description = 'benchmark demonstration' ) pipeline.pipeline_setup_summary() pipeline.run_pipeline( scenario_builder, repeats = 5, verbose = 0 )","title":"Pipeline Setup and Execution"},{"location":"user-guide/pipeline/#result-analysis-and-visualization","text":"The pipeline provides two key functions for helping analyze the pipeline execution results: show_pipeline_results() which generates tabular summaries of specific metrics across different algorithms and strategies. plot_pipeline_results() which creates comparative visualizations of performance metrics across different imputation and federated aggregation strategies. pipeline.plot_pipeline_results(metric_aspect = 'fed_pred_personalized', plot_type = 'bar')","title":"Result Analysis and Visualization"},{"location":"user-guide/scenario_simulation/","text":"Constructing Distributed (Federated) Missing Data Scenarios In this section, we will demonstrate how to construct federated missing data scenarios using the fedimpute.scenario.ScenarioBuilder module. What is Distributed Missing Data Scenario Distributed Missing Data Scenario represents a distributed network with a number of clients. Each client has its local dataset (we consider horizontal setting, which means these datasets contain same feature space). There are missing values inside local dataset of these clients. Our fedimpute.scenario.ScenarioBuilder module will construct all necessary components for the distributed missing data scenario (e.g., client-specific training and test data, missing data, etc.), which will be used in other modules for conducting federated imputation and prediction. The input to this module is a <np.ndarray> or <pd.DataFrame> dataset or datasets (real federated datasets) and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section. fedimpute.scenario.ScenarioBuilder provides two approaches for scenario construction that cover the most common research settings in distributed imputation: (1) simulation-based scenarios , where data partitioning and missing values are systematically simulated according to user-specified parameters, and (2) real-world scenarios , where data is naturally partitioned across silos with existing missing values. Both approaches output standard formats of data components (1) clients_train_data : client-specific training datasets, (2) clients_train_data_ms : client-specific training datasets with simulated or existed missing values, (3) clients_test_data : client-specific test datasets for local evaluation, and (4) global_test_data : global test dataset for federated evaluation. These structured outputs serve as consistent building blocks for subsequent steps in the fedimpute workflow, including distributed environment initialization, algorithm execution, and performance evaluation. Scenario Construction Interface Constructing Simulated Scenario The fedimpute.scenario.ScenarioBuilder module include the following core functionalities for constructing a simulated scenario: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the a centralized <np.ndarray> or <pd.DataFrame> data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the client-specific data components (clients' local training data, test etc.) The following example shows how to construct a simulated scenario. Firstly, initialize the fedimpute.scenario.ScenarioBuilder class and call the create_simulated_scenario method to simulate the federated missing data scenario. from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario () Supported Data Partition Strategies Data partition can be set by dp_strategy parameter, which takes following options iid-even : iid partition with even sample sizes iid-dir@<alpha> \uff1a iid parititon with sample sizes following dirichlet distribution with parameter alpha to control sample size heterogeneity e.g. iid-dir@0.1 niid-dir@<alpha> : non-iid partition based on some columns with dirichlet ditribution with parameter alpha to control data heterogneity e.g. niid-dir@0.1 niid-path@<n> : non-iid partition based on some columns with pathological distribution (shard partition) with parameter n control heterogeneity niid-path@2 , each client own 2 classes of a column values. Other Parameters for Data Partition num_clients (int) - Number of clients to partition the dataset. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. feature : Split the data samples based on the first feature column. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton. Supported Missing Data Mechanism Type Missing mechansim can be set by ms_mech_type parameter, which supports all commonly used three types of general missing mechanism. The options are shown as below. Refer to how to create missingness in python? for more details. MCAR Missing Mechanism mcar missing completely at random implemented using purely random mask. MAR Missing Mechanism mar_quantile : missing at random based on quantile of values of other features. mar_logit missing at random created based on logit regressoin on values of other features. MNAR Missing Mechanism mnar_logit : missingness based on values of feature itself and other features. mnar_sm_logit : self-masking missingness logit regression based on values of feature itself. mnar_sm_quantile : self-masking missingness based on quantile of values of feature itself. Missing Data Simulation Parameters The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randn : Random normal missing ratio with random float value for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. identity : Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. identity : Each client has the same missing mechanism. random : Random missing mechanism function for each client. We have another parameter ms_scenario which simplify the missing data heterogeneity simulation by providing 5 predefined homogeneous and heterogeneous mechanism settings. It has the following options ( Note: by setting this parameter, you don't need to specify the parameter above for missing mechanism heterogeneity): - mcar : MCAR setting - mar-heter : heterogeneous MAR setting - mar-homo : homogeneous MAR setting - mnar-heter : heterogeneous MNAR setting - mnar-homo : homogenous MNAR setting Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_logit and mnar_sm_logit , mnar_logit mechanism type. Constructing Real Scenario In certain cases, we have real data available with naturally occurring missing values and has well-defined partitions for distribution. To handle such cases, the module provides the create_real_scenario() method to construct distributed missing data scenario corresponding to the given data. Unlike simulation-based construction, this method expects input data as a Python List of <pandas.DataFrame> datasets, where each dataframe represents a client-specific local dataset. The method processes these distributed datasets to generate scenario components in the same standardized format described earlier (e.g. split training and test data for each client's local data and construct a global test dataset for federated prediction), ensuring consistent interfaces for subsequent distributed imputation and evaluation. Parameters : datas (List[pd.DataFrame]): input list of datasets data_config (Dict): data configuration seed (int): random seed for train-test splitting verbose (int): show processing information Usage : from fedimpute.data_prep import load_data, display_data, column_check from fedimpute.scenario import ScenarioBuilder data, data_config = load_data(\"fed_heart_disease\") scenario_builder = ScenarioBuilder() scenario_data = scenario_builder.create_real_scenario( data, data_config, ) scenario_builder.summarize_scenario() Scenario Exploration and Summary The module also provides comprehensive tools for analyzing scenario-specific data for any given distributed missing data scenario through a collection of visualization and analysis interfaces, example of these functions can be found in the tutorials. It includes the following APIs: summarize_scenario(log_to_file, file_path) provides a summary report of the scenario data components, user can choose whether show the summary to save summary report to the disk. visualize_missing_pattern(client_ids: List[int], data_type: str = 'train') visualizes the missing data pattern for client-specific local data. client_ids (List[int]): client ids to show the pattern data_type (str): train or test to show pattern for training data or test data. scenario_builder.visualize_missing_pattern(client_ids=[0, 1, 2, 3]) visualize_missing_distribution(client_ids: List[int], feature_ids: List[int]) visualizes the distribution of missing and observed values for features within client-specific local data. client_ids (List[int]): client ids to show the missing distribution. feature_ids (List[int]): feature indices to set for which feature the missing data distribution to be shown. scenario_builder.visualize_missing_distribution(client_ids = [0, 1], feature_ids = [0, 1, 2, 3, 4]) visualize_data_heterogeneity(client_ids: List[int], distance_method: str = 'swd',) visualizes the heatmap to assess cross-client local data heterogeneity. client_ids (List[int]): client ids to show the information. distance_method (str): method to calculate pair-wise client distance. swd - sliced wasserstein distance over local data. correlation - euclidean distance caculated on feature correlation matrix. scenario_builder.visualize_data_heterogeneity(client_ids=[0, 1, 2, 3], distance_method='swd') predefined setting ms_scenario - parameters mapping: mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"Distributed Missing Data Scenario Builder"},{"location":"user-guide/scenario_simulation/#constructing-distributed-federated-missing-data-scenarios","text":"In this section, we will demonstrate how to construct federated missing data scenarios using the fedimpute.scenario.ScenarioBuilder module.","title":"Constructing Distributed (Federated) Missing Data Scenarios"},{"location":"user-guide/scenario_simulation/#what-is-distributed-missing-data-scenario","text":"Distributed Missing Data Scenario represents a distributed network with a number of clients. Each client has its local dataset (we consider horizontal setting, which means these datasets contain same feature space). There are missing values inside local dataset of these clients. Our fedimpute.scenario.ScenarioBuilder module will construct all necessary components for the distributed missing data scenario (e.g., client-specific training and test data, missing data, etc.), which will be used in other modules for conducting federated imputation and prediction. The input to this module is a <np.ndarray> or <pd.DataFrame> dataset or datasets (real federated datasets) and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section. fedimpute.scenario.ScenarioBuilder provides two approaches for scenario construction that cover the most common research settings in distributed imputation: (1) simulation-based scenarios , where data partitioning and missing values are systematically simulated according to user-specified parameters, and (2) real-world scenarios , where data is naturally partitioned across silos with existing missing values. Both approaches output standard formats of data components (1) clients_train_data : client-specific training datasets, (2) clients_train_data_ms : client-specific training datasets with simulated or existed missing values, (3) clients_test_data : client-specific test datasets for local evaluation, and (4) global_test_data : global test dataset for federated evaluation. These structured outputs serve as consistent building blocks for subsequent steps in the fedimpute workflow, including distributed environment initialization, algorithm execution, and performance evaluation.","title":"What is Distributed Missing Data Scenario"},{"location":"user-guide/scenario_simulation/#scenario-construction-interface","text":"","title":"Scenario Construction Interface"},{"location":"user-guide/scenario_simulation/#constructing-simulated-scenario","text":"The fedimpute.scenario.ScenarioBuilder module include the following core functionalities for constructing a simulated scenario: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the a centralized <np.ndarray> or <pd.DataFrame> data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the client-specific data components (clients' local training data, test etc.) The following example shows how to construct a simulated scenario. Firstly, initialize the fedimpute.scenario.ScenarioBuilder class and call the create_simulated_scenario method to simulate the federated missing data scenario. from fedimpute.scenario import ScenarioBuilder scenario_builder = ScenarioBuilder () scenario_data = scenario_builder . create_simulated_scenario ( data , data_config , num_clients = 4 , dp_strategy = 'iid-even' , ms_scenario = 'mnar-heter' ) print ( 'Results Structure (Dict Keys):' ) print ( list ( scenario_data . keys ())) scenario_builder . summarize_scenario ()","title":"Constructing Simulated Scenario"},{"location":"user-guide/scenario_simulation/#supported-data-partition-strategies","text":"Data partition can be set by dp_strategy parameter, which takes following options iid-even : iid partition with even sample sizes iid-dir@<alpha> \uff1a iid parititon with sample sizes following dirichlet distribution with parameter alpha to control sample size heterogeneity e.g. iid-dir@0.1 niid-dir@<alpha> : non-iid partition based on some columns with dirichlet ditribution with parameter alpha to control data heterogneity e.g. niid-dir@0.1 niid-path@<n> : non-iid partition based on some columns with pathological distribution (shard partition) with parameter n control heterogeneity niid-path@2 , each client own 2 classes of a column values.","title":"Supported Data Partition Strategies"},{"location":"user-guide/scenario_simulation/#other-parameters-for-data-partition","text":"num_clients (int) - Number of clients to partition the dataset. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. feature : Split the data samples based on the first feature column. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton.","title":"Other Parameters for Data Partition"},{"location":"user-guide/scenario_simulation/#supported-missing-data-mechanism-type","text":"Missing mechansim can be set by ms_mech_type parameter, which supports all commonly used three types of general missing mechanism. The options are shown as below. Refer to how to create missingness in python? for more details. MCAR Missing Mechanism mcar missing completely at random implemented using purely random mask. MAR Missing Mechanism mar_quantile : missing at random based on quantile of values of other features. mar_logit missing at random created based on logit regressoin on values of other features. MNAR Missing Mechanism mnar_logit : missingness based on values of feature itself and other features. mnar_sm_logit : self-masking missingness logit regression based on values of feature itself. mnar_sm_quantile : self-masking missingness based on quantile of values of feature itself.","title":"Supported Missing Data Mechanism Type"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-parameters","text":"The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randn : Random normal missing ratio with random float value for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. identity : Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. identity : Each client has the same missing mechanism. random : Random missing mechanism function for each client. We have another parameter ms_scenario which simplify the missing data heterogeneity simulation by providing 5 predefined homogeneous and heterogeneous mechanism settings. It has the following options ( Note: by setting this parameter, you don't need to specify the parameter above for missing mechanism heterogeneity): - mcar : MCAR setting - mar-heter : heterogeneous MAR setting - mar-homo : homogeneous MAR setting - mnar-heter : heterogeneous MNAR setting - mnar-homo : homogenous MNAR setting Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_logit and mnar_sm_logit , mnar_logit mechanism type.","title":"Missing Data Simulation Parameters"},{"location":"user-guide/scenario_simulation/#constructing-real-scenario","text":"In certain cases, we have real data available with naturally occurring missing values and has well-defined partitions for distribution. To handle such cases, the module provides the create_real_scenario() method to construct distributed missing data scenario corresponding to the given data. Unlike simulation-based construction, this method expects input data as a Python List of <pandas.DataFrame> datasets, where each dataframe represents a client-specific local dataset. The method processes these distributed datasets to generate scenario components in the same standardized format described earlier (e.g. split training and test data for each client's local data and construct a global test dataset for federated prediction), ensuring consistent interfaces for subsequent distributed imputation and evaluation. Parameters : datas (List[pd.DataFrame]): input list of datasets data_config (Dict): data configuration seed (int): random seed for train-test splitting verbose (int): show processing information Usage : from fedimpute.data_prep import load_data, display_data, column_check from fedimpute.scenario import ScenarioBuilder data, data_config = load_data(\"fed_heart_disease\") scenario_builder = ScenarioBuilder() scenario_data = scenario_builder.create_real_scenario( data, data_config, ) scenario_builder.summarize_scenario()","title":"Constructing Real Scenario"},{"location":"user-guide/scenario_simulation/#scenario-exploration-and-summary","text":"The module also provides comprehensive tools for analyzing scenario-specific data for any given distributed missing data scenario through a collection of visualization and analysis interfaces, example of these functions can be found in the tutorials. It includes the following APIs: summarize_scenario(log_to_file, file_path) provides a summary report of the scenario data components, user can choose whether show the summary to save summary report to the disk. visualize_missing_pattern(client_ids: List[int], data_type: str = 'train') visualizes the missing data pattern for client-specific local data. client_ids (List[int]): client ids to show the pattern data_type (str): train or test to show pattern for training data or test data. scenario_builder.visualize_missing_pattern(client_ids=[0, 1, 2, 3]) visualize_missing_distribution(client_ids: List[int], feature_ids: List[int]) visualizes the distribution of missing and observed values for features within client-specific local data. client_ids (List[int]): client ids to show the missing distribution. feature_ids (List[int]): feature indices to set for which feature the missing data distribution to be shown. scenario_builder.visualize_missing_distribution(client_ids = [0, 1], feature_ids = [0, 1, 2, 3, 4]) visualize_data_heterogeneity(client_ids: List[int], distance_method: str = 'swd',) visualizes the heatmap to assess cross-client local data heterogeneity. client_ids (List[int]): client ids to show the information. distance_method (str): method to calculate pair-wise client distance. swd - sliced wasserstein distance over local data. correlation - euclidean distance caculated on feature correlation matrix. scenario_builder.visualize_data_heterogeneity(client_ids=[0, 1, 2, 3], distance_method='swd')","title":"Scenario Exploration and Summary"},{"location":"user-guide/scenario_simulation/#predefined-setting-ms_scenario-parameters-mapping","text":"mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"predefined setting ms_scenario - parameters mapping:"}]}