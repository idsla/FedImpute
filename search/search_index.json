{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FedImpute FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios. Features of FedImpute Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs. Quick Start Overview of FedImpute Installation and Basic Usage User Guide Data Preparation Federated Missing Data Scenario Simulation Federated Imputation Evaluation Support and Contact FedImpute is developed by Rutgers Institute for Data Science, Learning, and Applications (i-DLSA) , lead by Professor Jaideep Vaidya . For any questions, please contact Sitao Min for support.","title":"Home"},{"location":"#welcome-to-fedimpute","text":"FedImpute is a benchmarking and evaluation tool to assess the effectiveness of federated imputation across various missing data scenarios.","title":"Welcome to FedImpute"},{"location":"#features-of-fedimpute","text":"Flexible Missing Data Simulation : Provide flexible API to simulate missing data scenarios under various missing data distribution and data partition strategies. Built-in Federated Imputation Methods : Supports multiple imputation techniques, including mean, and model-based approaches, tailored for distributed data. Easy Integration : Designed to be easily extended with federated imputation algorithms and workflows. Customizability : Offers extensive configuration options to adapt the imputation process to specific needs.","title":"Features of FedImpute"},{"location":"#quick-start","text":"Overview of FedImpute Installation and Basic Usage","title":"Quick Start"},{"location":"#user-guide","text":"Data Preparation Federated Missing Data Scenario Simulation Federated Imputation Evaluation","title":"User Guide"},{"location":"#support-and-contact","text":"FedImpute is developed by Rutgers Institute for Data Science, Learning, and Applications (i-DLSA) , lead by Professor Jaideep Vaidya . For any questions, please contact Sitao Min for support.","title":"Support and Contact"},{"location":"about/","text":"Arbor mecum nostro Rhesi Illic spumam Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus! Est niveo serpentem sit Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail ( office . monochrome ( 46 ), koffice_ascii , mailItunes ); node += cable_bezel_menu ; if ( powerFatAtm . clipboard_text_vertical . bccKvmLeaf ( dmaOverwriteAscii , plain ) + passiveCameraWaveform ( 462879 )) { gateway = multicasting ( drive , hubMashupAvatar . floppy_zero_qwerty . rom_linux_log ( digital , dllDebug , 2 )); } Sit est ternisque inter Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Arbor mecum nostro Rhesi"},{"location":"about/#arbor-mecum-nostro-rhesi","text":"","title":"Arbor mecum nostro Rhesi"},{"location":"about/#illic-spumam","text":"Lorem markdownum India exarsit ignibus aequoris Titan et postera vates: corpus, poenae inque. Ut arce, motu, dispar traiecit Lacinia tamen . Fudi vix non membra heros. Lyciae erat nudorum media, dei vicit dolorem frustra leni aequor at concita soror monstris manumque vulnus. Silvis optabat . Incepti de currus, cum urit sperato poenas sine, fata matris aliisque tellurem visum Cycno nupta. Fatigatum aestus!","title":"Illic spumam"},{"location":"about/#est-niveo-serpentem-sit","text":"Quoque Troum. Sermone et nunc tot credite aequem meri quam tutos, peti stravere, gregibus novavit et. Mihi Iuppiter mirabile, quo grave furibunda nunc gemitus, guttura Patraeque funeris favilla. Flores Triptolemus Matri. var pda_postscript_dma = postscript_cmos_mail ( office . monochrome ( 46 ), koffice_ascii , mailItunes ); node += cable_bezel_menu ; if ( powerFatAtm . clipboard_text_vertical . bccKvmLeaf ( dmaOverwriteAscii , plain ) + passiveCameraWaveform ( 462879 )) { gateway = multicasting ( drive , hubMashupAvatar . floppy_zero_qwerty . rom_linux_log ( digital , dllDebug , 2 )); }","title":"Est niveo serpentem sit"},{"location":"about/#sit-est-ternisque-inter","text":"Tamen illis sedare Frigus; natalis rege isset dilaniat auras. Vero deam premis habitat hinc sternuntur meritus pedum et cum. Lemnius crescentemque lugebere flere os illic quamquam facto dextera quae tantum color vestes hoc veniunt magnus quiete: et. Erunt rudis et dolor trahat Philoctete tormentis Atlas Iphide partibus! Cycnum abiit lunae fuit defendere mutantur quadriiugo neci; fui unum, Cycnum agebat? Pater illa ergo virgae lapidumque domitos, sororis tu mitior carmina innixus: confusa sumus. Si et, maerentes ortas Daedalon tectoque genitor famuli. Enim paventem armis dignus vis instructo usque animoque, ignara aper iter est campus laceris in iniecit Oileos. In tepido tamen: artus illa ramisque desinite aures, diversae? Promptum fuit accessit ambiguum querulas consenuere, te alii herbas vox conplexusque. Aestuat mihi opes miser, ratis, circumdata aurum. Hunc mihi exiguas gelidis ab faciem, et Ceyx draconibus, Corythumque diversa, alis Priapi.","title":"Sit est ternisque inter"},{"location":"get_started/","text":"Installation Firstly, install python >= 3.10.0, we have two ways to install Install from pip: pip install fedimpute Install from package repo: git clone https://github.com/idsla/FedImpute cd FedImpute # create virtual env python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate # Install the required packages pip install -r requirements.txt Basic Usage Step 1. Prepare Data import numpy as np data = np . random . rand ( 10000 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 10 , } Step 2. Simulate Federated Missing Data Scenario from fedimpute.simulator import Simulator simulator = Simulator () # classifical simulation function simulation_results = simulator . simulate_scenario ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_mech_type = 'mcar' , verbose = 1 ) # lite simulation function simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'niid-dir@0.1' , ms_scenario = 'mar-heter' , verbose = 1 ) Step 3. Execute Federated Imputation Algorithms Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv () env . reset_env () env . configuration ( imputer = 'gain' , fed_strategy = 'fedavg' , fit_mode = 'fed' ) env . setup_from_simulator ( simulator = simulator , verbose = 1 ) env . run_fed_imputation ( run_type = 'sequential' ) Step 4. Evaluate imputation outcomes from fedimpute.evaluation import Evaluator evaluator = Evaluator () evaluator . evaluate ( env , [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ]) evaluator . show_results ()","title":"Install & Basic Usage"},{"location":"get_started/#installation","text":"Firstly, install python >= 3.10.0, we have two ways to install Install from pip: pip install fedimpute Install from package repo: git clone https://github.com/idsla/FedImpute cd FedImpute # create virtual env python -m venv ./venv # window gitbash source ./venv/Scripts/activate # linux/unix source ./venv/bin/activate # Install the required packages pip install -r requirements.txt","title":"Installation"},{"location":"get_started/#basic-usage","text":"","title":"Basic Usage"},{"location":"get_started/#step-1-prepare-data","text":"import numpy as np data = np . random . rand ( 10000 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 10 , }","title":"Step 1. Prepare Data"},{"location":"get_started/#step-2-simulate-federated-missing-data-scenario","text":"from fedimpute.simulator import Simulator simulator = Simulator () # classifical simulation function simulation_results = simulator . simulate_scenario ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_mech_type = 'mcar' , verbose = 1 ) # lite simulation function simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'niid-dir@0.1' , ms_scenario = 'mar-heter' , verbose = 1 )","title":"Step 2. Simulate Federated Missing Data Scenario"},{"location":"get_started/#step-3-execute-federated-imputation-algorithms","text":"Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv () env . reset_env () env . configuration ( imputer = 'gain' , fed_strategy = 'fedavg' , fit_mode = 'fed' ) env . setup_from_simulator ( simulator = simulator , verbose = 1 ) env . run_fed_imputation ( run_type = 'sequential' )","title":"Step 3. Execute Federated Imputation Algorithms"},{"location":"get_started/#step-4-evaluate-imputation-outcomes","text":"from fedimpute.evaluation import Evaluator evaluator = Evaluator () evaluator . evaluate ( env , [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ]) evaluator . show_results ()","title":"Step 4. Evaluate imputation outcomes"},{"location":"overview/","text":"Overview of FedImpute FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment. Understanding Missing Data in Horizontal Federated Learning Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions. Types of Missing Data Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction . Missing Data Under Federated Learning Scenario Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees. Missing Data Imputation of Federated Datasets A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need. Architecture and Components FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms. License FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"Overview of FedImpute"},{"location":"overview/#overview-of-fedimpute","text":"FedImpute is an open-source Python package designed to facilitate the simulation, imputation and evaluation of missing data in federated learning environments. By leveraging advanced statistical and machine learning imputation techniques, FedImpute enables collaborative data imputation under federated data scenario. This package is ideal for researchers and practitioners working with decentralized datasets with missing data and care about how to design and evaluate the state-of-art missing data imputation method under federated learning evironment.","title":"Overview of FedImpute"},{"location":"overview/#understanding-missing-data-in-horizontal-federated-learning","text":"Missing data occurs when <NA> data value is stored for a variable in an observation within a dataset. In data analysis, missing data can significantly impact the quality of insights and the performance of predictive models. The causes of missing data can vary widely, including data entry errors, non-response in surveys, or interruptions in data collection. It's crucial to address these gaps effectively, as improper handling can lead to biased estimates and incorrect conclusions.","title":"Understanding Missing Data in Horizontal Federated Learning"},{"location":"overview/#types-of-missing-data","text":"Missing data can be categorized into three types, each with its own implications for data analysis: Missing Completely at Random (MCAR) : The probability of a data point being missing is the arbitary random. Missing at Random (MAR) : The probability of a data point being missing is related to other observed variables, but not to the missing data itself. Missing Not at Random (MNAR) : The probability of a data point being missing is related to the reason it is missing. For more information on missing data, refer to the Missing Data Introduction .","title":"Types of Missing Data"},{"location":"overview/#missing-data-under-federated-learning-scenario","text":"Horizontal federated learning (HFL) involves a scenario where multiple parties, each with their own datasets. These datasets often have the same feature space but different observations. Usually, a common case of HFL is when the data is distributed across multiple silos, for example, hospitals, banks, or research institutions. In these scenarios, missing data can be a significant challenge, as for these common application domains such as healthcare, the missing data is very prevalent. The missing data scenario under these scenarios can be further complex, because of the following dimensions: Missing Mechanism Diversity . Missing values in the data can occur due to different reasons and are modeled through missing mechanisms, which are categorized into three distinct types depending upon how the missing values are related to the data. Missing Mechanism Heterogeneity . Missing values in the same feature can be due to different missing mechanisms across silos. Data Heterogeneity . The data across silos can be non-iid to varying degrees.","title":"Missing Data Under Federated Learning Scenario"},{"location":"overview/#missing-data-imputation-of-federated-datasets","text":"A common approach to handling missing data is imputation, where missing values are estimated based on the observed data. More details of imputation methods and their performance can be found in the How to deal with missing values? , Imputation Book , Hyperimpute . Under HFL, effective strategies for handling missing data must consider how to unify the decentralized or local imputation techniques, where each party performed independently. By using common federated learning solutions to allow for collaborative imputation without exposing individual data points, we call it Federated Imputation . Due to the reason of the complexity of the problem, it is important to have a benchmarking tool to rapidly evaluate the performance of federated imputation algorithms under different missing data scenarios. FedImpute is designed to address this need.","title":"Missing Data Imputation of Federated Datasets"},{"location":"overview/#architecture-and-components","text":"FedImpute is built on top of the following components: Federated Missing Data Scenario Simulation , Federated Imputation Execution Environment , and Evaluation . Federated Missing Data Scenario Simulation : This component simulates missing data scenarios under various missing data distribution and data partition strategies. Federated Imputation Execution Environment : This component executes federated imputation algorithms on the simulated missing data scenarios. Evaluation : This component evaluates the imputation outcomes and provides insights into the performance of the imputation algorithms.","title":"Architecture and Components"},{"location":"overview/#license","text":"FedImpute is released under the GPL v3.0 License, allowing free use, modification, and distribution of the software. This license encourages open collaboration by permitting the incorporation of this package into both open and closed-source projects.","title":"License"},{"location":"api/fed_impute/","text":"Federated Imputation Execution Environment Client fedimpute . execution_environment . client . client .Client class Client ( client_id : int , train_data : np .ndarray , test_data : np .ndarray , X_train_ms : np .ndarray , data_config : dict , imp_model_name , imp_model_params , fed_strategy : str , fed_strategy_params : dict , client_config : dict , seed = 0 ) Client class presenting a client in the federated imputation execution environment, it contains the training and testing data, missing data, imputed data, imputation model class, and federated strategy class. Attributes client_id : int \u2014 client id X_train : np .ndarray \u2014 training data y_train : np .ndarray \u2014 training labels X_test : np .ndarray \u2014 testing data y_test : np .ndarray \u2014 testing labels X_train_ms : np .ndarray \u2014 missing data X_train_mask : np .ndarray \u2014 missing data mask X_train_imp : np .ndarray \u2014 imputed data data_utils : dict \u2014 data statistics imputer : BaseImputer \u2014 imputation model fed_strategy : BaseFedStrategy \u2014 federated strategy seed : int \u2014 seed client_config : dict \u2014 client configuration Methods initial_impute \u2014 Initial imputation fit_local_imp_model \u2014 Fit a local imputation model update_local_imp_model \u2014 Fit a local imputation model local_imputation \u2014 Perform local imputation save_imp_model \u2014 Save imputation model load_imp_model \u2014 Save imputation model calculate_data_utils \u2014 Calculate data statistic profile fedimpute . execution_environment . client . client . Client .initial_impute method Client . initial_impute ( imp_values : np .ndarray , col_type : str = 'num' ) \u2192 None Initial imputation Parameters imp_values : np .ndarray \u2014 imputation values col_type : str \u2014 column type, 'num' or 'cat' fedimpute . execution_environment . client . client . Client .fit_local_imp_model method Client . fit_local_imp_model ( params : dict ) \u2192 Tuple [ Union [dict, torch . nn . Module ], dict] Fit a local imputation model Parameters params : dict \u2014 instructions for fitting the imputation model Returns Tuple [ Union [dict, torch . nn . Module ], dict] \u2014 model parameters and fitting results dictionary fedimpute . execution_environment . client . client . Client .local_imputation method Client . local_imputation ( params : dict ) \u2192 Union [None, np .ndarray] Perform local imputation Parameters params : dict \u2014 instructions for imputation - e.g temp_imp for temporary imputation Returns Union [None, np .ndarray] \u2014 imputed data or None Server fedimpute . execution_environment . server . server .Server class Server ( fed_strategy_name : str , fed_strategy_params : dict , imputer_name : str , imputer_params : dict , global_test : np .ndarray , data_config : dict , server_config : Dict [str, Union [str, int, float]] , seed : int = 21 ) Server class to be used in the federated imputation environment Attributes fed_strategy \u2014 str - name of the federated strategy fed_strategy_params \u2014 dict - parameters of the federated strategy server_config \u2014 dict - configuration of the server X_test_global \u2014 np.ndarray - global test data Methods global_evaluation calculate_data_utils Workflow fedimpute . execution_environment . workflows . workflow .BaseWorkflow class BaseWorkflow ( ) Bases : ABC Abstract class for the workflow to be used in the federated imputation environment Methods fed_imp_sequential \u2014 Sequential federated imputation workflow fed_imp_parallel \u2014 Parallel federated imputation workflow run_fed_imp \u2014 Run the federated imputation workflow based on the eval_and_track eval_and_track_parallel fedimpute . execution_environment . workflows . workflow . BaseWorkflow .run_fed_imp method BaseWorkflow . run_fed_imp ( clients : List [ Client ] , server : Server , evaluator , tracker : Tracker , run_type : str ) \u2192 Tracker Run the federated imputation workflow based on the Parameters clients : List [ Client ] \u2014 List Client - list of clients server : Server \u2014 Server - server evaluator \u2014 Evaluator - evaluator tracker : Tracker \u2014 Tracker - tracker to tracking results run_type : str \u2014 str - type of the workflow run (sequential or parallel) Returns Tracker \u2014 Tracker - tracker with tracked results Raises ValueError","title":"FedImpute Execution Environment"},{"location":"api/fed_impute/#federated-imputation-execution-environment","text":"","title":"Federated Imputation Execution Environment"},{"location":"api/fed_impute/#client","text":"fedimpute . execution_environment . client . client .Client class Client ( client_id : int , train_data : np .ndarray , test_data : np .ndarray , X_train_ms : np .ndarray , data_config : dict , imp_model_name , imp_model_params , fed_strategy : str , fed_strategy_params : dict , client_config : dict , seed = 0 ) Client class presenting a client in the federated imputation execution environment, it contains the training and testing data, missing data, imputed data, imputation model class, and federated strategy class. Attributes client_id : int \u2014 client id X_train : np .ndarray \u2014 training data y_train : np .ndarray \u2014 training labels X_test : np .ndarray \u2014 testing data y_test : np .ndarray \u2014 testing labels X_train_ms : np .ndarray \u2014 missing data X_train_mask : np .ndarray \u2014 missing data mask X_train_imp : np .ndarray \u2014 imputed data data_utils : dict \u2014 data statistics imputer : BaseImputer \u2014 imputation model fed_strategy : BaseFedStrategy \u2014 federated strategy seed : int \u2014 seed client_config : dict \u2014 client configuration Methods initial_impute \u2014 Initial imputation fit_local_imp_model \u2014 Fit a local imputation model update_local_imp_model \u2014 Fit a local imputation model local_imputation \u2014 Perform local imputation save_imp_model \u2014 Save imputation model load_imp_model \u2014 Save imputation model calculate_data_utils \u2014 Calculate data statistic profile fedimpute . execution_environment . client . client . Client .initial_impute method Client . initial_impute ( imp_values : np .ndarray , col_type : str = 'num' ) \u2192 None Initial imputation Parameters imp_values : np .ndarray \u2014 imputation values col_type : str \u2014 column type, 'num' or 'cat' fedimpute . execution_environment . client . client . Client .fit_local_imp_model method Client . fit_local_imp_model ( params : dict ) \u2192 Tuple [ Union [dict, torch . nn . Module ], dict] Fit a local imputation model Parameters params : dict \u2014 instructions for fitting the imputation model Returns Tuple [ Union [dict, torch . nn . Module ], dict] \u2014 model parameters and fitting results dictionary fedimpute . execution_environment . client . client . Client .local_imputation method Client . local_imputation ( params : dict ) \u2192 Union [None, np .ndarray] Perform local imputation Parameters params : dict \u2014 instructions for imputation - e.g temp_imp for temporary imputation Returns Union [None, np .ndarray] \u2014 imputed data or None","title":"Client"},{"location":"api/fed_impute/#server","text":"fedimpute . execution_environment . server . server .Server class Server ( fed_strategy_name : str , fed_strategy_params : dict , imputer_name : str , imputer_params : dict , global_test : np .ndarray , data_config : dict , server_config : Dict [str, Union [str, int, float]] , seed : int = 21 ) Server class to be used in the federated imputation environment Attributes fed_strategy \u2014 str - name of the federated strategy fed_strategy_params \u2014 dict - parameters of the federated strategy server_config \u2014 dict - configuration of the server X_test_global \u2014 np.ndarray - global test data Methods global_evaluation calculate_data_utils","title":"Server"},{"location":"api/fed_impute/#workflow","text":"fedimpute . execution_environment . workflows . workflow .BaseWorkflow class BaseWorkflow ( ) Bases : ABC Abstract class for the workflow to be used in the federated imputation environment Methods fed_imp_sequential \u2014 Sequential federated imputation workflow fed_imp_parallel \u2014 Parallel federated imputation workflow run_fed_imp \u2014 Run the federated imputation workflow based on the eval_and_track eval_and_track_parallel fedimpute . execution_environment . workflows . workflow . BaseWorkflow .run_fed_imp method BaseWorkflow . run_fed_imp ( clients : List [ Client ] , server : Server , evaluator , tracker : Tracker , run_type : str ) \u2192 Tracker Run the federated imputation workflow based on the Parameters clients : List [ Client ] \u2014 List Client - list of clients server : Server \u2014 Server - server evaluator \u2014 Evaluator - evaluator tracker : Tracker \u2014 Tracker - tracker to tracking results run_type : str \u2014 str - type of the workflow run (sequential or parallel) Returns Tracker \u2014 Tracker - tracker with tracked results Raises ValueError","title":"Workflow"},{"location":"api/imputation_models/","text":"Imputation Models Non-NN Based Imputer fedimpute . execution_environment . imputation . base . base_imputer .BaseMLImputer class BaseMLImputer ( ) Abstract class for the non-NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .initialize method BaseMLImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 int - seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .fit method BaseMLImputer . fit ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 dict Fit imputer to train local imputation models Parameters X : np .array \u2014 np.array - float numpy array features y : np .array \u2014 np.array - target missing_mask : np .array \u2014 np.array - missing mask params : dict \u2014 parameters for local training fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .impute method BaseMLImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X Mean fedimpute . execution_environment . imputation . imputers . simple_imputer .SimpleImputer class SimpleImputer ( strategy : str = 'mean' ) Bases : BaseMLImputer Simple imputer class for imputing missing values in data using simple strategies like mean, median etc. Attributes strategy : str \u2014 strategy for imputation - mean, median etc. mean_params : np .array \u2014 mean parameters for imputation model_type : str \u2014 type of the model - numpy or sklearn model_persistable : bool \u2014 whether model is persistable or not name : str \u2014 name of the imputer Raises ValueError Methods get_imp_model_params set_imp_model_params initialize fit impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model EM fedimpute . execution_environment . imputation . imputers . em_imputer .EMImputer class EMImputer ( clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin EM imputer class for imputing missing values in data using Expectation Maximization algorithm. Attributes clip \u2014 bool - whether to clip the imputed values use_y \u2014 bool - whether to use target variable in imputation min_values \u2014 np.array - minimum values for clipping max_values \u2014 np.array - maximum values for clipping data_utils_info \u2014 dict - information about data seed \u2014 int - seed for randomization name \u2014 str = 'em' - name of the imputer model_type \u2014 str = 'simple' - type of the imputer - simple or nn - neural network based or not mu \u2014 np.array - mean of the data sigma \u2014 np.array - covariance matrix of the data miss \u2014 np.array - missing values indices obs \u2014 np.array - observed values indices model_persistable \u2014 bool - whether model is persistable or not Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit the imputer on the data. impute \u2014 Impute the missing values in the data. save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds get_visit_indices _em \u2014 Perform the EM step for imputing missing values. _converged \u2014 Checks if the EM loop has converged. ICE fedimpute . execution_environment . imputation . imputers . linear_ice_imputer .LinearICEImputer class LinearICEImputer ( estimator_num : str = 'ridge_cv' , estimator_cat : str = 'logistic' , mm_model : str = 'logistic' , mm_model_params = None , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin Linear ICE imputer class for imputing missing values in data using linear models. Attributes estimator_num : str \u2014 estimator for numerical columns estimator_cat : str \u2014 estimator for categorical columns mm_model \u2014 missing mechanism model mm_model_params : dict \u2014 missing mechanism model parameters clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target variable in imputation imp_models : list \u2014 list of imputation models data_utils_info : dict \u2014 information about data seed : int \u2014 seed for randomization model_type : str \u2014 type of the imputer - simple or nn - neural network based or not, defaults to 'sklearn' model_persistable : bool \u2014 whether model is persistable or not, defaults to False name : str \u2014 name of the imputer, defaults to 'linear_ice' Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices MissForest fedimpute . execution_environment . imputation . imputers . missforest_imputer .MissForestImputer class MissForestImputer ( n_estimators : int = 200 , bootstrap : bool = True , n_jobs : int = 2 , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin MissForest imputer class for the federated imputation environment Attributes n_estimators : int \u2014 number of trees in the forest bootstrap : bool \u2014 whether bootstrap samples are used when building trees n_jobs : int \u2014 number of jobs to run in parallel clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target values for imputation imp_models : list \u2014 list of imputation models mm_model : object \u2014 model for missing mask imputation data_utils_info : dict \u2014 data utils information seed : int \u2014 seed for randomization model_type : str \u2014 type of the model, defaults to 'sklearn' model_persistable : bool \u2014 whether the model is persistable, defaults to False name : str \u2014 name of the imputer, defaults to 'missforest' Methods get_imp_model_params set_imp_model_params initialize fit impute save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices NN Based Imputer fedimpute . execution_environment . imputation . base . base_imputer .BaseNNImputer class BaseNNImputer ( ) Abstract class for the NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. configure_model \u2014 Fetch model for training configure_optimizer \u2014 Configure optimizer for training impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .initialize method BaseNNImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .configure_model method BaseNNImputer . configure_model ( params : dict , X : np .ndarray , y : np .ndarray , missing_mask : np .ndarray ) \u2192 Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] Fetch model for training Parameters params : dict \u2014 parameters for training X : np .ndarray \u2014 imputed data y : np .ndarray \u2014 target missing_mask : np .ndarray \u2014 missing mask Returns Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] \u2014 model, train_dataloader fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .impute method BaseNNImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X GAIN fedimpute . execution_environment . imputation . imputers . gain_imputer .GAINImputer class GAINImputer ( h_dim : int = 20 , n_layers : int = 2 , activation : str = 'relu' , initializer : str = 'kaiming' , loss_alpha : float = 10 , hint_rate : float = 0.9 , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin GAIN imputer class for imputing missing values in data using Generative Adversarial Imputation Networks. Attributes h_dim : int \u2014 dimension of hidden layers n_layers : int \u2014 number of layers activation : str \u2014 activation function initializer : str \u2014 initializer for weights loss_alpha : float \u2014 alpha parameter for loss hint_rate : float \u2014 hint rate for loss clip : bool \u2014 whether to clip the imputed values batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training scheduler_params : dict \u2014 scheduler parameters Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds MIWAE fedimpute . execution_environment . imputation . imputers . miwae_imputer .MIWAEImputer class MIWAEImputer ( name : str = 'miwae' , latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"Imputation Models"},{"location":"api/imputation_models/#imputation-models","text":"","title":"Imputation Models"},{"location":"api/imputation_models/#non-nn-based-imputer","text":"fedimpute . execution_environment . imputation . base . base_imputer .BaseMLImputer class BaseMLImputer ( ) Abstract class for the non-NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .initialize method BaseMLImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 int - seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .fit method BaseMLImputer . fit ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 dict Fit imputer to train local imputation models Parameters X : np .array \u2014 np.array - float numpy array features y : np .array \u2014 np.array - target missing_mask : np .array \u2014 np.array - missing mask params : dict \u2014 parameters for local training fedimpute . execution_environment . imputation . base . base_imputer . BaseMLImputer .impute method BaseMLImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X","title":"Non-NN Based Imputer"},{"location":"api/imputation_models/#mean","text":"fedimpute . execution_environment . imputation . imputers . simple_imputer .SimpleImputer class SimpleImputer ( strategy : str = 'mean' ) Bases : BaseMLImputer Simple imputer class for imputing missing values in data using simple strategies like mean, median etc. Attributes strategy : str \u2014 strategy for imputation - mean, median etc. mean_params : np .array \u2014 mean parameters for imputation model_type : str \u2014 type of the model - numpy or sklearn model_persistable : bool \u2014 whether model is persistable or not name : str \u2014 name of the imputer Raises ValueError Methods get_imp_model_params set_imp_model_params initialize fit impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model","title":"Mean"},{"location":"api/imputation_models/#em","text":"fedimpute . execution_environment . imputation . imputers . em_imputer .EMImputer class EMImputer ( clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin EM imputer class for imputing missing values in data using Expectation Maximization algorithm. Attributes clip \u2014 bool - whether to clip the imputed values use_y \u2014 bool - whether to use target variable in imputation min_values \u2014 np.array - minimum values for clipping max_values \u2014 np.array - maximum values for clipping data_utils_info \u2014 dict - information about data seed \u2014 int - seed for randomization name \u2014 str = 'em' - name of the imputer model_type \u2014 str = 'simple' - type of the imputer - simple or nn - neural network based or not mu \u2014 np.array - mean of the data sigma \u2014 np.array - covariance matrix of the data miss \u2014 np.array - missing values indices obs \u2014 np.array - observed values indices model_persistable \u2014 bool - whether model is persistable or not Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit the imputer on the data. impute \u2014 Impute the missing values in the data. save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds get_visit_indices _em \u2014 Perform the EM step for imputing missing values. _converged \u2014 Checks if the EM loop has converged.","title":"EM"},{"location":"api/imputation_models/#ice","text":"fedimpute . execution_environment . imputation . imputers . linear_ice_imputer .LinearICEImputer class LinearICEImputer ( estimator_num : str = 'ridge_cv' , estimator_cat : str = 'logistic' , mm_model : str = 'logistic' , mm_model_params = None , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin Linear ICE imputer class for imputing missing values in data using linear models. Attributes estimator_num : str \u2014 estimator for numerical columns estimator_cat : str \u2014 estimator for categorical columns mm_model \u2014 missing mechanism model mm_model_params : dict \u2014 missing mechanism model parameters clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target variable in imputation imp_models : list \u2014 list of imputation models data_utils_info : dict \u2014 information about data seed : int \u2014 seed for randomization model_type : str \u2014 type of the imputer - simple or nn - neural network based or not, defaults to 'sklearn' model_persistable : bool \u2014 whether model is persistable or not, defaults to False name : str \u2014 name of the imputer, defaults to 'linear_ice' Methods get_imp_model_params set_imp_model_params initialize \u2014 Initialize imputer - statistics imputation models etc. fit \u2014 Fit imputer to train local imputation models impute \u2014 Impute missing values using an imputation model save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices","title":"ICE"},{"location":"api/imputation_models/#missforest","text":"fedimpute . execution_environment . imputation . imputers . missforest_imputer .MissForestImputer class MissForestImputer ( n_estimators : int = 200 , bootstrap : bool = True , n_jobs : int = 2 , clip : bool = True , use_y : bool = False ) Bases : BaseMLImputer , ICEImputerMixin MissForest imputer class for the federated imputation environment Attributes n_estimators : int \u2014 number of trees in the forest bootstrap : bool \u2014 whether bootstrap samples are used when building trees n_jobs : int \u2014 number of jobs to run in parallel clip : bool \u2014 whether to clip the imputed values use_y : bool \u2014 whether to use target values for imputation imp_models : list \u2014 list of imputation models mm_model : object \u2014 model for missing mask imputation data_utils_info : dict \u2014 data utils information seed : int \u2014 seed for randomization model_type : str \u2014 type of the model, defaults to 'sklearn' model_persistable : bool \u2014 whether the model is persistable, defaults to False name : str \u2014 name of the imputer, defaults to 'missforest' Methods get_imp_model_params set_imp_model_params initialize fit impute save_model load_model get_clip_thresholds set_clip_thresholds get_visit_indices","title":"MissForest"},{"location":"api/imputation_models/#nn-based-imputer","text":"fedimpute . execution_environment . imputation . base . base_imputer .BaseNNImputer class BaseNNImputer ( ) Abstract class for the NN based imputer to be used in the federated imputation environment Methods get_imp_model_params \u2014 Return model parameters set_imp_model_params \u2014 Set model parameters initialize \u2014 Initialize imputer - statistics imputation models etc. configure_model \u2014 Fetch model for training configure_optimizer \u2014 Configure optimizer for training impute \u2014 Impute missing values using an imputation model save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .initialize method BaseNNImputer . initialize ( X : np .array , missing_mask : np .array , data_utils : dict , params : dict , seed : int ) \u2192 None Initialize imputer - statistics imputation models etc. Parameters X : np .array \u2014 data with intial imputed values missing_mask : np .array \u2014 missing mask of data data_utils : dict \u2014 data utils dictionary - contains information about data params : dict \u2014 params for initialization seed : int \u2014 seed for randomization fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .configure_model method BaseNNImputer . configure_model ( params : dict , X : np .ndarray , y : np .ndarray , missing_mask : np .ndarray ) \u2192 Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] Fetch model for training Parameters params : dict \u2014 parameters for training X : np .ndarray \u2014 imputed data y : np .ndarray \u2014 target missing_mask : np .ndarray \u2014 missing mask Returns Tuple [ torch . nn . Module , torch . utils . data . DataLoader ] \u2014 model, train_dataloader fedimpute . execution_environment . imputation . base . base_imputer . BaseNNImputer .impute method BaseNNImputer . impute ( X : np .array , y : np .array , missing_mask : np .array , params : dict ) \u2192 np .ndarray Impute missing values using an imputation model Parameters X : np .array \u2014 numpy array of features y : np .array \u2014 numpy array of target missing_mask : np .array \u2014 missing mask params : dict \u2014 parameters for imputation Returns np .ndarray \u2014 imputed data - numpy array - same dimension as X","title":"NN Based Imputer"},{"location":"api/imputation_models/#gain","text":"fedimpute . execution_environment . imputation . imputers . gain_imputer .GAINImputer class GAINImputer ( h_dim : int = 20 , n_layers : int = 2 , activation : str = 'relu' , initializer : str = 'kaiming' , loss_alpha : float = 10 , hint_rate : float = 0.9 , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin GAIN imputer class for imputing missing values in data using Generative Adversarial Imputation Networks. Attributes h_dim : int \u2014 dimension of hidden layers n_layers : int \u2014 number of layers activation : str \u2014 activation function initializer : str \u2014 initializer for weights loss_alpha : float \u2014 alpha parameter for loss hint_rate : float \u2014 hint rate for loss clip : bool \u2014 whether to clip the imputed values batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training scheduler_params : dict \u2014 scheduler parameters Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds","title":"GAIN"},{"location":"api/imputation_models/#miwae","text":"fedimpute . execution_environment . imputation . imputers . miwae_imputer .MIWAEImputer class MIWAEImputer ( name : str = 'miwae' , latent_size : int = 5 , n_hidden : int = 16 , n_hidden_layers : int = 2 , out_dist = 'studentt' , K : int = 20 , L : int = 100 , activation = 'tanh' , initializer = 'xavier' , clip : bool = True , batch_size : int = 256 , learning_rate : int = 0.001 , weight_decay : int = 0.0001 , scheduler : str = 'step' , optimizer : str = 'sgd' ) Bases : BaseNNImputer , JMImputerMixin MiWAE imputer class for imputing missing values in data using Multiple Imputation with Auxiliary Deep Generative Models. Attributes name : str \u2014 name of the imputer clip : bool \u2014 whether to clip the imputed values latent_size : int \u2014 size of the latent space n_hidden : int \u2014 number of hidden units n_hidden_layers : int \u2014 number of hidden layers out_dist : str \u2014 output distribution K : int \u2014 number of samples L : int \u2014 number of MCMC samples activation : str \u2014 activation function initializer : str \u2014 initializer for weights batch_size : int \u2014 batch size for training learning_rate : int \u2014 learning rate for optimizer weight_decay : int \u2014 weight decay for optimizer scheduler : str \u2014 scheduler for optimizer optimizer : str \u2014 optimizer for training Methods get_imp_model_params set_imp_model_params initialize configure_model configure_optimizer impute save_model \u2014 Save the imputer model load_model \u2014 Load the imputer model get_clip_thresholds set_clip_thresholds fit","title":"MIWAE"},{"location":"api/simulation/","text":"Simulator fedimpute . simulator . simulator .Simulator class Simulator ( debug_mode : bool = False ) Simulator class for simulating missing data scenarios in federated learning environment Attributes data : np .ndarray \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary clients_train_data : List [ np .ndarray] \u2014 list of clients training data clients_test_data : List [ np .ndarray] \u2014 list of clients test data clients_train_data_ms : List [ np .ndarray] \u2014 list of clients training data with missing values global_test : np .ndarray \u2014 global test data client_seeds : List [int] \u2014 list of seeds for clients stats : dict \u2014 simulation statistics debug_mode : bool \u2014 whether to enable debug mode Methods simulate_scenario \u2014 Simulate missing data scenario simulate_scenario_lite \u2014 Simulate missing data scenario save load export_data summarize visualization Core Simulation Function fedimpute . simulator . simulator . Simulator .simulate_scenario method Simulator . simulate_scenario ( data : np .array , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , dp_split_cols : Union [str, int, List [int]] = 'target' , dp_niid_alpha : float = 0.1 , dp_size_niid_alpha : float = 0.1 , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 2000 , dp_even_sample_size : int = 1000 , dp_sample_iid_direct : bool = False , dp_local_test_size : float = 0.1 , dp_global_test_size : float = 0.1 , dp_local_backup_size : float = 0.05 , dp_reg_bins : int = 50 , ms_mech_type : str = 'mcar' , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , ms_global_mechanism : bool = False , ms_mr_dist_clients : str = 'randu-int' , ms_mf_dist_clients : str = 'identity' , ms_mm_dist_clients : str = 'random' , ms_missing_features : str = 'all' , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , ms_mm_funcs_bank : str = 'lr' , ms_mm_strictness : bool = True , ms_mm_obs : bool = False , ms_mm_feature_option : str = 'allk=0.2' , ms_mm_beta_option : str = None , seed : int = 100330201 , verbose : int = 0 ) \u2192 Dict [str, List [ np .ndarray]] Simulate missing data scenario Parameters data : np .array \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path dp_split_cols : Union [str, int, List [int]] \u2014 split columns option - target , first , default: target dp_niid_alpha : float \u2014 non-iid alpha for data partition, default: 0.1 dp_size_niid_alpha : float \u2014 size niid alpha for data partition, default: 0.1 dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 2000 dp_even_sample_size : int \u2014 even sample size for data partition, default: 1000 dp_sample_iid_direct : bool \u2014 sample iid data directly, default: False dp_local_test_size : float \u2014 local test size ratio, default: 0.1 dp_global_test_size : float \u2014 global test size ratio, default: 0.1 dp_local_backup_size : float \u2014 local backup size ratio, default: 0.05 dp_reg_bins : int \u2014 regression bins, default: 50 ms_mech_type : str \u2014 missing mechanism type, default: 'mcar' - mcar , mar_sigmoid , mnar_sigmoid , mar_quantile , mnar_quantile ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - all , all-num , random obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - random , rest ms_global_mechanism : bool \u2014 global missing mechanism, default: False ms_mr_dist_clients : str \u2014 missing ratio distribution, default: 'randu-int' - 'fixed', 'uniform', 'uniform_int', 'gaussian', 'gaussian_int' ms_mf_dist_clients : str \u2014 missing features distribution, default: 'identity' - 'identity', 'random', 'random2' ms_mm_dist_clients : str \u2014 missing mechanism functions distribution, default: 'random' - 'identity', 'random', 'random2' ms_missing_features : str \u2014 missing features strategy, default: 'all' - 'all', 'all-num' ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 ms_mm_funcs_bank : str \u2014 missing mechanism functions banks, default: 'lr' - None, 'lr', 'mt', 'all' ms_mm_strictness : bool \u2014 missing adding probabilistic or deterministic, default: True ms_mm_obs : bool \u2014 missing adding based on observed data, default: False ms_mm_feature_option : str \u2014 missing mechanism associated with which features, default: 'allk=0.2' - 'self', 'all', 'allk=0.1' ms_mm_beta_option : str \u2014 mechanism beta coefficient option, default: None - (mnar) self, sphere, randu, (mar) fixed, randu, randn seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns Dict [str, List [ np .ndarray]] \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError Lite Simulation Function fedimpute . simulator . simulator . Simulator .simulate_scenario_lite method Simulator . simulate_scenario_lite ( data : np .array , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , ms_scenario : str = 'mcar' , dp_split_col_option : str = 'target' , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 8000 , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , seed : int = 100330201 , verbose : int = 0 ) Simulate missing data scenario Parameters data : np .array \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path ms_scenario : str \u2014 predefined missing data scenario, default: 'mcar' - mcar , mar-heter \uff0c mar-homo , mnar-heter , mnar-homo dp_split_col_option : str \u2014 iid/niid column strategy partition base on - 'target', 'feature', default: 'target' ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - 'all', 'all-num', 'random' obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - 'random', 'rest' dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 8000 ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns dict \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError NotImplementedError","title":"Scenario simulation"},{"location":"api/simulation/#simulator","text":"fedimpute . simulator . simulator .Simulator class Simulator ( debug_mode : bool = False ) Simulator class for simulating missing data scenarios in federated learning environment Attributes data : np .ndarray \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary clients_train_data : List [ np .ndarray] \u2014 list of clients training data clients_test_data : List [ np .ndarray] \u2014 list of clients test data clients_train_data_ms : List [ np .ndarray] \u2014 list of clients training data with missing values global_test : np .ndarray \u2014 global test data client_seeds : List [int] \u2014 list of seeds for clients stats : dict \u2014 simulation statistics debug_mode : bool \u2014 whether to enable debug mode Methods simulate_scenario \u2014 Simulate missing data scenario simulate_scenario_lite \u2014 Simulate missing data scenario save load export_data summarize visualization","title":"Simulator"},{"location":"api/simulation/#core-simulation-function","text":"fedimpute . simulator . simulator . Simulator .simulate_scenario method Simulator . simulate_scenario ( data : np .array , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , dp_split_cols : Union [str, int, List [int]] = 'target' , dp_niid_alpha : float = 0.1 , dp_size_niid_alpha : float = 0.1 , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 2000 , dp_even_sample_size : int = 1000 , dp_sample_iid_direct : bool = False , dp_local_test_size : float = 0.1 , dp_global_test_size : float = 0.1 , dp_local_backup_size : float = 0.05 , dp_reg_bins : int = 50 , ms_mech_type : str = 'mcar' , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , ms_global_mechanism : bool = False , ms_mr_dist_clients : str = 'randu-int' , ms_mf_dist_clients : str = 'identity' , ms_mm_dist_clients : str = 'random' , ms_missing_features : str = 'all' , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , ms_mm_funcs_bank : str = 'lr' , ms_mm_strictness : bool = True , ms_mm_obs : bool = False , ms_mm_feature_option : str = 'allk=0.2' , ms_mm_beta_option : str = None , seed : int = 100330201 , verbose : int = 0 ) \u2192 Dict [str, List [ np .ndarray]] Simulate missing data scenario Parameters data : np .array \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path dp_split_cols : Union [str, int, List [int]] \u2014 split columns option - target , first , default: target dp_niid_alpha : float \u2014 non-iid alpha for data partition, default: 0.1 dp_size_niid_alpha : float \u2014 size niid alpha for data partition, default: 0.1 dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 2000 dp_even_sample_size : int \u2014 even sample size for data partition, default: 1000 dp_sample_iid_direct : bool \u2014 sample iid data directly, default: False dp_local_test_size : float \u2014 local test size ratio, default: 0.1 dp_global_test_size : float \u2014 global test size ratio, default: 0.1 dp_local_backup_size : float \u2014 local backup size ratio, default: 0.05 dp_reg_bins : int \u2014 regression bins, default: 50 ms_mech_type : str \u2014 missing mechanism type, default: 'mcar' - mcar , mar_sigmoid , mnar_sigmoid , mar_quantile , mnar_quantile ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - all , all-num , random obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - random , rest ms_global_mechanism : bool \u2014 global missing mechanism, default: False ms_mr_dist_clients : str \u2014 missing ratio distribution, default: 'randu-int' - 'fixed', 'uniform', 'uniform_int', 'gaussian', 'gaussian_int' ms_mf_dist_clients : str \u2014 missing features distribution, default: 'identity' - 'identity', 'random', 'random2' ms_mm_dist_clients : str \u2014 missing mechanism functions distribution, default: 'random' - 'identity', 'random', 'random2' ms_missing_features : str \u2014 missing features strategy, default: 'all' - 'all', 'all-num' ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 ms_mm_funcs_bank : str \u2014 missing mechanism functions banks, default: 'lr' - None, 'lr', 'mt', 'all' ms_mm_strictness : bool \u2014 missing adding probabilistic or deterministic, default: True ms_mm_obs : bool \u2014 missing adding based on observed data, default: False ms_mm_feature_option : str \u2014 missing mechanism associated with which features, default: 'allk=0.2' - 'self', 'all', 'allk=0.1' ms_mm_beta_option : str \u2014 mechanism beta coefficient option, default: None - (mnar) self, sphere, randu, (mar) fixed, randu, randn seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns Dict [str, List [ np .ndarray]] \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError","title":"Core Simulation Function"},{"location":"api/simulation/#lite-simulation-function","text":"fedimpute . simulator . simulator . Simulator .simulate_scenario_lite method Simulator . simulate_scenario_lite ( data : np .array , data_config : dict , num_clients : int , dp_strategy : str = 'iid-even' , ms_scenario : str = 'mcar' , dp_split_col_option : str = 'target' , ms_cols : Union [str, List [int]] = 'all' , obs_cols : Union [str, List [int]] = 'random' , dp_min_samples : Union [float, int] = 50 , dp_max_samples : Union [float, int] = 8000 , ms_mr_lower : float = 0.3 , ms_mr_upper : float = 0.7 , seed : int = 100330201 , verbose : int = 0 ) Simulate missing data scenario Parameters data : np .array \u2014 data to be used for simulation data_config : dict \u2014 data configuration dictionary num_clients : int \u2014 number of clients dp_strategy : str \u2014 data partition strategy, default: 'iid-even' - iid-even , iid-dir , niid-dir , niid-path ms_scenario : str \u2014 predefined missing data scenario, default: 'mcar' - mcar , mar-heter \uff0c mar-homo , mnar-heter , mnar-homo dp_split_col_option : str \u2014 iid/niid column strategy partition base on - 'target', 'feature', default: 'target' ms_cols : Union [str, List [int]] \u2014 missing columns, default: 'all' - 'all', 'all-num', 'random' obs_cols : Union [str, List [int]] \u2014 fully observed columns for MAR, default: 'random' - 'random', 'rest' dp_min_samples : Union [float, int] \u2014 minimum samples for clients, default: 50 dp_max_samples : Union [float, int] \u2014 maximum samples for clients, default: 8000 ms_mr_lower : float \u2014 minimum missing ratio for each feature, default: 0.3 ms_mr_upper : float \u2014 maximum missing ratio for each feature, default: 0.7 seed : int \u2014 random seed, default: 100330201 verbose : int \u2014 whether verbose the simulation process, default: 0 Returns dict \u2014 dictionary of clients training data, test data, training data with missing values, global test data Raises ValueError NotImplementedError","title":"Lite Simulation Function"},{"location":"tutorial/basic_usage/","text":"","title":"Basic usage"},{"location":"user-guide/data_prep/","text":"Dataset and Preprocessing The first step for using FedImpute is to prepare the data. Input Data Format and Preprocessing The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced. Required Preprocessing Steps There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ | Ordering Features To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset. One-hot Encoding Categorical Features Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute. Data Normalization (Optional) It is recommended to normalize the numerical features in the dataset within range of 0 and 1. Helper Functions for Preprocessing FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features , one_hot_encoding # Example for data with numpy array data = ... data = ordering_features ( data , numerical_cols = [ 0 , 1 , 3 , 4 , 8 ], target_col =- 1 ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) # Example data with pandas dataframe data = ... data = ordering_features ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) - ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. - one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) Data Configuration Dictionary To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target' : 'house_price' , 'task_type' : 'classification' , 'clf_type' : 'binary' , 'num_cols' : 10 , } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Preparation"},{"location":"user-guide/data_prep/#dataset-and-preprocessing","text":"The first step for using FedImpute is to prepare the data.","title":"Dataset and Preprocessing"},{"location":"user-guide/data_prep/#input-data-format-and-preprocessing","text":"The data should be tabular data in the form of a numpy array ( <np.ndarray> ), where each row represents an observation and each column represents a feature. It will be the input to the simulation process, where it will be partitioned into subset as local dataset for each party and the missing data will be introduced.","title":"Input Data Format and Preprocessing"},{"location":"user-guide/data_prep/#required-preprocessing-steps","text":"There are some basic preprocessing steps that you need to follow before using FedImpute, The final dataset should be in the form of a numpy array with the columns ordered as follows format: | --------------------- | ------------------ | ------ | | numerical features... | binary features... | target | | --------------------- | ------------------ | ------ | | 0.1 3 5 ... | 1 0 1 0 0 0 | ... | ... | 0.5 10 1 ... | 0 0 1 0 0 1 | ... | | --------------------- | ------------------ | ------ |","title":"Required Preprocessing Steps"},{"location":"user-guide/data_prep/#ordering-features","text":"To facilitate the ease of use for FedImpute, you have to order the features in the dataset such that the numerical features are placed first, followed by the binary features . The target variable should be the last column in the dataset.","title":"Ordering Features"},{"location":"user-guide/data_prep/#one-hot-encoding-categorical-features","text":"Currently, FedImpute only supports numerical and binary features, does not support categorical features in the dataset. So you have to one-hot encode the categorical features into binary features before using FedImpute.","title":"One-hot Encoding Categorical Features"},{"location":"user-guide/data_prep/#data-normalization-optional","text":"It is recommended to normalize the numerical features in the dataset within range of 0 and 1.","title":"Data Normalization (Optional)"},{"location":"user-guide/data_prep/#helper-functions-for-preprocessing","text":"FedImpute provides several helper functions to perform the required preprocessing steps. Example of the helper functions are as follows: from fedimpute.data_prep.helper import ordering_features , one_hot_encoding # Example for data with numpy array data = ... data = ordering_features ( data , numerical_cols = [ 0 , 1 , 3 , 4 , 8 ], target_col =- 1 ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) # Example data with pandas dataframe data = ... data = ordering_features ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' ) data = one_hot_encoding ( data , numerical_cols_num = 5 , max_cateogories = 10 ) - ordering_features(data, numerical_cols: List[str or int], target_col: int or str) : This function will order the features in the dataset such that the numerical features are placed first, followed by the binary features. The target variable should be the last column in the dataset. - one_hot_encoding(data, numerical_cols_num: int) : This function will one-hot encode the categorical features into binary features. It assumes you data is already orderd as numerical cols + cat_cols + target, so You just need to specify the number of numerical columns. Note : The ordering_features function is required to be called before the one_hot_encoding function. We also provide a one-for-all function to perform all the preprocessing steps at once. from fedimpute.data_prep import prep_data data = ... data = prep_data ( data , numerical_cols = [ 'age' , 'income' , 'height' , 'weight' , 'temperature' ], target_col = 'house_price' )","title":"Helper Functions for Preprocessing"},{"location":"user-guide/data_prep/#data-configuration-dictionary","text":"To allow FedImpute to understand the data and the task type, you need to provide a configuration dictionary called data_config . The example of the data_config dictionary is as follows: data_config = { 'target' : 'house_price' , 'task_type' : 'classification' , 'clf_type' : 'binary' , 'num_cols' : 10 , } The data_config dictionary should contain the following keys: target : The target variable name. task_type : The task type of the target variable. It can be either classification or regression . clf_type : The classification type of the target variable. It can be either binary or multi-class for classification task. And set it to None for the regression task. num_cols : The number of columns which are numerical (continous variable).","title":"Data Configuration Dictionary"},{"location":"user-guide/evaluation/","text":"Evaluation of Imputation Outcomes Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Downstream Prediction : Evaluate the performance of downstream prediction tasks using imputed data (supports both local or federated prediction). Basic Usage The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator () evaluator . evaluate ( env , [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ]) evaluator . show_results () The Evaluator.evaluate() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect. Supported Evaluation Metrics The following evaluation metrics are supported for each evaluation aspect: Imputation Quality Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions. Downstream Prediction After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. We contains three built-in local downstream prediction models (Linear regression linear , Random Forest tree , and two-layer neural network nn ) and various metrics such as accuracy accu , AUROC auroc , f1 score f1 for evaluation of classification tasks, and mean square error mse , r2 score r2 , mean square log error msle , mean abusolute error mae for evaluation of regression tasks. Fedimpute also supports federated downstream task evaluation for a two-layer neural network with FedAvg fedavg as federated learning strategies. By assessing the performance of the downstream tasks, we can gain insights into how different imputation methods impact the overall effectiveness of the learning process in the presence of missing data. Variance Metrics (Measure the variance of imputationa and prediction outcomes) Variance variance : Variance is a statistical measure that quantifies the spread of a set of values around their mean. We calculate the variance of each client's imputation quality metric value. Jain Index jain-index :The Jain index, also known as the Jain fairness index, is a metric used to evaluate the fairness or equality of resource allocation in distributed systems. We adopt this metric to assess the equality of imputation quality across clients. The Jain index ranges from $ 1/N $ to $1$, where $1$ is the number of clients. A value of $1$ indicates perfect equality, meaning that all clients have the same imputation quality, while a value of $1/N$ represents the worst-case scenario, where one client dominates the imputation performance. Entropy entropy : We utilize entropy to capture the distributional variation of imputation quality across clients. We compute the entropy of this distribution using the standard formula: $-\\sum(p_i * log(p_i))$, where $p_i$ is the normalized imputation quality metric value for client $i$. A higher entropy value indicates a more","title":"Evaluation"},{"location":"user-guide/evaluation/#evaluation-of-imputation-outcomes","text":"Fedimpute provides a comprehensive evaluation module to assess the effectiveness of federated imputation algorithms across various missing data scenarios. The evaluation can be categorized into the following aspects: Imputation Quality : Evaluate the quality of imputed data. Downstream Prediction : Evaluate the performance of downstream prediction tasks using imputed data (supports both local or federated prediction).","title":"Evaluation of Imputation Outcomes"},{"location":"user-guide/evaluation/#basic-usage","text":"The Evaluator class is the evaluation module's main class, use its evaluation() function to perform evaluation. from fedimpute.evaluation import Evaluator evaluator = Evaluator () evaluator . evaluate ( env , [ 'imp_quality' , 'pred_downstream_local' , 'pred_downstream_fed' ]) evaluator . show_results () The Evaluator.evaluate() method is used to evaluate the imputation outcomes. It takes the FedImpEnv object (see Federated Imputaton and a list of evaluation aspects as input. The evaluation aspects can be one or more of the following: imp_quality : Evaluate the quality of imputed data. pred_downstream_local : Evaluate the performance of downstream prediction tasks using imputed data in a local setting. pred_downstream_fed : Evaluate the performance of downstream prediction tasks using imputed data in a federated setting. The Evaluator.show_results() method is used to display the evaluation results. It prints the evaluation results for each evaluation aspect.","title":"Basic Usage"},{"location":"user-guide/evaluation/#supported-evaluation-metrics","text":"The following evaluation metrics are supported for each evaluation aspect:","title":"Supported Evaluation Metrics"},{"location":"user-guide/evaluation/#imputation-quality","text":"Root Mean Squared Error (RMSE) rmse : RMSE is calculated by taking the square root of the mean of the squared differences between the imputed and original values. A lower RMSE indicates better imputation accuracy. Normalized RMSE nrmse : Normalized RMSE is an extension of the standard RMSE that allows for a more intuitive interpretation and comparison of imputation qualities. It is calculated by dividing the RMSE by the range (i.e., standard deviation) of the original data. This normalization process scales the RMSE to a value between 0 and 1 to provide a standardized metric independent of the data scale. Sliced Wasserstein Distance sliced-ws : Sliced Wasserstein distance is a metric that measures the dissimilarity between two high-dimensional probability distributions. We use sliced Wasserstein distance to assess the discrepancy between the probability distributions of the imputed data and the original data for each client. A smaller Wasserstein distance indicates a higher similarity between the imputed and original data distributions.","title":"Imputation Quality"},{"location":"user-guide/evaluation/#downstream-prediction","text":"After missing data are imputed, the downstream task prediction can be performed on imputed data. During the data partition stage, we retain a local test dataset for each client and a global test dataset for global data. These test datasets can be used to evaluate downstream prediction models trained on clients' local imputed datasets to measure the goodness of imputation and how it influences the prediction. We contains three built-in local downstream prediction models (Linear regression linear , Random Forest tree , and two-layer neural network nn ) and various metrics such as accuracy accu , AUROC auroc , f1 score f1 for evaluation of classification tasks, and mean square error mse , r2 score r2 , mean square log error msle , mean abusolute error mae for evaluation of regression tasks. Fedimpute also supports federated downstream task evaluation for a two-layer neural network with FedAvg fedavg as federated learning strategies. By assessing the performance of the downstream tasks, we can gain insights into how different imputation methods impact the overall effectiveness of the learning process in the presence of missing data.","title":"Downstream Prediction"},{"location":"user-guide/evaluation/#variance-metrics-measure-the-variance-of-imputationa-and-prediction-outcomes","text":"Variance variance : Variance is a statistical measure that quantifies the spread of a set of values around their mean. We calculate the variance of each client's imputation quality metric value. Jain Index jain-index :The Jain index, also known as the Jain fairness index, is a metric used to evaluate the fairness or equality of resource allocation in distributed systems. We adopt this metric to assess the equality of imputation quality across clients. The Jain index ranges from $ 1/N $ to $1$, where $1$ is the number of clients. A value of $1$ indicates perfect equality, meaning that all clients have the same imputation quality, while a value of $1/N$ represents the worst-case scenario, where one client dominates the imputation performance. Entropy entropy : We utilize entropy to capture the distributional variation of imputation quality across clients. We compute the entropy of this distribution using the standard formula: $-\\sum(p_i * log(p_i))$, where $p_i$ is the normalized imputation quality metric value for client $i$. A higher entropy value indicates a more","title":"Variance Metrics (Measure the variance of imputationa and prediction outcomes)"},{"location":"user-guide/extend_guide/","text":"","title":"Extend guide"},{"location":"user-guide/fed_imp/","text":"Executing Federated Imputation Algorithms The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms. Overview and Basic Usage Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv () env . configuration ( imputer = 'gain' , fed_strategy = 'fedavg' , fit_mode = 'fed' ) env . setup_from_simulator ( simulator = simulator , verbose = 1 ) env . run_fed_imputation ( run_type = 'sequential' ) Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\" Environment Configuration The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation. Supported Federated Imputation Algorithms Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Fed-Mean Non-NN - fed_mean simple - Fed-EM Non-NN - fed_em em EM , FedEM Fed-ICE Non-NN - fed_ice ice FedICE Fed-MissForest Non-NN - fed_missforest ice MissForest , Fed Randomforest MIWAE NN fedavg , fedprox ,... miwae jm MIWAE GAIN NN fedavg , fedprox , ... gain jm GAIN Federated Strategies: Method Type Fed_strategy(code) Reference FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAdam global FL fedadam FedAdam FedAdagrad global FL fedadagrad FedAdaGrad FedYogi global FL fedyogi FedYogi FedAvg-FT personalized FL fedavg_ft FedAvg-FT Environment Setup After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env . setup_from_simulator ( simulator , verbose = 1 ) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_train_data_ms = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_test_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] global_test = np . random . rand ( 100 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 9 , } clients_seeds = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] env . setup_from_data ( clients_train_data , clients_test_data , clients_train_data_ms , clients_seeds , global_test , data_config , verbose = 1 ) Execute Federated Imputation After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. Currently, we support two types of simulation execution (1) Run FL in sequential mode ( run_type=\"sequential\" ), in this model, there is no parallel, whole processes of imputation for clients run sequantially by using for loop (2) Run federated imputation in parallel mode ( run_type=\"parallel\" ), it will simulate different processes for clients and server and then using workflow to manage communication between clients and server to approach the real world FL environment. env . run_fed_imputation ( run_type = 'squential' ) Miscellaneous verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Federated Imputation"},{"location":"user-guide/fed_imp/#executing-federated-imputation-algorithms","text":"The FedImputeEnv class is the execution_environment module's main class. It is used to configure the federated imputation environment and execute federated imputation algorithms.","title":"Executing Federated Imputation Algorithms"},{"location":"user-guide/fed_imp/#overview-and-basic-usage","text":"Use needs to initialize the FedImputeEnv class and configure the environment using the configuration method - what imputer to use, what federated strategy to use, and what fitting mode to use. Then, use the setup_from_simulator method to set up the environment using the simulated data from simulator class, see Scenario Simulation Section . Finally, use the run_fed_imputation method to execute the federated imputation algorithms. from fedimpute.execution_environment import FedImputeEnv env = FedImputeEnv () env . configuration ( imputer = 'gain' , fed_strategy = 'fedavg' , fit_mode = 'fed' ) env . setup_from_simulator ( simulator = simulator , verbose = 1 ) env . run_fed_imputation ( run_type = 'sequential' ) Note that if you use cuda version of torch, remember to set environment variable for cuda deterministic behavior first # bash (linux) export CUBLAS_WORKSPACE_CONFIG = :4096:8 # powershell (windows) $Env :CUBLAS_WORKSPACE_CONFIG = \":4096:8\"","title":"Overview and Basic Usage"},{"location":"user-guide/fed_imp/#environment-configuration","text":"The env.configuration() method is used to configure the environment. It takes the following arguments: Options : imputer (str) - name of imputation algorithm to use. Options: fed_mean , fed_em , fed_ice , fed_missforest , gain , miwae fed_strategy (str) - name of federated strategy to use. Options: fedavg , fedprox , scaffold , fedavg_ft fit_mode (str) - name of fitting mode to use - federated imputation, local-only imputation or centralized imputation. Options: fed , local , central save_dir_path (str) - path to persist clients and server training process information (imputation models, imputed data etc.) for future use. Other Params : imputer_params (Union[None, dict]) = None - parameters for imputer fed_strategy_params (Union[None, dict]) = None - parameters for federated strategy workflow_params (Union[None, dict]) = None - parameters for workflow - Workflow class contains the logic for federated imputation workflow. It is associated with each Imputer class. The built-in workflows are: ice - for ICE based imputation, em - for EM imputation, jm - for joint modeling based imputation such as VAE or GAN based imputation.","title":"Environment Configuration"},{"location":"user-guide/fed_imp/#supported-federated-imputation-algorithms","text":"Federated Imputation Algorithms: Method Type Fed Strategy Imputer (code) Workflow Reference Fed-Mean Non-NN - fed_mean simple - Fed-EM Non-NN - fed_em em EM , FedEM Fed-ICE Non-NN - fed_ice ice FedICE Fed-MissForest Non-NN - fed_missforest ice MissForest , Fed Randomforest MIWAE NN fedavg , fedprox ,... miwae jm MIWAE GAIN NN fedavg , fedprox , ... gain jm GAIN Federated Strategies: Method Type Fed_strategy(code) Reference FedAvg global FL fedavg FedAvg FedProx global FL fedprox FedProx Scaffold global FL scaffold Scaffold FedAdam global FL fedadam FedAdam FedAdagrad global FL fedadagrad FedAdaGrad FedYogi global FL fedyogi FedYogi FedAvg-FT personalized FL fedavg_ft FedAvg-FT","title":"Supported Federated Imputation Algorithms"},{"location":"user-guide/fed_imp/#environment-setup","text":"After configuring environment, we need to initialize the environment - initialize Client s, Server objects with simulated data from simulation module. Currently, the FedImputeEnv class supports the two ways to set up the environment. First way is to directly setup the environment from simulator class by using env.setup_from_simulator(simulator) method. env . setup_from_simulator ( simulator , verbose = 1 ) The second way is to setup the environment by using env.setup_from_data() method. It can be used in the scenario where user have their own data that not simulated from simulator class. Example: import numpy as np clients_train_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_train_data_ms = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] clients_test_data = [ np . random . rand ( 100 , 10 ) for _ in range ( 10 )] global_test = np . random . rand ( 100 , 10 ) data_config = { 'target' : 9 , 'task_type' : 'regression' , 'clf_type' : None , 'num_cols' : 9 , } clients_seeds = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] env . setup_from_data ( clients_train_data , clients_test_data , clients_train_data_ms , clients_seeds , global_test , data_config , verbose = 1 )","title":"Environment Setup"},{"location":"user-guide/fed_imp/#execute-federated-imputation","text":"After setting up the environment, we can execute the federated imputation algorithms using run_fed_imputation() method. Currently, we support two types of simulation execution (1) Run FL in sequential mode ( run_type=\"sequential\" ), in this model, there is no parallel, whole processes of imputation for clients run sequantially by using for loop (2) Run federated imputation in parallel mode ( run_type=\"parallel\" ), it will simulate different processes for clients and server and then using workflow to manage communication between clients and server to approach the real world FL environment. env . run_fed_imputation ( run_type = 'squential' )","title":"Execute Federated Imputation"},{"location":"user-guide/fed_imp/#miscellaneous","text":"verbose (int) - Verbosity level. 0: no output, 1: minimal output, 2: detailed output seed (int) - Seed for reproducibility logging (bool) - Whether to log the training process","title":"Miscellaneous"},{"location":"user-guide/scenario_simulation/","text":"Simulating Federated Missing Data Scenarios In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section. Overview and Basic Usage The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator () simulation_results = simulator . simulate_scenario ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_mech_type = 'mcar' , verbose = 1 ) # or use the lite simulation function simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_scenario = 'mar-homo' , dp_split_col_option = 'target' , verbose = 1 ) Classical Simulation Function - simulate_scenario The simulate_scenario method has the following major parameters for data partitioning and missing data simulation. Data Partitioning Parameters The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton. Missing Data Simulation Parameters The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type. Lite Simulation Function - Simluting with Predefined Strategies and Scenarios We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use. from fedimpute.simulator import Simulator simulator = Simulator () simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_scenario = 'mar-heter' , dp_split_col_option = 'target' , verbose = 1 ) The simulate_scenario_lite method has the following major parameters for data partitioning and missing data simulation. Data Partitioning Options - dp_strategy iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir@<alpha> : Partition the data samples i.i.d across the clients with sample sizes follows dirichlet distribution with parameter alpha ,e.g. iid-dir@0.5 . niid-dir@<alpha> : Partition the data samples non-i.i.d across the clients with dirichlet distribution with parameter alpha ,e.g. niid-dir@0.5 niid-path@<k> : Partition the data samples non-i.i.d across the clients with pathological distribution with parameter k ,e.g. niid-path@2 . Missing Data Simulation Scenarios - ms_scenario mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"Federated Missing Data Scenario Simulation"},{"location":"user-guide/scenario_simulation/#simulating-federated-missing-data-scenarios","text":"In this section, we will demonstrate how to simulate federated missing data scenarios using the fedimpute.simulator module. The input to this module is a <np.ndarray> dataset and a data configuration dictionary data_config . Details on how to preparing the dataset and the data configuration dictionary are provided in the Data Preparation section.","title":"Simulating Federated Missing Data Scenarios"},{"location":"user-guide/scenario_simulation/#overview-and-basic-usage","text":"The fedimpute.simulator module include the following core functionalities: (1) Data Partition : Partition the dataset horizontally into multiple clients. (2) Missing Data Simulation : Introduce missing values in the dataset of each client. It takes the data and data configuration as input and perform data partition and missing data simulation logic based on the parameters specified by the user and output the following: Clients' local training data Clients' local training data missing mask (representing the missing data) Clients' local test data (used for downstream local prediction evaluation) Global test dataset (used for downstream federated prediction evaluation) The following example demonstrates how to use fedimpute.simulator module. Initialize the Simulator class and call the simulate_scenario method to simulate_scenario simulate the federated missing data scenario. from fedimpute.simulator import Simulator simulator = Simulator () simulation_results = simulator . simulate_scenario ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_mech_type = 'mcar' , verbose = 1 ) # or use the lite simulation function simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_scenario = 'mar-homo' , dp_split_col_option = 'target' , verbose = 1 )","title":"Overview and Basic Usage"},{"location":"user-guide/scenario_simulation/#classical-simulation-function-simulate_scenario","text":"The simulate_scenario method has the following major parameters for data partitioning and missing data simulation.","title":"Classical Simulation Function - simulate_scenario"},{"location":"user-guide/scenario_simulation/#data-partitioning-parameters","text":"The core parameters for data partitioning are number of clients and data partition strategies. num_clients (int) - Number of clients to partition the dataset. dp_strategy (str) - Data partitioning strategy. The available strategies are: iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir : Partition the data samples i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_size_niid_alpha parameter. iid-random : Partition the data samples i.i.d across the clients with random sample sizes. iid-hs : Partition the data samples i.i.d across the clients with hub-and-spoke distribution, one client has significant more samples than the others. niid-dir : Partition the data samples non-i.i.d across the clients with sample sizes follows Dirichlet distribtion with parameter controlled by dp_niid_alpha parameter. niid-path : Partition the data samples non-i.i.d across the clients with sample sizes follows a pathological distribution, each client have two classes of target label. dp_split_cols (Union[str, int, List[int]]) - Column index or name to split the data samples. If the column is continuous, it will be binned into categories by dp_reg_bins . target : Split the data samples based on the target column. first : Split the data samples based on the first feature column. random : Split the data samples based on a random column. <column index> : Split the data samples based on the specified column index. Other Parameters dp_size_niid_alpha (float) - The parameter for Dirichlet distribution in iid-dir strategy. dp_niid_alpha (float) - The parameter for Dirichlet distribution in niid-dir strategy. dp_local_test_size (float) = 0.1 - The size of local test set for each client for downstream local federated prediction evaluation. dp_global_test_size (float) = 0.1 - The size of global test set for the downstream federated prediction evaluation. dp_min_samples (int) - Minimum number of samples in each client. dp_max_samples (int) - Maximum number of samples in each client. dp_even_sample_size (int) - Sample size for each client in iid-even strategy. dp_sample_iid_direct (bool) - Instead of partition data i.i.d, sample data i.i.d from global population (original data) for each client. dp_local_backup_size (float) = 0.05 - backup sample size to avoid all samples in data to be missing dp_reg_bins (int) = 50 - Used for non-i.i.d data partitioning, if column for non-i.i.d partition is continuous, binning it into categories for meaningful non-i.i.d partiton.","title":"Data Partitioning Parameters"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-parameters","text":"The missing data simulation component is used to simulate missing data in the dataset of each client. The core concept here is the missing data heterogeneity which means the each client can have a different missing data characteristics in terms of missing ratio, missing feature and missing mechanisms. The core parameters for missing data simulation are: ms_cols (Union[str, List[int]]) - features to introduce missing values. all : introduce missing values in all features ( default ). all-num : introduce missing values in all numerical features. ms_mech_type (str) - Missing data mechanism type for all clients. The available mechanisms are: mcar : Missing Completely At Random (MCAR) mechanism. mar_sigmoid : Missing At Random (MAR) mechanism simulated using logistic regression model. mar_quantile : Missing At Random (MNAR) mechanism simulated using quantile. mnar_sigmoid : Missing Not At Random (MNAR) mechanism simulated using logistic regression model. mnar_quantile : Missing Not At Random (MNAR) mechanism simulated using quantile. ms_global_mechanism (bool) - If True, all clients have the same missing data mechanism. If False, each client has a different missing data mechanism. This is used for control homogenous or heterogeneous missing data scenario. ms_mr_dist_clients (str) - Missing ratio distribution across clients. The available options: fixed : Missing ratio is the same for all clients. randu : Random uniform missing ratio with random float value for each client. randu-int : Random uniform integer missing ratio e.g., 0.1, 0.3 for each client. randn : Random normal missing ratio with random float value for each client. randn-int : Random normal integer missing ratio e.g., 0.1, 0.3 for each client. ms_mf_dist_clients (str) - Missing feature distribution across clients. 'identity': Each client has the same missing features. ms_mm_dist_clients (str) - Missing mechanism distribution across clients. 'identity': Each client has the same missing mechanism. 'random': Random missing mechanism function for each client. Other Parameters ms_mr_lower (float) = 0.3 - Lower bound of missing ratio ms_mr_upper (float) = 0.7 - Upper bound of missing ratio ms_mm_funcs_bank (str) = 'lr' - missing mechanism function direction bank for MAR, MNAR mechanism. It is a string with any of l , r , m , t four types of functions. l : left side missing r : right side missing m : middle missing t : two sides missing ms_mm_strictness (bool) - If True, the missing mechanism function is strict, otherwise it is probabilistic. ms_mm_obs (bool) = False - This is for MAR mechanism, if True, the missing data is related to some fully observed variables. ms_mm_feature_option (str) = 'allk=0.2' - This is for MAR, MNAR mechanism, strategies for selecting features which missing value is correlated. allk=<ratio> means select k (determined by ratio) highly correlated features from all features. ms_mm_beta_option (str) = None, strategies set coefficient of logistic function for mar_sigmoid and mnar_sigmoid mechanism type.","title":"Missing Data Simulation Parameters"},{"location":"user-guide/scenario_simulation/#lite-simulation-function-simluting-with-predefined-strategies-and-scenarios","text":"We provide a lite version of simulation function simulate_scenario_lite which can be used to simulate the missing data scenario with predefined strategies and scenarios with way fewer parameters for ease of use. from fedimpute.simulator import Simulator simulator = Simulator () simulation_results = simulator . simulate_scenario_lite ( data , data_config , num_clients = 10 , dp_strategy = 'iid-even' , ms_scenario = 'mar-heter' , dp_split_col_option = 'target' , verbose = 1 ) The simulate_scenario_lite method has the following major parameters for data partitioning and missing data simulation.","title":"Lite Simulation Function - Simluting with Predefined Strategies and Scenarios"},{"location":"user-guide/scenario_simulation/#data-partitioning-options-dp_strategy","text":"iid-even : Partition the data samples i.i.d across the clients with equal sample sizes. iid-dir@<alpha> : Partition the data samples i.i.d across the clients with sample sizes follows dirichlet distribution with parameter alpha ,e.g. iid-dir@0.5 . niid-dir@<alpha> : Partition the data samples non-i.i.d across the clients with dirichlet distribution with parameter alpha ,e.g. niid-dir@0.5 niid-path@<k> : Partition the data samples non-i.i.d across the clients with pathological distribution with parameter k ,e.g. niid-path@2 .","title":"Data Partitioning Options - dp_strategy"},{"location":"user-guide/scenario_simulation/#missing-data-simulation-scenarios-ms_scenario","text":"mcar - Missing Completely At Random (MCAR) mechanism. ms_mech_type = 'mcar' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = None ms_mm_obs = False mar-heter - Missing At Random (MAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'randu' ms_mm_obs = True mar-homo - Missing At Random (MAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'fixed' ms_mm_obs = True mnar-heter - Missing Not At Random (MNAR) mechanism with heterogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = False ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False mnar-homo - Missing Not At Random (MNAR) mechanism with homogeneous missing data scenario. ms_mech_type = 'mnar_sigmoid' ms_global_mechanism = True ms_mr_dist_clients = 'randu-int' ms_mm_dist_clients = 'identity' ms_mm_beta_option = 'self' ms_mm_obs = False","title":"Missing Data Simulation Scenarios - ms_scenario"}]}