{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\min\\research_projects\\FedImpute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\min\\research_projects\\FedImpute\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "We first load `codrna` dataset from fedimpute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   X1   |   X2   |   X3   |   X4   |   X5   |   X6   |   X7   |   X8   |   y    |\n",
      "|--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
      "| 0.7554 | 0.1364 | 0.0352 | 0.4132 | 0.6937 | 0.1591 | 0.3329 | 0.7154 | 1.0000 |\n",
      "| 0.7334 | 0.7879 | 0.3819 | 0.3693 | 0.5619 | 0.4830 | 0.4351 | 0.5160 | 0.0000 |\n",
      "| 0.7752 | 0.1364 | 0.1761 | 0.3290 | 0.7410 | 0.4259 | 0.4644 | 0.5268 | 1.0000 |\n",
      "| 0.5905 | 0.7424 | 0.2720 | 0.2898 | 0.6920 | 0.3205 | 0.4019 | 0.6290 | 1.0000 |\n",
      "| 0.7366 | 0.1212 | 0.2465 | 0.3290 | 0.7410 | 0.3249 | 0.5086 | 0.5631 | 1.0000 |\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "Data Dimensions:  (5000, 9)\n",
      "Data Config:\n",
      " {'target': 'y', 'task_type': 'classification', 'natural_partition': False}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fedimpute.data_prep import load_data, display_data\n",
    "data, data_config = load_data(\"codrna\")\n",
    "display_data(data)\n",
    "print(\"Data Dimensions: \", data.shape)\n",
    "print(\"Data Config:\\n\", data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a distributed data scenario\n",
    "\n",
    "We then construct a distributed data scenario with 4 clients and heterogenous MNAR missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Missing data simulation...\n",
      "Results Structure (Dict Keys):\n",
      "['clients_train_data', 'clients_test_data', 'clients_train_data_ms', 'clients_seeds', 'global_test_data', 'data_config', 'stats']\n",
      "==================================================================\n",
      "Scenario Summary\n",
      "==================================================================\n",
      "Total clients: 4\n",
      "Global Test Data: (500, 9)\n",
      "Missing Mechanism Category: MNAR (Self Masking Logit)\n",
      "Clients Data Summary:\n",
      "     Train     Test      Miss     MS Ratio    MS Feature    Seed\n",
      "--  --------  -------  --------  ----------  ------------  ------\n",
      "C1  (1125,9)  (113,9)  (1125,8)     0.47         8/8        6077\n",
      "C2  (1125,9)  (113,9)  (1125,8)     0.51         8/8        577\n",
      "C3  (1125,9)  (113,9)  (1125,8)     0.46         8/8        7231\n",
      "C4  (1125,9)  (113,9)  (1125,8)     0.47         8/8        5504\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fedimpute.scenario import ScenarioBuilder\n",
    "\n",
    "scenario_builder = ScenarioBuilder()\n",
    "scenario_data = scenario_builder.create_simulated_scenario(\n",
    "    data, data_config, num_clients = 4, dp_strategy='iid-even', ms_scenario='mnar-heter'\n",
    ")\n",
    "print('Results Structure (Dict Keys):')\n",
    "print(list(scenario_data.keys()))\n",
    "scenario_builder.summarize_scenario()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build New Imputer\n",
    "\n",
    "In the following example, we develop a new imputer MICE imputation with a 2 layer neural network as underlying machine learning model for imputation, it should inherit the abstract class `BaseMLImputer` and implement all its abstract methods. It also inherit `ICEImputerMixin` class which contains some helper function for ICE imputation. We add comment in class to give more instructions on how we implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedimpute.execution_environment.imputation.base import BaseMLImputer, ICEImputerMixin\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MLPICEImputer(BaseMLImputer, ICEImputerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__(name='mlp_mice', model_persistable=False) # it needs two parameters: name of imputer and whether the model is persistable (can be saved to disk), we set it to False because ICE imptutation is not persistable\n",
    "        \n",
    "        self.imp_models = [] # list of imputation models (each for a feature)\n",
    "        self.min_values = None # min values of features used for clipping\n",
    "        self.max_values = None # max values of features used for clipping\n",
    "        self.seed = None # seed for randomization\n",
    "        self.fit_res_history = {} # fit results history\n",
    "\n",
    "    def initialize(\n",
    "            self, X: np.array, missing_mask: np.array, data_utils: dict, params: dict, seed: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize imputer - statistics imputation models etc.\n",
    "\n",
    "        Args:\n",
    "            X: data with intial imputed values\n",
    "            missing_mask: missing mask of data\n",
    "            data_utils: data utils dictionary - contains information about data\n",
    "            params: params for initialization\n",
    "            seed: int - seed for randomization\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialized imputation models (from sklearn's MLPRegressor (fully connected neural network))\n",
    "        self.imp_models = []\n",
    "        for i in range(data_utils['n_features']):\n",
    "            estimator = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000)\n",
    "            X_train = X[:, np.arange(X.shape[1]) != i][0:10]\n",
    "            y_train = X[:, i][0:10]\n",
    "            estimator.fit(X_train, y_train)\n",
    "            self.imp_models.append(estimator)\n",
    "\n",
    "        # initialize min max values for a clipping threshold (this method is defined in `ICEImputerMixin`)\n",
    "        self.min_values, self.max_values = self.get_clip_thresholds(data_utils)\n",
    "        self.seed = seed\n",
    "    \n",
    "    def get_imp_model_params(self, params: dict) -> OrderedDict:\n",
    "        \"\"\"\n",
    "        Return model parameters\n",
    "\n",
    "        Args:\n",
    "            params: dict contains parameters for get_imp_model_params\n",
    "\n",
    "        Returns:\n",
    "            OrderedDict - model parameters dictionary\n",
    "        \"\"\"\n",
    "        # This method is used to get the parameters of the imputation model for a given feature\n",
    "        # get feature index of imputation models\n",
    "        feature_idx = params['feature_idx']\n",
    "        imp_model = self.imp_models[feature_idx]\n",
    "        \n",
    "        # get parameters from sklearn model\n",
    "        coefs = imp_model.coefs_\n",
    "        intercept = imp_model.intercepts_\n",
    "        \n",
    "        # convert parameters to a dictionary (we need to convert parameters to ordered dictionary as required by `BaseMLImputer`)\n",
    "        parameters = {}\n",
    "        for i in range(len(coefs)):\n",
    "            parameters[f'coef_{i}'] = coefs[i]\n",
    "            parameters[f'intercept_{i}'] = intercept[i]\n",
    "\n",
    "        return OrderedDict(parameters)\n",
    "\n",
    "    def set_imp_model_params(self, updated_model_dict: OrderedDict, params: dict) -> None:\n",
    "        \"\"\"\n",
    "        Set model parameters\n",
    "\n",
    "        Args:\n",
    "            updated_model_dict: global model parameters dictionary\n",
    "            params: parameters for set parameters function\n",
    "        \"\"\"\n",
    "        # This method is used to set the parameters of the imputation model for a given feature (update models)\n",
    "        # get feature index of imputation models\n",
    "        feature_idx = params['feature_idx']\n",
    "        imp_model = self.imp_models[feature_idx]\n",
    "\n",
    "        # set parameters to sklearn model\n",
    "        coefs = []\n",
    "        intercepts = []\n",
    "        for i in range(len(imp_model.coefs_)):\n",
    "            coefs.append(updated_model_dict[f'coef_{i}'])\n",
    "            intercepts.append(updated_model_dict[f'intercept_{i}'])\n",
    "        imp_model.coefs_ = coefs\n",
    "        imp_model.intercepts_ = intercepts\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, missing_mask: np.array, params: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Fit imputer to train local imputation models\n",
    "\n",
    "        Args:\n",
    "            X: np.array - float numpy array features\n",
    "            y: np.array - target\n",
    "            missing_mask: np.array - missing mask\n",
    "            params: parameters for local training\n",
    "        \"\"\"\n",
    "        # This method is used to fit the imputation model for a given feature\n",
    "        # get complete data of the feature\n",
    "        feature_idx = params['feature_idx']\n",
    "        row_mask = missing_mask[:, feature_idx]\n",
    "        X_train = X[~row_mask][:, np.arange(X.shape[1]) != feature_idx]\n",
    "        y_train = X[~row_mask][:, feature_idx]\n",
    "\n",
    "        # fit MLP imputation models\n",
    "        estimator = self.imp_models[feature_idx]\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_pred = estimator.predict(X_train)\n",
    "        loss = np.mean((y_pred - y_train) ** 2)\n",
    "        self.fit_res_history[feature_idx].append({\n",
    "            'loss': loss,\n",
    "            'sample_size': X_train.shape[0]\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'sample_size': X_train.shape[0]\n",
    "        }\n",
    "\n",
    "    def impute(self, X: np.array, y: np.array, missing_mask: np.array, params: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Impute missing values using an imputation model\n",
    "\n",
    "        Args:\n",
    "            X (np.array): numpy array of features\n",
    "            y (np.array): numpy array of target\n",
    "            missing_mask (np.array): missing mask\n",
    "            params (dict): parameters for imputation\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: imputed data - numpy array - same dimension as X\n",
    "        \"\"\"\n",
    "        # This method is used to impute the missing values using the imputation model for a given feature\n",
    "        # get incomplete data of the feature\n",
    "        feature_idx = params['feature_idx']\n",
    "        row_mask = missing_mask[:, feature_idx]\n",
    "        if np.sum(row_mask) == 0:\n",
    "            return X\n",
    "\n",
    "        # impute missing values\n",
    "        X_test = X[row_mask][:, np.arange(X.shape[1]) != feature_idx]\n",
    "        estimator = self.imp_models[feature_idx]\n",
    "        imputed_values = estimator.predict(X_test)\n",
    "        imputed_values = np.clip(imputed_values, self.min_values[feature_idx], self.max_values[feature_idx])\n",
    "        X[row_mask, feature_idx] = np.squeeze(imputed_values)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_fit_res(self, params: dict) -> dict:\n",
    "\n",
    "        # This method is used to get fit results for a given feature from the fit history\n",
    "        try:\n",
    "            feature_idx = params['feature_idx']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Feature index not found in params\")\n",
    "        \n",
    "        return self.fit_res_history[feature_idx][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register New Imputer to Environment\n",
    "\n",
    "Once finishing develop new imputer, it needs to be registered into fedimpute, so it can be used by constructed environment. We need to call `register_imputer` method from `env.register` object. It takes name of imputer, class of imputer, workflow associated with imputer and a list of supported federated strategy of imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fedimpute.execution_environment import FedImputeEnv\n",
    "\n",
    "env = FedImputeEnv(debug_mode=False)\n",
    "env.register.register_imputer(\n",
    "\tname = 'mlp_mice',                     # name of we give to the new imputer\n",
    "\timputer = MLPICEImputer,              # the class of the new imputer we just developed\n",
    "\tworkflow = 'ice',                     # because it is ICE imputation, we use 'ice' workflow\n",
    "\tfed_strategy = ['local', 'fedmice']   # we support local and fedmice strategy for this imputer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSetting up clients...\u001b[0m\n",
      "\u001b[1mSetting up server...\u001b[0m\n",
      "\u001b[1mSetting up workflow...\u001b[0m\n",
      "\u001b[1mEnvironment setup complete.\u001b[0m\n",
      "============================================================\n",
      "Environment Information:\n",
      "============================================================\n",
      "Workflow: ICE (Imputation via Chain Equation)\n",
      "Clients:\n",
      " - Client 0: imputer: mlp_mice, fed-strategy: local\n",
      " - Client 1: imputer: mlp_mice, fed-strategy: local\n",
      " - Client 2: imputer: mlp_mice, fed-strategy: local\n",
      " - Client 3: imputer: mlp_mice, fed-strategy: local\n",
      "Server: fed-strategy: local\n",
      "============================================================\n",
      "\n",
      "\u001b[32m\u001b[1mImputation Start ...\u001b[0m\n",
      "\u001b[1mInitial: imp_rmse: 0.1658 imp_ws: 0.0827 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9301a6b5f4bc464f8f2eb4956822f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ICE Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70da5039de7d41569a70a44b5a6f0a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 0: imp_rmse: 0.2145 imp_ws: 0.1100 loss: 0.0074 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54403692f6944e19cb933eced644020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 1: imp_rmse: 0.2279 imp_ws: 0.1138 loss: 0.0065 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3709943ab14a37bfcad557745eee95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 2: imp_rmse: 0.2276 imp_ws: 0.1131 loss: 0.0059 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c0af15aada407493b4e4ebddd67844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 3: imp_rmse: 0.2351 imp_ws: 0.1153 loss: 0.0061 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060f8bb43d674882b54e155f10b47a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 4: imp_rmse: 0.2383 imp_ws: 0.1164 loss: 0.0062 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dd568d520e41d4b2a73621ea6d50d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 5: imp_rmse: 0.2378 imp_ws: 0.1158 loss: 0.0059 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f065966db3043d8a11541c6500aa7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 6: imp_rmse: 0.2397 imp_ws: 0.1158 loss: 0.0059 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c179bb55104233a2671fef788267bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 7: imp_rmse: 0.2432 imp_ws: 0.1186 loss: 0.0063 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac358a2e3042412a892a525e189ff504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature_idx:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch 8: imp_rmse: 0.2389 imp_ws: 0.1167 loss: 0.0063 \u001b[0m\n",
      "\u001b[1mAll clients converged, iteration 8\u001b[0m\n",
      "\u001b[1mFinal: imp_rmse: 0.2389 imp_ws: 0.1167 \u001b[0m\n",
      "\u001b[32m\u001b[1mFinished. Running time: 65.1471 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# then we can use the new imputer in the environment and run the federated imputation\n",
    "env.configuration(imputer = 'mlp_mice', fed_strategy='local', workflow_params={'log_metric': None})\n",
    "env.setup_from_scenario_builder(scenario_builder = scenario_builder, verbose=1)\n",
    "env.show_env_info()\n",
    "env.run_fed_imputation(verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
